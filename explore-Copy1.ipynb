{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import common dependencies\n",
    "import pandas as pd  # noqa\n",
    "import numpy as np\n",
    "import matplotlib  # noqa\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime  # noqa\n",
    "import PIL  # noqa\n",
    "import glob  # noqa\n",
    "import pickle  # noqa\n",
    "from pathlib import Path  # noqa\n",
    "from scipy import misc  # noqa\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import pdb\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "TRADE_COST_FRAC = .003\n",
    "EPSILON = 1e-10\n",
    "ADV_MULT = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uni_tokens = set()\n",
    "uni_commands = set()\n",
    "uni_actions = set()\n",
    "fname = 'tasks_with_length_tags.txt'\n",
    "with open(fname) as f:\n",
    "    content = f.readlines()\n",
    "content2 = [c.split(' ') for c in content]\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "commands = []\n",
    "actions = []\n",
    "content = [l.replace('\\n', '') for l in content]\n",
    "commands = [x.split(':::')[1].split(' ')[1:-1] for x in content]\n",
    "actions = [x.split(':::')[2].split(' ')[1:-2] for x in content]\n",
    "structures = [x.split(':::')[3].split(' ')[2:] for x in content]\n",
    "\n",
    "structures = [[int(l) for l in program] for program in structures]\n",
    "#actions = [[wd.replace('\\n', '') for wd in res] for res in actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 10, 49, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_actions_per_subprogram = max([max([s for s in struct]) for struct in structures]) + 1\n",
    "max_num_subprograms = max([len(s) for s in structures]) + 1\n",
    "max_cmd_len = max([len(s) for s in commands]) + 1\n",
    "max_act_len = max([len(a) for a in actions]) + 1\n",
    "cmd_lengths_list = [len(s)+1 for s in commands]\n",
    "cmd_lengths_np = np.array(cmd_lengths_list)\n",
    "max_num_subprograms, max_cmd_len, max_act_len, max_actions_per_subprogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_fmap_invmap(unique, num_unique):\n",
    "    fmap = dict(zip(unique, range(num_unique)))\n",
    "    invmap = dict(zip(range(num_unique), unique))\n",
    "    return fmap, invmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for li in content2:\n",
    "    for wd in li:\n",
    "        uni_tokens.add(wd)\n",
    "for li in commands:\n",
    "    for wd in li:\n",
    "        uni_commands.add(wd)\n",
    "for li in actions:\n",
    "    for wd in li:\n",
    "        uni_actions.add(wd)\n",
    "uni_commands.add('end_command')\n",
    "uni_actions.add('end_subprogram')\n",
    "uni_actions.add('end_action')\n",
    "num_cmd = len(uni_commands)\n",
    "num_act = len(uni_actions)\n",
    "command_map, command_invmap = build_fmap_invmap(uni_commands, num_cmd)\n",
    "action_map, action_invmap = build_fmap_invmap(uni_actions, num_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dense_scaled(prev_layer, layer_size, name=None, reuse=False, scale=1.0):\n",
    "    output = tf.layers.dense(prev_layer, layer_size, reuse=reuse) * scale\n",
    "    return output\n",
    "\n",
    "\n",
    "def dense_relu(dense_input, layer_size, scale=1.0):\n",
    "    dense = dense_scaled(dense_input, layer_size, scale=scale)\n",
    "    output = tf.nn.leaky_relu(dense)\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_grad_norm(opt_fcn, loss):\n",
    "    gvs = opt_fcn.compute_gradients(loss)\n",
    "    grad_norm = tf.sqrt(tf.reduce_sum(\n",
    "        [tf.reduce_sum(tf.square(grad)) for grad, var in gvs if grad is not None]))\n",
    "    return grad_norm\n",
    "\n",
    "\n",
    "def apply_clipped_optimizer(opt_fcn, loss, clip_norm=.1, clip_single=.03, clip_global_norm=False):\n",
    "    gvs = opt_fcn.compute_gradients(loss)\n",
    "\n",
    "    if clip_global_norm:\n",
    "        gs, vs = zip(*[(g, v) for g, v in gvs if g is not None])\n",
    "        capped_gs, grad_norm_total = tf.clip_by_global_norm([g for g in gs], clip_norm)\n",
    "        capped_gvs = list(zip(capped_gs, vs))\n",
    "    else:\n",
    "        grad_norm_total = tf.sqrt(\n",
    "            tf.reduce_sum([tf.reduce_sum(tf.square(grad)) for grad, var in gvs if grad is not None]))\n",
    "        capped_gvs = [(tf.clip_by_value(grad, -1 * clip_single, clip_single), var)\n",
    "                      for grad, var in gvs if grad is not None]\n",
    "        capped_gvs = [(tf.clip_by_norm(grad, clip_norm), var) for grad, var in capped_gvs if grad is not None]\n",
    "\n",
    "    optimizer = opt_fcn.apply_gradients(capped_gvs)\n",
    "\n",
    "    return optimizer, grad_norm_total\n",
    "\n",
    "\n",
    "def mlp(x, hidden_sizes, output_size=None, name='', reuse=False):\n",
    "    prev_layer = x\n",
    "\n",
    "    for idx, l in enumerate(hidden_sizes):\n",
    "        dense = dense_scaled(prev_layer, l, name='mlp' + name + '_' + str(idx))\n",
    "        prev_layer = tf.nn.leaky_relu(dense)\n",
    "\n",
    "    output = prev_layer\n",
    "\n",
    "    if output_size is not None:\n",
    "        output = dense_scaled(prev_layer, output_size, name='mlp' + name + 'final')\n",
    "\n",
    "    return output\n",
    "\n",
    "def mlp_with_adversaries(x, hidden_sizes, output_size=None, name='', reuse=False):\n",
    "    prev_layer = x\n",
    "    adv_phs = []\n",
    "    for idx, l in enumerate(hidden_sizes):\n",
    "        \n",
    "        adversary = tf.placeholder_with_default(tf.zeros_like(prev_layer), prev_layer.shape)\n",
    "        prev_layer = prev_layer + adversary\n",
    "        adv_phs.append(adversary)\n",
    "        \n",
    "        dense = dense_scaled(prev_layer, l, name='mlp' + name + '_' + str(idx))\n",
    "        prev_layer = tf.nn.leaky_relu(dense)\n",
    "\n",
    "    output = prev_layer\n",
    "\n",
    "    if output_size is not None:\n",
    "        output = dense_scaled(prev_layer, output_size, name='mlp' + name + 'final')\n",
    "\n",
    "    return output, adv_phs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "commands_ind = [[command_map[c] for c in cmd] + [0] * (max_cmd_len - len(cmd)) for cmd in commands]\n",
    "actions_ind = [[action_map[a] for a in act] + [0] * (max_act_len - len(act)) for act in actions]\n",
    "cmd_np = np.array(commands_ind)\n",
    "actions_structured = []\n",
    "mask_structured = []\n",
    "for row in range(len(structures)):\n",
    "    mask_row = []\n",
    "    action_row = []\n",
    "    act = actions_ind[row]\n",
    "    struct = structures[row]\n",
    "    start = 0\n",
    "    for step in struct:\n",
    "        end = start + step\n",
    "        a = act[start:end]\n",
    "        padding = max_actions_per_subprogram - step - 1\n",
    "        action_row.append(a + [action_map['end_action']] + [0] * padding)\n",
    "        start = end\n",
    "    actions_structured.append(\n",
    "        action_row + [[action_map['end_subprogram']] + [0] * (max_actions_per_subprogram - 1)] +\n",
    "        [[0] * max_actions_per_subprogram] * (max_num_subprograms - len(struct) - 1)\n",
    "    )\n",
    "act_np = np.array(actions_structured)\n",
    "struct_padded = [[sa + 1 for sa in s] + [1] + [0] * (max_num_subprograms - len(s) - 1) for s in structures]\n",
    "struct_np = np.array(struct_padded)\n",
    "\n",
    "mask_list = [[np.concatenate((np.ones(st), np.zeros(max_actions_per_subprogram - st)), 0) \n",
    "              for st in s] for s in struct_np]\n",
    "mask_np = np.array(mask_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\clip_ops.py:110: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "default_sizes = 128\n",
    "size_emb = 64\n",
    "num_layers_encoder = 6\n",
    "hidden_filters = 256\n",
    "num_layers_subprogram = 3\n",
    "hidden_filters_subprogram = 256\n",
    "init_mag = 1e-2\n",
    "cmd_mat = tf.Variable(init_mag*tf.random_normal([num_cmd, size_emb]))\n",
    "act_mat = tf.Variable(init_mag*tf.random_normal([num_act, size_emb]))\n",
    "act_st_emb = tf.Variable(init_mag*tf.random_normal([size_emb]))\n",
    "global_bs = None\n",
    "global_time_len = 7\n",
    "action_lengths = None\n",
    "max_num_actions= None\n",
    "# global_bs = 8\n",
    "global_time_len = 7\n",
    "max_num_actions = 9\n",
    "output_keep_prob = tf.placeholder_with_default(1.0, ())\n",
    "state_keep_prob = tf.placeholder_with_default(1.0, ())\n",
    "cmd_ind = tf.placeholder(tf.int32, shape=(global_bs, 10,))\n",
    "act_ind = tf.placeholder(tf.int32, shape=(global_bs, global_time_len, 9))\n",
    "mask_ph = tf.placeholder(tf.float32, shape=(global_bs, global_time_len, 9))\n",
    "cmd_lengths = tf.placeholder(tf.int32, shape=(global_bs,))\n",
    "act_lengths = tf.placeholder(tf.int32, shape=(global_bs, 7))\n",
    "learning_rate = tf.placeholder(tf.float32, shape = (None))\n",
    "\n",
    "cmd_emb = tf.nn.embedding_lookup(cmd_mat, cmd_ind)\n",
    "act_emb = tf.nn.embedding_lookup(act_mat, act_ind)\n",
    "tf_bs = tf.shape(act_ind)[0]\n",
    "act_st_emb_expanded = tf.tile(tf.reshape(\n",
    "    act_st_emb, [1, 1, 1, size_emb]), [tf_bs, global_time_len, 1, 1])\n",
    "act_emb_with_st = tf.concat((act_st_emb_expanded, act_emb), 2)\n",
    "\n",
    "first_cell_encoder = [tf.nn.rnn_cell.LSTMCell(\n",
    "    hidden_filters, forget_bias=1., name = 'layer1_'+d) for d in ['f', 'b']]\n",
    "hidden_cells_encoder = [[tf.nn.rnn_cell.LSTMCell(\n",
    "    hidden_filters,forget_bias=1., name = 'layer' + str(lidx) + '_' + d)  for d in ['f', 'b']]\n",
    "                        for lidx in range(num_layers_encoder - 1)]\n",
    "hidden_cells_encoder = [[tf.nn.rnn_cell.DropoutWrapper(cell,\n",
    "    output_keep_prob=output_keep_prob, state_keep_prob=state_keep_prob,\n",
    "    variational_recurrent=True, dtype=tf.float32) for cell in cells] for cells in hidden_cells_encoder[:-1]] + [hidden_cells_encoder[-1]]\n",
    "cells_encoder = [first_cell_encoder] + hidden_cells_encoder\n",
    "c1, c2 = zip(*cells_encoder)\n",
    "cells_encoder = [c1, c2]\n",
    "def encode(x, num_layers, cells, initial_states, lengths, name='',):\n",
    "    prev_layer = x\n",
    "    shortcut = x\n",
    "    hiddenlayers = []\n",
    "    returncells = []\n",
    "    cell_fw, cell_bw = cells\n",
    "    bs = tf.shape(x)[0]\n",
    "    for idx in range(num_layers):\n",
    "        prev_layer, c = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw = cell_fw[idx],\n",
    "                cell_bw = cell_bw[idx],\n",
    "                inputs = prev_layer,\n",
    "                sequence_length=lengths,\n",
    "                initial_state_fw=None,\n",
    "                initial_state_bw=None,\n",
    "                dtype=tf.float32,\n",
    "                scope='encoder'+str(idx)\n",
    "            )\n",
    "        prev_layer = tf.concat(prev_layer, 2)\n",
    "        prev_layer = tf.nn.leaky_relu(prev_layer)\n",
    "        returncells.append(c)\n",
    "        hiddenlayers.append(prev_layer)\n",
    "        if idx == num_layers - 1:\n",
    "            #pdb.set_trace()\n",
    "            output = tf.gather_nd(\n",
    "                        prev_layer,\n",
    "                        tf.stack([tf.range(bs), lengths], 1),\n",
    "                        name=None\n",
    "                    )\n",
    "            return prev_layer, returncells, hiddenlayers, output\n",
    "        prev_layer = tf.concat((prev_layer, shortcut), 2)\n",
    "encoding_last_layer, encoding_final_cells, encoding_hidden_layers, encoding_last_timestep = encode(\n",
    "    cmd_emb, num_layers_encoder, cells_encoder,None, lengths = cmd_lengths, name = 'encoder')\n",
    "# encoding_last_timestep = encoding_last_layer[:,cmd_lengths, :]\n",
    "hidden_filters_encoder = encoding_last_timestep.shape[-1].value\n",
    "first_cell_subprogram = tf.nn.rnn_cell.LSTMCell(\n",
    "    hidden_filters_subprogram, forget_bias=1., name = 'subpogramlayer1_')\n",
    "hidden_cells_subprogram = [tf.nn.rnn_cell.LSTMCell(\n",
    "    hidden_filters_subprogram,forget_bias=1., name = 'subpogramlayer' + str(lidx))\n",
    "                        for lidx in range(num_layers_subprogram - 1)]\n",
    "\n",
    "cells_subprogram_rnn = [first_cell_subprogram] + hidden_cells_subprogram\n",
    "\n",
    "attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "    num_units=hidden_filters_encoder, memory=encoding_last_layer,\n",
    "    memory_sequence_length=cmd_lengths)\n",
    "attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "    num_units=hidden_filters_encoder//2, memory=encoding_last_layer,\n",
    "    memory_sequence_length=cmd_lengths)\n",
    "cells_subprogram = [\n",
    "    tf.contrib.seq2seq.AttentionWrapper(\n",
    "        cell, attention_mechanism, attention_layer_size = hidden_filters_subprogram) \n",
    "    for cell in cells_subprogram_rnn]\n",
    "\n",
    "def subprogram(x, num_layers, cells, initial_states, lengths, reuse, name='',):\n",
    "    prev_layer = x\n",
    "    shortcut = x\n",
    "    hiddenlayers = []\n",
    "    returncells = []\n",
    "    bs = tf.shape(x)[0]\n",
    "    for idx in range(num_layers):\n",
    "        print(idx)\n",
    "        if idx == 0:\n",
    "            num_past_units = hidden_filters\n",
    "        else:\n",
    "            num_past_units = hidden_filters_subprogram\n",
    "        with tf.variable_scope(name + 'subprogram' + str(idx), reuse=reuse):\n",
    "#             self_attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "#                 num_units=num_past_units, memory=prev_layer,\n",
    "#                 memory_sequence_length=tf.expand_dims(tf.range(10), 0))\n",
    "#             cell_with_selfattention = tf.contrib.seq2seq.AttentionWrapper(\n",
    "#                     cells[idx], self_attention_mechanism, attention_layer_size = num_past_units)\n",
    "\n",
    "            prev_layer, c = tf.nn.dynamic_rnn(\n",
    "                    cell = cells[idx],\n",
    "                    inputs = prev_layer,\n",
    "                    sequence_length=lengths,\n",
    "                    initial_state = None,\n",
    "                    dtype=tf.float32,\n",
    "                )\n",
    "            prev_layer = tf.concat(prev_layer, 2)\n",
    "            prev_layer = tf.nn.leaky_relu(prev_layer)\n",
    "            returncells.append(c)\n",
    "            hiddenlayers.append(prev_layer)\n",
    "            if idx == num_layers - 1:\n",
    "                output = tf.gather_nd(\n",
    "                            prev_layer,\n",
    "                            tf.stack([tf.range(bs), lengths], 1),\n",
    "                            name=None\n",
    "                        )\n",
    "                return prev_layer, returncells, hiddenlayers, output\n",
    "            prev_layer = tf.concat((prev_layer, shortcut), 2)\n",
    "encodings = [encoding_last_timestep]\n",
    "last_encoding = encoding_last_timestep\n",
    "initial_cmb_encoding = last_encoding\n",
    "loss = 0\n",
    "action_probabilities_presoftmax = []\n",
    "for sub_idx in range(max_num_subprograms): \n",
    "    from_last_layer = tf.tile(tf.expand_dims(tf.concat((\n",
    "        last_encoding, initial_cmb_encoding), 1), 1), [1, max_num_actions + 1, 1])\n",
    "    autoregressive = act_emb_with_st[:,sub_idx, :, :]\n",
    "    x_input = tf.concat((from_last_layer, autoregressive), -1)\n",
    "    subprogram_last_layer, _, subprogram_hidden_layers, subprogram_output = subprogram(\n",
    "        x_input, num_layers_subprogram, cells_subprogram,None, \n",
    "        lengths = act_lengths[:, sub_idx], reuse = (sub_idx > 0), name = 'subprogram')\n",
    "    action_prob_flat = mlp(\n",
    "        tf.reshape(subprogram_last_layer, [-1, hidden_filters_subprogram]),\n",
    "        [], output_size = num_act, name = 'action_choice_mlp', reuse = (sub_idx > 0))\n",
    "    action_prob_expanded = tf.reshape(action_prob_flat, [-1, max_num_actions + 1, num_act])\n",
    "    action_probabilities_layer = tf.nn.softmax(action_prob_expanded, axis=-1)\n",
    "    action_probabilities_presoftmax.append(action_prob_expanded)\n",
    "    delta = mlp(\n",
    "        subprogram_output, [64], output_size = hidden_filters_encoder, name = 'global_transform',\n",
    "        reuse = (sub_idx > 0)\n",
    "    )\n",
    "    last_encoding = last_encoding + delta\n",
    "    encodings.append(last_encoding)\n",
    "act_presoftmax = tf.stack(action_probabilities_presoftmax, 1)[:, :, :-1, :]\n",
    "#batch, subprogram, timestep, action_selection\n",
    "logprobabilities = tf.nn.log_softmax(act_presoftmax, -1)\n",
    "act_presoftmax_flat = tf.reshape(act_presoftmax, [-1, 9, num_act])\n",
    "mask_ph_flat = tf.reshape(mask_ph, [-1, max_actions_per_subprogram])\n",
    "act_ind_flat = tf.reshape(act_ind, [-1, max_actions_per_subprogram])\n",
    "ppl_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "    logits = act_presoftmax_flat,\n",
    "    targets = act_ind_flat,\n",
    "    weights = mask_ph_flat,\n",
    "    average_across_timesteps=False,\n",
    "    average_across_batch=False,\n",
    "    softmax_loss_function=None,\n",
    "    name=None\n",
    ")\n",
    "ppl_loss_avg = tf.reduce_mean(tf.pow(ppl_loss, 1.5)) * 100\n",
    "\n",
    "tfvars = tf.trainable_variables()\n",
    "weight_norm = tf.reduce_mean([tf.reduce_sum(tf.square(var)) for var in tfvars])*1e-5\n",
    "\n",
    "action_taken = tf.argmax(act_presoftmax, -1, output_type=tf.int32)\n",
    "correct_mat = tf.cast(tf.equal(action_taken, act_ind), tf.float32) * mask_ph\n",
    "correct_percent = tf.reduce_sum(correct_mat, [1, 2])/tf.reduce_sum(mask_ph, [1, 2])\n",
    "percent_correct = tf.reduce_mean(correct_percent)\n",
    "percent_fully_correct = tf.reduce_mean(tf.cast(tf.equal(correct_percent, 1), tf.float32))\n",
    "\n",
    "loss = ppl_loss_avg + weight_norm\n",
    "\n",
    "opt_fcn = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "#opt_fcn = tf.train.MomentumOptimizer(learning_rate=learning_rate, use_nesterov=True, momentum=.8)\n",
    "optimizer, grad_norm_total = apply_clipped_optimizer(opt_fcn, loss)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'LeakyRelu_5/Maximum:0' shape=(?, 10, 512) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10455,), (10455,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "trn_percent = .5\n",
    "num_samples = mask_np.shape[0]\n",
    "ordered_samples = np.arange(num_samples)\n",
    "np.random.shuffle(ordered_samples)\n",
    "trn_samples = ordered_samples[:int(np.ceil(num_samples*trn_percent))]\n",
    "val_samples_all = ordered_samples[int(np.ceil(num_samples*trn_percent)):]\n",
    "val_samples = val_samples_all\n",
    "trn_samples.shape, val_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 10 trn_loss 82.73037747202672 val_loss 73.61355\n",
      "itr: 10 trn_acc 0.0 trn_single_acc 0.243841471677016 val_acc 0.0\n",
      "itr: 20 trn_loss 73.54314672282783 val_loss 72.66657257080078\n",
      "itr: 20 trn_acc 0.0 trn_single_acc 0.35161358316131863 val_acc 0.0\n",
      "itr: 30 trn_loss 64.41353259093657 val_loss 71.06220588684083\n",
      "itr: 30 trn_acc 0.0 trn_single_acc 0.4063561076970395 val_acc 0.0\n",
      "itr: 40 trn_loss 54.82856947920858 val_loss 68.32857212066651\n",
      "itr: 40 trn_acc 0.0 trn_single_acc 0.46662783961255727 val_acc 3.8259205757640304e-05\n",
      "itr: 50 trn_loss 44.567429220285824 val_loss 64.97378993606569\n",
      "itr: 50 trn_acc 0.0006363281250000001 trn_single_acc 0.5331290216682378 val_acc 7.269249093951658e-05\n",
      "itr: 60 trn_loss 36.120762428789185 val_loss 61.462414436721815\n",
      "itr: 60 trn_acc 0.001456421163641758 trn_single_acc 0.5866531821523068 val_acc 0.00036193208865006453\n",
      "itr: 70 trn_loss 30.645905207393653 val_loss 57.98415242420198\n",
      "itr: 70 trn_acc 0.002432814400092235 trn_single_acc 0.6186860041370972 val_acc 0.0008326733568013878\n",
      "itr: 80 trn_loss 26.196545057872957 val_loss 54.47410616981889\n",
      "itr: 80 trn_acc 0.003469210316405103 trn_single_acc 0.6524902854646559 val_acc 0.001189386880058155\n",
      "itr: 90 trn_loss 23.011204240476527 val_loss 51.06148063279305\n",
      "itr: 90 trn_acc 0.00576830056415921 trn_single_acc 0.6767199833227686 val_acc 0.0016443362871480932\n",
      "itr: 100 trn_loss 20.342764578162374 val_loss 47.79364166680378\n",
      "itr: 100 trn_acc 0.005621796451723359 trn_single_acc 0.693397510352089 val_acc 0.0021398739701217074\n",
      "itr: 110 trn_loss 18.37669358948103 val_loss 44.68291820904675\n",
      "itr: 110 trn_acc 0.008558003291565369 trn_single_acc 0.705484494735287 val_acc 0.0027484595420097404\n",
      "itr: 120 trn_loss 16.593263559359443 val_loss 41.78883598286863\n",
      "itr: 120 trn_acc 0.007978375146667433 trn_single_acc 0.7228481442628655 val_acc 0.003248362490576663\n",
      "itr: 130 trn_loss 15.277636097603981 val_loss 39.025188666930404\n",
      "itr: 130 trn_acc 0.0077541384463757365 trn_single_acc 0.7373827551671406 val_acc 0.0040043488136310805\n",
      "itr: 140 trn_loss 13.809419407452989 val_loss 36.42526198239679\n",
      "itr: 140 trn_acc 0.009833795809911106 trn_single_acc 0.7596695797446537 val_acc 0.004761254898433038\n",
      "itr: 150 trn_loss 13.017227468978176 val_loss 34.074653871162724\n",
      "itr: 150 trn_acc 0.012189520298886723 trn_single_acc 0.7725232324852758 val_acc 0.0056242015839136975\n",
      "itr: 160 trn_loss 12.064823489836792 val_loss 31.815615875098697\n",
      "itr: 160 trn_acc 0.013907143468695612 trn_single_acc 0.7906173378454794 val_acc 0.007051260229693414\n",
      "itr: 170 trn_loss 11.103860647742389 val_loss 29.735063370383017\n",
      "itr: 170 trn_acc 0.01660577450536482 trn_single_acc 0.8060821795108146 val_acc 0.009292093029689695\n",
      "itr: 180 trn_loss 10.356493314098712 val_loss 27.798049285694567\n",
      "itr: 180 trn_acc 0.024111018724620466 trn_single_acc 0.8218593593239238 val_acc 0.011586221704845344\n",
      "itr: 190 trn_loss 9.80820088333498 val_loss 26.02044562818224\n",
      "itr: 190 trn_acc 0.034098830121169425 trn_single_acc 0.8304033240483826 val_acc 0.014320473879544183\n",
      "itr: 200 trn_loss 9.403577502846193 val_loss 24.30071403075708\n",
      "itr: 200 trn_acc 0.03955413876346238 trn_single_acc 0.8383182645150494 val_acc 0.01760387361940883\n",
      "itr: 210 trn_loss 8.739695858588222 val_loss 22.69783226391184\n",
      "itr: 210 trn_acc 0.03957279380354302 trn_single_acc 0.8500412439012068 val_acc 0.021027608804975232\n",
      "itr: 220 trn_loss 8.144153931784741 val_loss 21.23363194233633\n",
      "itr: 220 trn_acc 0.04656984884038084 trn_single_acc 0.8578569185291818 val_acc 0.024319396102196824\n",
      "itr: 230 trn_loss 7.573568918193041 val_loss 19.864054978189976\n",
      "itr: 230 trn_acc 0.053542137316544294 trn_single_acc 0.8649529121243094 val_acc 0.026889847830325532\n",
      "itr: 240 trn_loss 7.153684163706041 val_loss 18.598709708718758\n",
      "itr: 240 trn_acc 0.052484451038293294 trn_single_acc 0.8690544781760923 val_acc 0.028964134334376605\n",
      "itr: 250 trn_loss 7.07442530578908 val_loss 17.468478411079914\n",
      "itr: 250 trn_acc 0.06344879964136506 trn_single_acc 0.871612995058607 val_acc 0.03282047043340011\n",
      "itr: 260 trn_loss 6.987133814453024 val_loss 16.403100534953\n",
      "itr: 260 trn_acc 0.06285751376798111 trn_single_acc 0.8720667507125293 val_acc 0.03546860051241461\n",
      "itr: 270 trn_loss 6.579560968674864 val_loss 15.402952806363096\n",
      "itr: 270 trn_acc 0.07105980495934018 trn_single_acc 0.8773469768781282 val_acc 0.03794756552952775\n",
      "itr: 280 trn_loss 6.3357501192128804 val_loss 14.485153844543925\n",
      "itr: 280 trn_acc 0.06410812944000174 trn_single_acc 0.8782409054144619 val_acc 0.04033167083303554\n",
      "itr: 290 trn_loss 6.080827399395638 val_loss 13.668814334799615\n",
      "itr: 290 trn_acc 0.07890078984079058 trn_single_acc 0.8829096316300303 val_acc 0.04341471644556901\n",
      "itr: 300 trn_loss 5.834913878404795 val_loss 12.908513015684278\n",
      "itr: 300 trn_acc 0.07204876644543853 trn_single_acc 0.8834508057177537 val_acc 0.04589294823094947\n",
      "itr: 310 trn_loss 5.657166708521951 val_loss 12.20544385866236\n",
      "itr: 310 trn_acc 0.08253586956094976 trn_single_acc 0.8856327917566857 val_acc 0.0494146050401158\n",
      "itr: 320 trn_loss 5.437329355418991 val_loss 11.559882783495587\n",
      "itr: 320 trn_acc 0.07906701371057467 trn_single_acc 0.8890008865324779 val_acc 0.051838041966047306\n",
      "itr: 330 trn_loss 5.360713238358622 val_loss 10.947521385791779\n",
      "itr: 330 trn_acc 0.0855985471414685 trn_single_acc 0.8902840720470391 val_acc 0.05419130172570326\n",
      "itr: 340 trn_loss 5.262299876425437 val_loss 10.390621868291092\n",
      "itr: 340 trn_acc 0.07755259238859795 trn_single_acc 0.890642734151236 val_acc 0.055362319614646834\n",
      "itr: 350 trn_loss 5.133072295619291 val_loss 9.88245686964863\n",
      "itr: 350 trn_acc 0.08093600936320498 trn_single_acc 0.891152623563613 val_acc 0.05679882824375451\n",
      "itr: 360 trn_loss 4.997412949830953 val_loss 9.40331724652654\n",
      "itr: 360 trn_acc 0.09567724952783757 trn_single_acc 0.8947109862894664 val_acc 0.06085591325122912\n",
      "itr: 370 trn_loss 4.922774118493062 val_loss 8.97465496691173\n",
      "itr: 370 trn_acc 0.08865341188092488 trn_single_acc 0.8956666253876803 val_acc 0.06339777313732027\n",
      "itr: 380 trn_loss 4.7383505552620635 val_loss 8.605554223943702\n",
      "itr: 380 trn_acc 0.1039739484950431 trn_single_acc 0.8985288857558097 val_acc 0.06514981809016693\n",
      "itr: 390 trn_loss 4.827864661915675 val_loss 8.229422248204607\n",
      "itr: 390 trn_acc 0.0961110221535394 trn_single_acc 0.8962313534160574 val_acc 0.06845788737615909\n",
      "itr: 400 trn_loss 4.644867854265609 val_loss 7.880382630348258\n",
      "itr: 400 trn_acc 0.09496229858481894 trn_single_acc 0.8983994919645499 val_acc 0.07184643593860536\n",
      "itr: 410 trn_loss 4.505500380066161 val_loss 7.56316798906331\n",
      "itr: 410 trn_acc 0.12066305220995889 trn_single_acc 0.8996039148010463 val_acc 0.07463788060038867\n",
      "itr: 420 trn_loss 4.390877611493563 val_loss 7.272994273729367\n",
      "itr: 420 trn_acc 0.11820497453594522 trn_single_acc 0.9022183416302251 val_acc 0.07660498679598793\n",
      "itr: 430 trn_loss 4.346261291194179 val_loss 7.018687218869248\n",
      "itr: 430 trn_acc 0.11403879257622239 trn_single_acc 0.9026532292065893 val_acc 0.08062311077941023\n",
      "itr: 440 trn_loss 4.335019631410976 val_loss 6.792761060992944\n",
      "itr: 440 trn_acc 0.11904419060753658 trn_single_acc 0.9015063331722488 val_acc 0.08149432448889521\n",
      "itr: 450 trn_loss 4.3649641041819685 val_loss 6.564719247877658\n",
      "itr: 450 trn_acc 0.12019416575665921 trn_single_acc 0.9005210031290181 val_acc 0.0856930506975565\n",
      "itr: 460 trn_loss 4.220084646165066 val_loss 6.352503724151904\n",
      "itr: 460 trn_acc 0.1223060457657778 trn_single_acc 0.9023989825509924 val_acc 0.0868607134596509\n",
      "itr: 470 trn_loss 4.254766613515041 val_loss 6.1577688699251905\n",
      "itr: 470 trn_acc 0.13068809565553563 trn_single_acc 0.9026580235754614 val_acc 0.09003499635839483\n",
      "itr: 480 trn_loss 4.220396291889953 val_loss 5.986607156272515\n",
      "itr: 480 trn_acc 0.12855203867929615 trn_single_acc 0.9010094524229937 val_acc 0.09376224716410657\n",
      "itr: 490 trn_loss 4.15370677910418 val_loss 5.840887286134766\n",
      "itr: 490 trn_acc 0.1374107602672 trn_single_acc 0.9035123387248924 val_acc 0.0997853533021118\n",
      "itr: 500 trn_loss 4.137387787950757 val_loss 5.688288692714161\n",
      "itr: 500 trn_acc 0.1462761602913599 trn_single_acc 0.9040613504157526 val_acc 0.1002994050614392\n",
      "itr: 510 trn_loss 4.058725441235207 val_loss 5.556553176714229\n",
      "itr: 510 trn_acc 0.14401417837936523 trn_single_acc 0.9050629723653094 val_acc 0.1040427792101224\n",
      "itr: 520 trn_loss 4.011314371251262 val_loss 5.421543191379231\n",
      "itr: 520 trn_acc 0.13187962965202213 trn_single_acc 0.9066799539278491 val_acc 0.10313634944222447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 530 trn_loss 3.9424820489393975 val_loss 5.290455976580907\n",
      "itr: 530 trn_acc 0.13924244201248592 trn_single_acc 0.906011486783596 val_acc 0.1040422264241762\n",
      "itr: 540 trn_loss 3.848489902129992 val_loss 5.171028661240932\n",
      "itr: 540 trn_acc 0.1447441252969408 trn_single_acc 0.9085616373152337 val_acc 0.10753565968605164\n",
      "itr: 550 trn_loss 3.7996094803502736 val_loss 5.05852350667995\n",
      "itr: 550 trn_acc 0.15561692143350445 trn_single_acc 0.9095588683717861 val_acc 0.10968501068531528\n",
      "itr: 560 trn_loss 3.82266519753528 val_loss 4.965835485540763\n",
      "itr: 560 trn_acc 0.14579502011859863 trn_single_acc 0.9082444108828034 val_acc 0.11003166986148705\n",
      "itr: 570 trn_loss 3.83767043634504 val_loss 4.871118563726189\n",
      "itr: 570 trn_acc 0.14089223787523864 trn_single_acc 0.9085458618720705 val_acc 0.11226618784047201\n",
      "itr: 580 trn_loss 3.822626772293102 val_loss 4.785970275087335\n",
      "itr: 580 trn_acc 0.1422005454300051 trn_single_acc 0.9111407249299901 val_acc 0.11597978917399379\n",
      "itr: 590 trn_loss 3.7702861004564783 val_loss 4.713127050831775\n",
      "itr: 590 trn_acc 0.15300304259745026 trn_single_acc 0.9104938486900638 val_acc 0.11972375226890444\n",
      "itr: 600 trn_loss 3.7474730287051226 val_loss 4.636158055694276\n",
      "itr: 600 trn_acc 0.15737428432226097 trn_single_acc 0.9118887558921401 val_acc 0.12206031989641845\n",
      "itr: 610 trn_loss 3.621366705452265 val_loss 4.559695565668977\n",
      "itr: 610 trn_acc 0.1721728166716836 trn_single_acc 0.9143257698947012 val_acc 0.12635357032645816\n",
      "itr: 620 trn_loss 3.607523200459897 val_loss 4.489648345680449\n",
      "itr: 620 trn_acc 0.17990209564665904 trn_single_acc 0.915067763182671 val_acc 0.13104006812360056\n",
      "itr: 630 trn_loss 3.525511249720652 val_loss 4.404725292537879\n",
      "itr: 630 trn_acc 0.21639315603782933 trn_single_acc 0.9191267560844959 val_acc 0.13859603254842312\n",
      "itr: 640 trn_loss 3.390486810992904 val_loss 4.325320964349155\n",
      "itr: 640 trn_acc 0.24289813809441443 trn_single_acc 0.9228142213163586 val_acc 0.14908841355604724\n",
      "itr: 650 trn_loss 3.337986672392117 val_loss 4.25707269188122\n",
      "itr: 650 trn_acc 0.27117434381153294 trn_single_acc 0.9275170716629575 val_acc 0.15940195414986727\n",
      "itr: 660 trn_loss 3.1794724746214666 val_loss 4.153547911134565\n",
      "itr: 660 trn_acc 0.3140043095351902 trn_single_acc 0.9321034135401364 val_acc 0.1777993955633031\n",
      "itr: 670 trn_loss 2.9616905054358167 val_loss 4.0563515558426415\n",
      "itr: 670 trn_acc 0.37809169907012646 trn_single_acc 0.9393276385483297 val_acc 0.1971882747192073\n",
      "itr: 680 trn_loss 2.819760623456469 val_loss 3.952905798070267\n",
      "itr: 680 trn_acc 0.4184518101297334 trn_single_acc 0.943612329139944 val_acc 0.2193345844667156\n",
      "itr: 690 trn_loss 2.5117714832628066 val_loss 3.8111659861618\n",
      "itr: 690 trn_acc 0.48741520304430686 trn_single_acc 0.9512267400139087 val_acc 0.24710940002688764\n",
      "itr: 700 trn_loss 2.3135269631986324 val_loss 3.675085202784573\n",
      "itr: 700 trn_acc 0.5393850210558576 trn_single_acc 0.9553101886450799 val_acc 0.2746605337794632\n",
      "itr: 710 trn_loss 2.0871660074558065 val_loss 3.5218538545947387\n",
      "itr: 710 trn_acc 0.5677898621585776 trn_single_acc 0.958744358636326 val_acc 0.3037798459353456\n",
      "itr: 720 trn_loss 1.8956815605547481 val_loss 3.3678916785163686\n",
      "itr: 720 trn_acc 0.6150875268025939 trn_single_acc 0.96398453935858 val_acc 0.33486527696452223\n",
      "itr: 730 trn_loss 1.7403070568659889 val_loss 3.2091265046336956\n",
      "itr: 730 trn_acc 0.648355091490808 trn_single_acc 0.9680423279618627 val_acc 0.36624723519565183\n",
      "itr: 740 trn_loss 1.6410473525578744 val_loss 3.055204503870491\n",
      "itr: 740 trn_acc 0.6788880091928454 trn_single_acc 0.9709001430958393 val_acc 0.39550486665083334\n",
      "itr: 750 trn_loss 1.5420039822444933 val_loss 2.9115124760298285\n",
      "itr: 750 trn_acc 0.6908556395327216 trn_single_acc 0.9731693126025343 val_acc 0.4251557182769182\n",
      "itr: 760 trn_loss 1.4608136709162587 val_loss 2.755893965205105\n",
      "itr: 760 trn_acc 0.7082258921898386 trn_single_acc 0.9736844637360302 val_acc 0.45430920644106293\n",
      "itr: 770 trn_loss 1.2822660480569228 val_loss 2.593138056672113\n",
      "itr: 770 trn_acc 0.741857606599059 trn_single_acc 0.9775946642241248 val_acc 0.48694619646819076\n",
      "itr: 780 trn_loss 1.0765500640192243 val_loss 2.440697989770618\n",
      "itr: 780 trn_acc 0.7769198532355139 trn_single_acc 0.9809421351973675 val_acc 0.5169029394789186\n",
      "itr: 790 trn_loss 0.9503772584238904 val_loss 2.292348064698738\n",
      "itr: 790 trn_acc 0.8084714373370052 trn_single_acc 0.9838089635060326 val_acc 0.5466569328311366\n",
      "itr: 800 trn_loss 0.8207456304738223 val_loss 2.1418758848768134\n",
      "itr: 800 trn_acc 0.8378827665325563 trn_single_acc 0.9865533830494785 val_acc 0.5769075450900152\n",
      "itr: 810 trn_loss 0.7285531484469981 val_loss 2.0049126284833245\n",
      "itr: 810 trn_acc 0.8704087566286662 trn_single_acc 0.9892200283741028 val_acc 0.6046400336268205\n",
      "itr: 820 trn_loss 0.7178326392414173 val_loss 1.8762379411621832\n",
      "itr: 820 trn_acc 0.8660280069286328 trn_single_acc 0.9888350073522549 val_acc 0.6305174947836393\n",
      "itr: 830 trn_loss 0.7435892934925332 val_loss 1.7622808686283777\n",
      "itr: 830 trn_acc 0.8713040933723817 trn_single_acc 0.9896249069730663 val_acc 0.6535967815868611\n",
      "itr: 840 trn_loss 0.7369681751369282 val_loss 1.651422131483634\n",
      "itr: 840 trn_acc 0.8675678586286552 trn_single_acc 0.9894995669853555 val_acc 0.6759367670672436\n",
      "itr: 850 trn_loss 0.6381557455939847 val_loss 1.5675957457049605\n",
      "itr: 850 trn_acc 0.8842286318712871 trn_single_acc 0.9905380494340057 val_acc 0.6936802523783415\n",
      "itr: 860 trn_loss 0.5807265206458074 val_loss 1.4598001947245525\n",
      "itr: 860 trn_acc 0.8984644787159191 trn_single_acc 0.9917899763332032 val_acc 0.7159908524857375\n",
      "itr: 870 trn_loss 0.4340479271815085 val_loss 1.3530053594924385\n",
      "itr: 870 trn_acc 0.9298771469133796 trn_single_acc 0.9942174843068943 val_acc 0.7368355731701624\n",
      "itr: 880 trn_loss 0.3906261476056933 val_loss 1.256804597307805\n",
      "itr: 880 trn_acc 0.9366202918731305 trn_single_acc 0.9945153743735758 val_acc 0.7559497184040341\n",
      "itr: 890 trn_loss 0.3590733168887409 val_loss 1.1850835251178418\n",
      "itr: 890 trn_acc 0.9366695937742205 trn_single_acc 0.994835487660441 val_acc 0.7719759785964067\n",
      "itr: 900 trn_loss 0.3594176279008745 val_loss 1.104142075839586\n",
      "itr: 900 trn_acc 0.9398347602205148 trn_single_acc 0.9951787265571485 val_acc 0.7880160668939926\n",
      "itr: 910 trn_loss 0.2913845776342096 val_loss 1.0359128887024047\n",
      "itr: 910 trn_acc 0.9519357851516969 trn_single_acc 0.9962336232805684 val_acc 0.802767782758228\n",
      "itr: 920 trn_loss 0.29884547609990203 val_loss 0.9985113001972308\n",
      "itr: 920 trn_acc 0.9548360538674533 trn_single_acc 0.99668610123322 val_acc 0.8121992790852952\n",
      "itr: 930 trn_loss 0.3192752199324066 val_loss 0.9482915571728233\n",
      "itr: 930 trn_acc 0.9483029120770246 trn_single_acc 0.9963673281072557 val_acc 0.8232127348323138\n",
      "itr: 940 trn_loss 0.2472105001804494 val_loss 0.8865381245075659\n",
      "itr: 940 trn_acc 0.9648669730975701 trn_single_acc 0.9974343714621869 val_acc 0.8360612380271678\n",
      "itr: 950 trn_loss 0.32517562232809594 val_loss 0.8393839267723855\n",
      "itr: 950 trn_acc 0.9507818943821071 trn_single_acc 0.9962372971499369 val_acc 0.8466301489858798\n",
      "itr: 960 trn_loss 0.3612246065914456 val_loss 0.8018253297656562\n",
      "itr: 960 trn_acc 0.947099834440117 trn_single_acc 0.9957859069505027 val_acc 0.8545352812697565\n",
      "itr: 970 trn_loss 0.35405351792984835 val_loss 0.7545524225577521\n",
      "itr: 970 trn_acc 0.9495274566419388 trn_single_acc 0.9963202003239308 val_acc 0.8646532472452589\n",
      "itr: 980 trn_loss 0.23920090333686606 val_loss 0.716547529632879\n",
      "itr: 980 trn_acc 0.9650010980163756 trn_single_acc 0.9974930234955719 val_acc 0.8709856250716211\n",
      "itr: 990 trn_loss 0.22736920510984343 val_loss 0.6725522316113407\n",
      "itr: 990 trn_acc 0.9709184112933247 trn_single_acc 0.9981202485869879 val_acc 0.8796785514503386\n",
      "itr: 1000 trn_loss 0.26898534214244996 val_loss 0.6296017336799468\n",
      "itr: 1000 trn_acc 0.9649270846879549 trn_single_acc 0.9976050963663047 val_acc 0.8882099784269631\n",
      "itr: 1010 trn_loss 0.1965087101200648 val_loss 0.5888647289234786\n",
      "itr: 1010 trn_acc 0.974997236991815 trn_single_acc 0.9981576122806208 val_acc 0.8958691355754168\n",
      "itr: 1020 trn_loss 0.17456992949274017 val_loss 0.5534580678783407\n",
      "itr: 1020 trn_acc 0.9765934789562726 trn_single_acc 0.9982714575935858 val_acc 0.9020641443582682\n",
      "itr: 1030 trn_loss 0.14743967921856463 val_loss 0.5140518693067924\n",
      "itr: 1030 trn_acc 0.9816159805773524 trn_single_acc 0.998737836342833 val_acc 0.9098108646976032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 1040 trn_loss 0.18384925880071698 val_loss 0.4881851447765495\n",
      "itr: 1040 trn_acc 0.9785175228052558 trn_single_acc 0.9983229752709485 val_acc 0.9159029517507921\n",
      "itr: 1050 trn_loss 0.15986020713544766 val_loss 0.45469738749961247\n",
      "itr: 1050 trn_acc 0.9803898181614717 trn_single_acc 0.9984146378681519 val_acc 0.9223614389243456\n",
      "itr: 1060 trn_loss 0.12757083299321623 val_loss 0.42112428210646086\n",
      "itr: 1060 trn_acc 0.9847357002825586 trn_single_acc 0.9987589810857238 val_acc 0.9284323264249623\n",
      "itr: 1070 trn_loss 0.1602463979466069 val_loss 0.4068068050963\n",
      "itr: 1070 trn_acc 0.9807999255521006 trn_single_acc 0.9985978929723451 val_acc 0.9321648963470871\n",
      "itr: 1080 trn_loss 0.3743168475087682 val_loss 0.40575450214347086\n",
      "itr: 1080 trn_acc 0.9504044329815465 trn_single_acc 0.996450863582044 val_acc 0.9322243530910863\n",
      "itr: 1090 trn_loss 0.24679877841751768 val_loss 0.3829261352354364\n",
      "itr: 1090 trn_acc 0.9628187990733184 trn_single_acc 0.9975189233304138 val_acc 0.936974179687648\n",
      "itr: 1100 trn_loss 0.20310034890739415 val_loss 0.35682232362634625\n",
      "itr: 1100 trn_acc 0.9669909057715589 trn_single_acc 0.9980135623430597 val_acc 0.9418420421563527\n",
      "itr: 1110 trn_loss 0.13397943067384332 val_loss 0.3412629903787972\n",
      "itr: 1110 trn_acc 0.9811058116879698 trn_single_acc 0.9988196023265739 val_acc 0.9451901162400491\n",
      "itr: 1120 trn_loss 0.11602247051867415 val_loss 0.3159205831379832\n",
      "itr: 1120 trn_acc 0.9879083443420151 trn_single_acc 0.9991692278384566 val_acc 0.9497050623358471\n",
      "itr: 1130 trn_loss 0.14469847494753701 val_loss 0.2990458384714551\n",
      "itr: 1130 trn_acc 0.9812727111837445 trn_single_acc 0.998643580162534 val_acc 0.9532233160967691\n",
      "itr: 1140 trn_loss 0.16843017283392772 val_loss 0.3076964207496071\n",
      "itr: 1140 trn_acc 0.9815851633005899 trn_single_acc 0.9988557020088715 val_acc 0.9515212609382031\n",
      "itr: 1150 trn_loss 0.18166019715059115 val_loss 0.30428108465723613\n",
      "itr: 1150 trn_acc 0.9766133226641724 trn_single_acc 0.9985752713250386 val_acc 0.953117099464973\n",
      "itr: 1160 trn_loss 0.1404351970946315 val_loss 0.2891233941202082\n",
      "itr: 1160 trn_acc 0.9826301903895311 trn_single_acc 0.9988933018806516 val_acc 0.9555767897317173\n",
      "itr: 1170 trn_loss 0.16377177453675612 val_loss 0.27881372096699936\n",
      "itr: 1170 trn_acc 0.9778221588794067 trn_single_acc 0.998431395031757 val_acc 0.9563270977332619\n",
      "itr: 1180 trn_loss 0.11494323007167889 val_loss 0.26880844195987996\n",
      "itr: 1180 trn_acc 0.9840551901208641 trn_single_acc 0.9989222178121584 val_acc 0.9591544542784477\n",
      "itr: 1190 trn_loss 0.07684952718977901 val_loss 0.25119931185519295\n",
      "itr: 1190 trn_acc 0.9907145497363082 trn_single_acc 0.9993577170474958 val_acc 0.9618138558335588\n",
      "itr: 1200 trn_loss 0.10371970896002293 val_loss 0.23869681076057792\n",
      "itr: 1200 trn_acc 0.9870205921602582 trn_single_acc 0.9992245246887563 val_acc 0.9639873224497574\n"
     ]
    }
   ],
   "source": [
    "eval_itr = -1\n",
    "bs = 256\n",
    "for itr in range(10000):\n",
    "    samples = np.random.choice(trn_samples, size = bs, replace = False)\n",
    "    if True:#itr == 0:\n",
    "        trn_feed_dict = {\n",
    "            cmd_ind : cmd_np[samples],\n",
    "            act_ind : act_np[samples],\n",
    "            mask_ph : mask_np[samples],\n",
    "            act_lengths : np.clip(struct_np[samples], a_min = 1, a_max = None),\n",
    "            cmd_lengths : cmd_lengths_np[samples],\n",
    "        }\n",
    "        \n",
    "    trn_feed_dict[learning_rate] = .01 / (np.sqrt(itr + 10))\n",
    "    _, trn_loss, acc_trn_single, acc_trn = sess.run(\n",
    "        [optimizer, loss, percent_correct, percent_fully_correct], trn_feed_dict)\n",
    "    if itr == 0:\n",
    "        trn_loss_avg = trn_loss\n",
    "        acc_trn_avg = acc_trn\n",
    "        acc_trn_single_avg = acc_trn_single\n",
    "    else:\n",
    "        trn_loss_avg = trn_loss_avg * .9 + trn_loss * .1\n",
    "        acc_trn_avg = acc_trn_avg * .9 + acc_trn * .1\n",
    "        acc_trn_single_avg = acc_trn_single_avg * .9 + acc_trn_single * .1\n",
    "    if itr % 10 == 0 and itr > 0:\n",
    "        # val_samples = np.random.choice(val_samples_all, size = bs, replace = False)\n",
    "        eval_itr += 1\n",
    "        val_feed_dict = {\n",
    "            cmd_ind : cmd_np[val_samples],\n",
    "            act_ind : act_np[val_samples],\n",
    "            mask_ph : mask_np[val_samples],\n",
    "            act_lengths : np.clip(struct_np[val_samples], a_min = 1, a_max = None),\n",
    "            cmd_lengths : cmd_lengths_np[val_samples]\n",
    "        }\n",
    "        val_loss, acc_val = sess.run([loss, percent_fully_correct], val_feed_dict)\n",
    "        if eval_itr == 0:\n",
    "            val_loss_avg = val_loss\n",
    "            acc_val_avg = acc_val\n",
    "        else:\n",
    "            val_loss_avg = val_loss_avg * .9 + val_loss * .1\n",
    "            acc_val_avg = acc_val_avg * .9 + acc_val * .1\n",
    "        print('itr:', itr, 'trn_loss', trn_loss_avg, 'val_loss', val_loss_avg)\n",
    "        print('itr:', itr, 'trn_acc', acc_trn_avg, \n",
    "              'trn_single_acc', acc_trn_single_avg, 'val_acc', acc_val_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_percent.shape, percent_fully_correct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "10, None, 7, 9, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmd_np.shape, act_np.shape, mask_np.shape, struct_np.shape, cmd_lengths_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cmd_ind.shape, act_ind.shape, mask_ph.shape, act_lengths.shape, cmd_lengths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_samples.shape, val_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "act_presoftmax = tf.stack(action_probabilities_presoftmax, 1)[:, :, :-1, :]\n",
    "#batch, subprogram, timestep, action_selection\n",
    "logprobabilities = tf.nn.log_softmax(act_presoftmax, -1)\n",
    "act_presoftmax_flat = tf.reshape(act_presoftmax, [-1, 9, num_act])\n",
    "mask_ph_flat = tf.reshape(mask_ph, [-1, max_actions_per_subprogram])\n",
    "act_ind_flat = tf.reshape(act_ind, [-1, max_actions_per_subprogram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "act_presoftmax_flat = tf.reshape(act_presoftmax, [-1, 9, num_act])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_actions_per_subprogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess.run(act_presoftmax, feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(*actions_ind[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "command_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subprogram_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subprogram_last_layer[:,cmd_lengths,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoding_last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.gather(\n",
    "    encoding_last_layer,\n",
    "    [1,2],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.gather_nd(\n",
    "    encoding_last_layer,\n",
    "    np.array([[0,1,2,3,4], [1,4,3,2,5]]).T,\n",
    "    name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmd_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_command(sub_cmd, num_repeat):\n",
    "    return sub_cmd * num_repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_command(cmd):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uni_commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uni_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uni_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
