{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lee/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import common dependencies\n",
    "import pandas as pd  # noqa\n",
    "import numpy as np\n",
    "import matplotlib  # noqa\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime  # noqa\n",
    "import PIL  # noqa\n",
    "import glob  # noqa\n",
    "import pickle  # noqa\n",
    "from pathlib import Path  # noqa\n",
    "from scipy import misc  # noqa\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import pdb\n",
    "TRADE_COST_FRAC = .003\n",
    "EPSILON = 1e-10\n",
    "ADV_MULT = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uni_tokens = set()\n",
    "uni_commands = set()\n",
    "uni_actions = set()\n",
    "fname = 'tasks_with_length_tags.txt'\n",
    "with open(fname) as f:\n",
    "    content = f.readlines()\n",
    "content2 = [c.split(' ') for c in content]\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "commands = []\n",
    "actions = []\n",
    "content = [l.replace('\\n', '') for l in content]\n",
    "commands = [x.split(':::')[1].split(' ')[1:-1] for x in content]\n",
    "actions = [x.split(':::')[2].split(' ')[1:-2] for x in content]\n",
    "structures = [x.split(':::')[3].split(' ')[2:] for x in content]\n",
    "\n",
    "structures = [[int(l) for l in program] for program in structures]\n",
    "#actions = [[wd.replace('\\n', '') for wd in res] for res in actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 10, 49, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_actions_per_subprogram = max([max([s for s in struct]) for struct in structures]) + 1\n",
    "max_num_subprograms = max([len(s) for s in structures]) + 1\n",
    "max_cmd_len = max([len(s) for s in commands]) + 1\n",
    "max_act_len = max([len(a) for a in actions]) + 1\n",
    "cmd_lengths_list = [len(s)+1 for s in commands]\n",
    "cmd_lengths_np = np.array(cmd_lengths_list)\n",
    "max_num_subprograms, max_cmd_len, max_act_len, max_actions_per_subprogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_fmap_invmap(unique, num_unique):\n",
    "    fmap = dict(zip(unique, range(num_unique)))\n",
    "    invmap = dict(zip(range(num_unique), unique))\n",
    "    return fmap, invmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for li in content2:\n",
    "    for wd in li:\n",
    "        uni_tokens.add(wd)\n",
    "for li in commands:\n",
    "    for wd in li:\n",
    "        uni_commands.add(wd)\n",
    "for li in actions:\n",
    "    for wd in li:\n",
    "        uni_actions.add(wd)\n",
    "uni_commands.add('end_command')\n",
    "uni_actions.add('end_subprogram')\n",
    "uni_actions.add('end_action')\n",
    "num_cmd = len(uni_commands)\n",
    "num_act = len(uni_actions)\n",
    "command_map, command_invmap = build_fmap_invmap(uni_commands, num_cmd)\n",
    "action_map, action_invmap = build_fmap_invmap(uni_actions, num_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dense_scaled(prev_layer, layer_size, name=None, reuse=False, scale=1.0):\n",
    "    output = tf.layers.dense(prev_layer, layer_size, reuse=reuse) * scale\n",
    "    return output\n",
    "\n",
    "\n",
    "def dense_relu(dense_input, layer_size, scale=1.0):\n",
    "    dense = dense_scaled(dense_input, layer_size, scale=scale)\n",
    "    output = tf.nn.leaky_relu(dense)\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_grad_norm(opt_fcn, loss):\n",
    "    gvs = opt_fcn.compute_gradients(loss)\n",
    "    grad_norm = tf.sqrt(tf.reduce_sum(\n",
    "        [tf.reduce_sum(tf.square(grad)) for grad, var in gvs if grad is not None]))\n",
    "    return grad_norm\n",
    "\n",
    "\n",
    "def apply_clipped_optimizer(opt_fcn, loss, clip_norm=.1, clip_single=.03, clip_global_norm=False):\n",
    "    gvs = opt_fcn.compute_gradients(loss)\n",
    "\n",
    "    if clip_global_norm:\n",
    "        gs, vs = zip(*[(g, v) for g, v in gvs if g is not None])\n",
    "        capped_gs, grad_norm_total = tf.clip_by_global_norm([g for g in gs], clip_norm)\n",
    "        capped_gvs = list(zip(capped_gs, vs))\n",
    "    else:\n",
    "        grad_norm_total = tf.sqrt(\n",
    "            tf.reduce_sum([tf.reduce_sum(tf.square(grad)) for grad, var in gvs if grad is not None]))\n",
    "        capped_gvs = [(tf.clip_by_value(grad, -1 * clip_single, clip_single), var)\n",
    "                      for grad, var in gvs if grad is not None]\n",
    "        capped_gvs = [(tf.clip_by_norm(grad, clip_norm), var) for grad, var in capped_gvs if grad is not None]\n",
    "\n",
    "    optimizer = opt_fcn.apply_gradients(capped_gvs)\n",
    "\n",
    "    return optimizer, grad_norm_total\n",
    "\n",
    "\n",
    "def mlp(x, hidden_sizes, output_size=None, name='', reuse=False):\n",
    "    prev_layer = x\n",
    "\n",
    "    for idx, l in enumerate(hidden_sizes):\n",
    "        dense = dense_scaled(prev_layer, l, name='mlp' + name + '_' + str(idx))\n",
    "        prev_layer = tf.nn.leaky_relu(dense)\n",
    "\n",
    "    output = prev_layer\n",
    "\n",
    "    if output_size is not None:\n",
    "        output = dense_scaled(prev_layer, output_size, name='mlp' + name + 'final')\n",
    "\n",
    "    return output\n",
    "\n",
    "def mlp_with_adversaries(x, hidden_sizes, output_size=None, name='', reuse=False):\n",
    "    prev_layer = x\n",
    "    adv_phs = []\n",
    "    for idx, l in enumerate(hidden_sizes):\n",
    "        \n",
    "        adversary = tf.placeholder_with_default(tf.zeros_like(prev_layer), prev_layer.shape)\n",
    "        prev_layer = prev_layer + adversary\n",
    "        adv_phs.append(adversary)\n",
    "        \n",
    "        dense = dense_scaled(prev_layer, l, name='mlp' + name + '_' + str(idx))\n",
    "        prev_layer = tf.nn.leaky_relu(dense)\n",
    "\n",
    "    output = prev_layer\n",
    "\n",
    "    if output_size is not None:\n",
    "        output = dense_scaled(prev_layer, output_size, name='mlp' + name + 'final')\n",
    "\n",
    "    return output, adv_phs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "commands_ind = [[command_map[c] for c in cmd] + [0] * (max_cmd_len - len(cmd)) for cmd in commands]\n",
    "actions_ind = [[action_map[a] for a in act] + [0] * (max_act_len - len(act)) for act in actions]\n",
    "cmd_np = np.array(commands_ind)\n",
    "actions_structured = []\n",
    "mask_structured = []\n",
    "for row in range(len(structures)):\n",
    "    mask_row = []\n",
    "    action_row = []\n",
    "    act = actions_ind[row]\n",
    "    struct = structures[row]\n",
    "    start = 0\n",
    "    for step in struct:\n",
    "        end = start + step\n",
    "        a = act[start:end]\n",
    "        padding = max_actions_per_subprogram - step - 1\n",
    "        action_row.append(a + [action_map['end_action']] + [0] * padding)\n",
    "        start = end\n",
    "    actions_structured.append(\n",
    "        action_row + [[action_map['end_subprogram']] + [0] * (max_actions_per_subprogram - 1)] +\n",
    "        [[0] * max_actions_per_subprogram] * (max_num_subprograms - len(struct) - 1)\n",
    "    )\n",
    "act_np = np.array(actions_structured)\n",
    "struct_padded = [[sa + 1 for sa in s] + [1] + [0] * (max_num_subprograms - len(s) - 1) for s in structures]\n",
    "struct_np = np.array(struct_padded)\n",
    "\n",
    "mask_list = [[np.concatenate((np.ones(st), np.zeros(max_actions_per_subprogram - st)), 0) \n",
    "              for st in s] for s in struct_np]\n",
    "mask_np = np.array(mask_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "WARNING:tensorflow:From /home/lee/.local/lib/python3.6/site-packages/tensorflow/python/ops/clip_ops.py:110: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "default_sizes = 128\n",
    "size_emb = 64\n",
    "num_layers_encoder = 6\n",
    "hidden_filters = 128\n",
    "num_layers_subprogram = 3\n",
    "hidden_filters_subprogram = default_sizes\n",
    "cmd_mat = tf.Variable(1e-5*tf.random_normal([num_cmd, size_emb]))\n",
    "act_mat = tf.Variable(1e-5*tf.random_normal([num_act, size_emb]))\n",
    "global_bs = None\n",
    "global_time_len = 7\n",
    "action_lengths = None\n",
    "max_num_actions= None\n",
    "# global_bs = 8\n",
    "global_time_len = 7\n",
    "max_num_actions = 9\n",
    "cmd_ind = tf.placeholder(tf.int32, shape=(global_bs, 10,))\n",
    "act_ind = tf.placeholder(tf.int32, shape=(global_bs, global_time_len, 9))\n",
    "mask_ph = tf.placeholder(tf.float32, shape=(global_bs, global_time_len, 9))\n",
    "cmd_lengths = tf.placeholder(tf.int32, shape=(global_bs,))\n",
    "act_lengths = tf.placeholder(tf.int32, shape=(global_bs, 7))\n",
    "learning_rate = tf.placeholder(tf.float32, shape = (None))\n",
    "\n",
    "cmd_emb = tf.nn.embedding_lookup(cmd_mat, cmd_ind)\n",
    "act_emb = tf.nn.embedding_lookup(act_mat, act_ind)\n",
    "act_st_emb = tf.Variable(1e-5*tf.random_normal([size_emb]))\n",
    "tf_bs = tf.shape(act_ind)[0]\n",
    "act_st_emb_expanded = tf.tile(tf.reshape(\n",
    "    act_st_emb, [1, 1, 1, size_emb]), [tf_bs, global_time_len, 1, 1])\n",
    "act_emb_with_st = tf.concat((act_st_emb_expanded, act_emb), 2)\n",
    "\n",
    "first_cell_encoder = [tf.nn.rnn_cell.LSTMCell(\n",
    "    hidden_filters, forget_bias=1., name = 'layer1_'+d) for d in ['f', 'b']]\n",
    "hidden_cells_encoder = [[tf.nn.rnn_cell.LSTMCell(\n",
    "    hidden_filters,forget_bias=1., name = 'layer' + str(lidx) + '_' + d)  for d in ['f', 'b']]\n",
    "                        for lidx in range(num_layers_encoder - 1)]\n",
    "cells_encoder = [first_cell_encoder] + hidden_cells_encoder\n",
    "c1, c2 = zip(*cells_encoder)\n",
    "cells_encoder = [c1, c2]\n",
    "def encode(x, num_layers, cells, initial_states, lengths, name='',):\n",
    "    prev_layer = x\n",
    "    shortcut = x\n",
    "    hiddenlayers = []\n",
    "    returncells = []\n",
    "    cell_fw, cell_bw = cells\n",
    "    bs = tf.shape(x)[0]\n",
    "    for idx in range(num_layers):\n",
    "        prev_layer, c = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw = cell_fw[idx],\n",
    "                cell_bw = cell_bw[idx],\n",
    "                inputs = prev_layer,\n",
    "                sequence_length=lengths,\n",
    "                initial_state_fw=None,\n",
    "                initial_state_bw=None,\n",
    "                dtype=tf.float32,\n",
    "                scope='encoder'+str(idx)\n",
    "            )\n",
    "        prev_layer = tf.concat(prev_layer, 2)\n",
    "        prev_layer = tf.nn.leaky_relu(prev_layer)\n",
    "        returncells.append(c)\n",
    "        hiddenlayers.append(prev_layer)\n",
    "        if idx == num_layers - 1:\n",
    "            #pdb.set_trace()\n",
    "            output = tf.gather_nd(\n",
    "                        prev_layer,\n",
    "                        tf.stack([tf.range(bs), lengths], 1),\n",
    "                        name=None\n",
    "                    )\n",
    "            return prev_layer, returncells, hiddenlayers, output\n",
    "        prev_layer = tf.concat((prev_layer, shortcut), 2)\n",
    "encoding_last_layer, encoding_final_cells, encoding_hidden_layers, encoding_last_timestep = encode(\n",
    "    cmd_emb, num_layers_encoder, cells_encoder,None, lengths = cmd_lengths, name = 'encoder')\n",
    "# encoding_last_timestep = encoding_last_layer[:,cmd_lengths, :]\n",
    "hidden_filters_encoder = encoding_last_timestep.shape[-1].value\n",
    "first_cell_subprogram = tf.nn.rnn_cell.LSTMCell(\n",
    "    hidden_filters_subprogram, forget_bias=1., name = 'subpogramlayer1_')\n",
    "hidden_cells_subprogram = [tf.nn.rnn_cell.LSTMCell(\n",
    "    hidden_filters_subprogram,forget_bias=1., name = 'subpogramlayer' + str(lidx))\n",
    "                        for lidx in range(num_layers_subprogram - 1)]\n",
    "\n",
    "cells_subprogram_rnn = [first_cell_subprogram] + hidden_cells_subprogram\n",
    "\n",
    "attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "    num_units=hidden_filters_encoder, memory=encoding_last_layer,\n",
    "    memory_sequence_length=cmd_lengths)\n",
    "attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "    num_units=hidden_filters_encoder//2, memory=encoding_last_layer,\n",
    "    memory_sequence_length=cmd_lengths)\n",
    "cells_subprogram = [\n",
    "    tf.contrib.seq2seq.AttentionWrapper(\n",
    "        cell, attention_mechanism, attention_layer_size = hidden_filters_subprogram) \n",
    "    for cell in cells_subprogram_rnn]\n",
    "\n",
    "def subprogram(x, num_layers, cells, initial_states, lengths, reuse, name='',):\n",
    "    prev_layer = x\n",
    "    shortcut = x\n",
    "    hiddenlayers = []\n",
    "    returncells = []\n",
    "    bs = tf.shape(x)[0]\n",
    "    for idx in range(num_layers):\n",
    "        print(idx)\n",
    "        if idx == 0:\n",
    "            num_past_units = hidden_filters\n",
    "        else:\n",
    "            num_past_units = hidden_filters_subprogram\n",
    "        with tf.variable_scope(name + 'subprogram' + str(idx), reuse=reuse):\n",
    "            self_attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "                num_units=num_past_units, memory=prev_layer,\n",
    "                memory_sequence_length=lengths)\n",
    "            cell_with_selfattention = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                    cells[idx], self_attention_mechanism, attention_layer_size = num_past_units)\n",
    "\n",
    "            prev_layer, c = tf.nn.dynamic_rnn(\n",
    "                    cell = cell_with_selfattention,\n",
    "                    inputs = prev_layer,\n",
    "                    sequence_length=lengths,\n",
    "                    initial_state = None,\n",
    "                    dtype=tf.float32,\n",
    "                )\n",
    "            prev_layer = tf.concat(prev_layer, 2)\n",
    "            prev_layer = tf.nn.leaky_relu(prev_layer)\n",
    "            returncells.append(c)\n",
    "            hiddenlayers.append(prev_layer)\n",
    "            if idx == num_layers - 1:\n",
    "                output = tf.gather_nd(\n",
    "                            prev_layer,\n",
    "                            tf.stack([tf.range(bs), lengths], 1),\n",
    "                            name=None\n",
    "                        )\n",
    "                return prev_layer, returncells, hiddenlayers, output\n",
    "            prev_layer = tf.concat((prev_layer, shortcut), 2)\n",
    "encodings = [encoding_last_timestep]\n",
    "last_encoding = encoding_last_timestep\n",
    "initial_cmb_encoding = last_encoding\n",
    "loss = 0\n",
    "action_probabilities_presoftmax = []\n",
    "for sub_idx in range(max_num_subprograms): \n",
    "    from_last_layer = tf.tile(tf.expand_dims(tf.concat((\n",
    "        last_encoding, initial_cmb_encoding), 1), 1), [1, max_num_actions + 1, 1])\n",
    "    autoregressive = act_emb_with_st[:,sub_idx, :, :]\n",
    "    x_input = tf.concat((from_last_layer, autoregressive), -1)\n",
    "    subprogram_last_layer, _, subprogram_hidden_layers, subprogram_output = subprogram(\n",
    "        x_input, num_layers_subprogram, cells_subprogram,None, \n",
    "        lengths = act_lengths[:, sub_idx], reuse = (sub_idx > 0), name = 'subprogram')\n",
    "    action_prob_flat = mlp(\n",
    "        tf.reshape(subprogram_last_layer, [-1, hidden_filters_subprogram]),\n",
    "        [], output_size = num_act, name = 'action_choice_mlp', reuse = (sub_idx > 0))\n",
    "    action_prob_expanded = tf.reshape(action_prob_flat, [-1, max_num_actions + 1, num_act])\n",
    "    action_probabilities_layer = tf.nn.softmax(action_prob_expanded, axis=-1)\n",
    "    action_probabilities_presoftmax.append(action_prob_expanded)\n",
    "    delta = mlp(\n",
    "        subprogram_output, [64], output_size = hidden_filters_encoder, name = 'global_transform',\n",
    "        reuse = (sub_idx > 0)\n",
    "    )\n",
    "    last_encoding = last_encoding + delta\n",
    "    encodings.append(last_encoding)\n",
    "act_presoftmax = tf.stack(action_probabilities_presoftmax, 1)[:, :, :-1, :]\n",
    "#batch, subprogram, timestep, action_selection\n",
    "logprobabilities = tf.nn.log_softmax(act_presoftmax, -1)\n",
    "act_presoftmax_flat = tf.reshape(act_presoftmax, [-1, 9, num_act])\n",
    "mask_ph_flat = tf.reshape(mask_ph, [-1, max_actions_per_subprogram])\n",
    "act_ind_flat = tf.reshape(act_ind, [-1, max_actions_per_subprogram])\n",
    "ppl_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "    logits = act_presoftmax_flat,\n",
    "    targets = act_ind_flat,\n",
    "    weights = mask_ph_flat,\n",
    "    average_across_timesteps=False,\n",
    "    average_across_batch=False,\n",
    "    softmax_loss_function=None,\n",
    "    name=None\n",
    ")\n",
    "ppl_loss_avg = tf.reduce_mean(tf.pow(ppl_loss, 1.5)) * 100\n",
    "\n",
    "tfvars = tf.trainable_variables()\n",
    "weight_norm = tf.reduce_mean([tf.reduce_sum(tf.square(var)) for var in tfvars])*1e-3\n",
    "\n",
    "action_taken = tf.argmax(act_presoftmax, -1, output_type=tf.int32)\n",
    "correct_mat = tf.cast(tf.equal(action_taken, act_ind), tf.float32) * mask_ph\n",
    "correct_percent = tf.reduce_sum(correct_mat, [1, 2])/tf.reduce_sum(mask_ph, [1, 2])\n",
    "percent_correct = tf.reduce_mean(correct_percent)\n",
    "percent_fully_correct = tf.reduce_mean(tf.cast(tf.equal(correct_percent, 1), tf.float32))\n",
    "\n",
    "loss = ppl_loss_avg + weight_norm\n",
    "\n",
    "opt_fcn = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "optimizer, grad_norm_total = apply_clipped_optimizer(opt_fcn, loss)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'LeakyRelu_5/Maximum:0' shape=(?, 10, 256) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((210,), (20700,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "trn_percent = .01\n",
    "num_samples = mask_np.shape[0]\n",
    "ordered_samples = np.arange(num_samples)\n",
    "np.random.shuffle(ordered_samples)\n",
    "trn_samples = ordered_samples[:int(np.ceil(num_samples*trn_percent))]\n",
    "val_samples_all = ordered_samples[int(np.ceil(num_samples*trn_percent)):]\n",
    "val_samples = val_samples_all\n",
    "trn_samples.shape, val_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 10 trn_loss 86.35061699876739 val_loss 74.25813\n",
      "itr: 10 trn_acc 0.0 trn_single_acc 0.26231343246573324 val_acc 0.0\n",
      "itr: 20 trn_loss 70.77462243636045 val_loss 72.83163757324219\n",
      "itr: 20 trn_acc 0.0014062500000000002 trn_single_acc 0.43989527891308633 val_acc 0.00037681160029023886\n",
      "itr: 30 trn_loss 57.89140280643862 val_loss 70.36643455505371\n",
      "itr: 30 trn_acc 0.0026581735704531255 trn_single_acc 0.5631016794818173 val_acc 0.0013149758218787611\n",
      "itr: 40 trn_loss 45.93048391539236 val_loss 68.32682555389405\n",
      "itr: 40 trn_acc 0.02624175801718565 trn_single_acc 0.6395468136656974 val_acc 0.0030578744446393105\n",
      "itr: 50 trn_loss 38.67291135754609 val_loss 64.61781224136354\n",
      "itr: 50 trn_acc 0.05111564979466398 trn_single_acc 0.6986543609100662 val_acc 0.00915305289940443\n",
      "itr: 60 trn_loss 27.31301435512408 val_loss 60.33090856544496\n",
      "itr: 60 trn_acc 0.11431282712260134 trn_single_acc 0.7933211303985749 val_acc 0.018001032382098492\n",
      "itr: 70 trn_loss 22.586914715508282 val_loss 55.60584173386384\n",
      "itr: 70 trn_acc 0.14205298926484214 trn_single_acc 0.8414490851941956 val_acc 0.04711880312250345\n",
      "itr: 80 trn_loss 16.20109280834241 val_loss 51.664435760245524\n",
      "itr: 80 trn_acc 0.29705532521934475 trn_single_acc 0.8951047868754964 val_acc 0.0574455695678558\n",
      "itr: 90 trn_loss 12.049113316931361 val_loss 47.47442670112771\n",
      "itr: 90 trn_acc 0.4382168017083794 trn_single_acc 0.9306411885017981 val_acc 0.08355125522581631\n",
      "itr: 100 trn_loss 8.442537263103102 val_loss 43.19283047419121\n",
      "itr: 100 trn_acc 0.5775360509406012 trn_single_acc 0.9495245661213981 val_acc 0.15857777061433698\n",
      "itr: 110 trn_loss 10.385878494732134 val_loss 40.51678587586877\n",
      "itr: 110 trn_acc 0.6995688759856705 trn_single_acc 0.9666290617692037 val_acc 0.19939632173512375\n",
      "itr: 120 trn_loss 6.96637437603887 val_loss 36.74545661441562\n",
      "itr: 120 trn_acc 0.7734592511071317 trn_single_acc 0.9770940235128767 val_acc 0.26853881737113594\n",
      "itr: 130 trn_loss 3.1613268738475147 val_loss 40.44015762655805\n",
      "itr: 130 trn_acc 0.8954828056835115 trn_single_acc 0.9897149746488059 val_acc 0.3223660927461439\n",
      "itr: 140 trn_loss 6.612578689763747 val_loss 36.81273923023525\n",
      "itr: 140 trn_acc 0.8992910890470984 trn_single_acc 0.9876334086097538 val_acc 0.37493624759735356\n",
      "itr: 150 trn_loss 2.7398649118677247 val_loss 33.24569328162655\n",
      "itr: 150 trn_acc 0.9549309781591475 trn_single_acc 0.994826598455941 val_acc 0.43219624584634625\n",
      "itr: 160 trn_loss 1.516043689267148 val_loss 30.019164915683135\n",
      "itr: 160 trn_acc 0.9480211206895738 trn_single_acc 0.9947282270014841 val_acc 0.4853534337197499\n",
      "itr: 170 trn_loss 0.6579569428309161 val_loss 27.096608670773147\n",
      "itr: 170 trn_acc 0.9786722304282695 trn_single_acc 0.9979150215639411 val_acc 0.5315620514435849\n",
      "itr: 180 trn_loss 0.29011499846048006 val_loss 24.45843028326813\n",
      "itr: 180 trn_acc 0.9925634665749168 trn_single_acc 0.9992730129712728 val_acc 0.5747923206701828\n",
      "itr: 190 trn_loss 0.14947271225704545 val_loss 22.08364634484038\n",
      "itr: 190 trn_acc 0.9974070411255905 trn_single_acc 0.9997465152968504 val_acc 0.6140232340766448\n",
      "itr: 200 trn_loss 0.09654411493456284 val_loss 19.938782263725862\n",
      "itr: 200 trn_acc 0.9990958911444274 trn_single_acc 0.9999116153491165 val_acc 0.6502199461704604\n",
      "itr: 210 trn_loss 0.07668251399017809 val_loss 18.010450299617528\n",
      "itr: 210 trn_acc 0.9996847567345584 trn_single_acc 0.9999691821778011 val_acc 0.6822414300423365\n",
      "itr: 220 trn_loss 0.06894605923611352 val_loss 16.27245166543443\n",
      "itr: 220 trn_acc 0.9998900814699536 trn_single_acc 0.9999892544898283 val_acc 0.7111090750916155\n",
      "itr: 230 trn_loss 0.06564519600056842 val_loss 14.706949770241849\n",
      "itr: 230 trn_acc 0.9999616737784054 trn_single_acc 0.9999962532722753 val_acc 0.7370416460713761\n",
      "itr: 240 trn_loss 0.06401690410698288 val_loss 13.298418628981596\n",
      "itr: 240 trn_acc 0.9999866364728395 trn_single_acc 0.9999986935968214 val_acc 0.7607046310556844\n",
      "itr: 250 trn_loss 0.06299864206214216 val_loss 12.02988105518599\n",
      "itr: 250 trn_acc 0.9999953404261954 trn_single_acc 0.9999995444853774 val_acc 0.7820013175415619\n",
      "itr: 260 trn_loss 0.06226557423049364 val_loss 10.886191000070985\n",
      "itr: 260 trn_acc 0.9999983753070742 trn_single_acc 0.999999841171872 val_acc 0.8013036021597049\n",
      "itr: 270 trn_loss 0.06164382588143506 val_loss 9.857519265586271\n",
      "itr: 270 trn_acc 0.999999433504605 trn_single_acc 0.999999944620056 val_acc 0.8183809729522641\n",
      "itr: 280 trn_loss 0.06106088416670823 val_loss 8.932315573013545\n",
      "itr: 280 trn_acc 0.9999998024752692 trn_single_acc 0.9999999806902075 val_acc 0.8339100252484837\n",
      "itr: 290 trn_loss 0.06056319086102478 val_loss 8.09983802281325\n",
      "itr: 290 trn_acc 0.999999931127385 trn_single_acc 0.9999999932670915 val_acc 0.8478861723150812\n",
      "itr: 300 trn_loss 0.06004774325916052 val_loss 7.350362954519733\n",
      "itr: 300 trn_acc 0.9999999759856041 trn_single_acc 0.9999999976523799 val_acc 0.8604598766987923\n",
      "itr: 310 trn_loss 0.05957735848117499 val_loss 6.675876293762813\n",
      "itr: 310 trn_acc 0.9999999916266978 trn_single_acc 0.9999999991814356 val_acc 0.8717617147945228\n",
      "itr: 320 trn_loss 0.05913488709954186 val_loss 6.068275476262829\n",
      "itr: 320 trn_acc 0.99999999708041 trn_single_acc 0.9999999997145843 val_acc 0.8819526929065165\n",
      "itr: 330 trn_loss 0.05869318722729289 val_loss 5.5213335199050855\n",
      "itr: 330 trn_acc 0.999999998982002 trn_single_acc 0.9999999999004817 val_acc 0.891119745231084\n",
      "itr: 340 trn_loss 0.05826949908318718 val_loss 5.028948555251308\n",
      "itr: 340 trn_acc 0.999999999645046 trn_single_acc 0.9999999999653001 val_acc 0.8993700923231946\n",
      "itr: 350 trn_loss 0.05786340640470436 val_loss 4.585754033798291\n",
      "itr: 350 trn_acc 0.9999999998762351 trn_single_acc 0.9999999999879008 val_acc 0.9067954047060944\n",
      "itr: 360 trn_loss 0.05748461469134266 val_loss 4.187498131580037\n",
      "itr: 360 trn_acc 0.9999999999568457 trn_single_acc 0.9999999999957812 val_acc 0.9134733519140128\n",
      "itr: 370 trn_loss 0.05710067540842557 val_loss 3.828552853334143\n",
      "itr: 370 trn_acc 0.9999999999849529 trn_single_acc 0.999999999998529 val_acc 0.9194883383378307\n",
      "itr: 380 trn_loss 0.05676084586908496 val_loss 3.5051772874011347\n",
      "itr: 380 trn_acc 0.9999999999947533 trn_single_acc 0.9999999999994869 val_acc 0.9248825022934306\n",
      "itr: 390 trn_loss 0.05640019193402818 val_loss 3.21387348306898\n",
      "itr: 390 trn_acc 0.9999999999981705 trn_single_acc 0.999999999999821 val_acc 0.9297275879405524\n",
      "itr: 400 trn_loss 0.05606208930385838 val_loss 2.9515678501074647\n",
      "itr: 400 trn_acc 0.999999999999362 trn_single_acc 0.9999999999999375 val_acc 0.934112316825025\n",
      "itr: 410 trn_loss 0.05573456701299392 val_loss 2.715058995718819\n",
      "itr: 410 trn_acc 0.9999999999997774 trn_single_acc 0.9999999999999782 val_acc 0.9380585728210503\n",
      "itr: 420 trn_loss 0.055391518487423366 val_loss 2.5018944741204936\n",
      "itr: 420 trn_acc 0.9999999999999223 trn_single_acc 0.9999999999999923 val_acc 0.9416102032174731\n",
      "itr: 430 trn_loss 0.05506615808452581 val_loss 2.3100207150801024\n",
      "itr: 430 trn_acc 0.9999999999999728 trn_single_acc 0.9999999999999973 val_acc 0.9448066705742536\n",
      "itr: 440 trn_loss 0.05476841934124245 val_loss 2.1374943584940436\n",
      "itr: 440 trn_acc 0.9999999999999905 trn_single_acc 0.999999999999999 val_acc 0.9476786632191293\n",
      "itr: 450 trn_loss 0.05446558148134613 val_loss 1.9823603568144086\n",
      "itr: 450 trn_acc 0.9999999999999967 trn_single_acc 0.9999999999999994 val_acc 0.9502634565995174\n",
      "itr: 460 trn_loss 0.05416070772270838 val_loss 1.8425236288607816\n",
      "itr: 460 trn_acc 0.9999999999999988 trn_single_acc 0.9999999999999994 val_acc 0.9525945986180935\n",
      "itr: 470 trn_loss 0.05388419027279188 val_loss 1.716205532303515\n",
      "itr: 470 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.954692626434812\n",
      "itr: 480 trn_loss 0.05359412000994532 val_loss 1.6022931471029487\n",
      "itr: 480 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9565760234936318\n",
      "itr: 490 trn_loss 0.05331775667330847 val_loss 1.4999268294477075\n",
      "itr: 490 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9582952326486327\n",
      "itr: 500 trn_loss 0.05304313872001641 val_loss 1.4077286101374278\n",
      "itr: 500 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9598473548248247\n",
      "itr: 510 trn_loss 0.05277585909034138 val_loss 1.3386934796267673\n",
      "itr: 510 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9612346028704796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 520 trn_loss 0.052509035993110166 val_loss 1.26549057358151\n",
      "itr: 520 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9628164591270107\n",
      "itr: 530 trn_loss 0.05225979551474519 val_loss 1.1998601215509654\n",
      "itr: 530 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9639116247186738\n",
      "itr: 540 trn_loss 0.05200965872345383 val_loss 1.1639416447962596\n",
      "itr: 540 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9648344742974351\n",
      "itr: 550 trn_loss 0.05176377768403695 val_loss 1.1518060772137588\n",
      "itr: 550 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9656747008312384\n",
      "itr: 560 trn_loss 0.051518747890681005 val_loss 1.1208658047184419\n",
      "itr: 560 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9664598904504156\n",
      "itr: 570 trn_loss 0.05126978674085763 val_loss 1.088929095357514\n",
      "itr: 570 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9671713890839019\n",
      "itr: 580 trn_loss 0.051023333443730896 val_loss 1.0719959781973272\n",
      "itr: 580 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9678020759411214\n",
      "itr: 590 trn_loss 0.050789285462878676 val_loss 1.037037495947663\n",
      "itr: 590 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9684131697405183\n",
      "itr: 600 trn_loss 0.05056528145743258 val_loss 1.0376173533589697\n",
      "itr: 600 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9689100166191581\n",
      "itr: 610 trn_loss 0.050339278575436165 val_loss 1.0460223803750623\n",
      "itr: 610 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9696566981103948\n",
      "itr: 620 trn_loss 0.0501096868732497 val_loss 1.0131714686601117\n",
      "itr: 620 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9703915109062431\n",
      "itr: 630 trn_loss 0.0498853433472099 val_loss 0.9951760957999904\n",
      "itr: 630 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9710383525333617\n",
      "itr: 640 trn_loss 0.049671029416976646 val_loss 0.9955166629282128\n",
      "itr: 640 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9715915242590143\n",
      "itr: 650 trn_loss 0.04945985704274995 val_loss 0.975213723003342\n",
      "itr: 650 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9721231925270826\n",
      "itr: 660 trn_loss 0.04923752232222209 val_loss 0.9572406857783862\n",
      "itr: 660 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9726161898179535\n",
      "itr: 670 trn_loss 0.049031019974889095 val_loss 0.9766541227318587\n",
      "itr: 670 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9729922539893107\n",
      "itr: 680 trn_loss 0.04882216471883686 val_loss 0.9660813782767423\n",
      "itr: 680 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9733645254585132\n",
      "itr: 690 trn_loss 0.04861730081534694 val_loss 0.9489684856673452\n",
      "itr: 690 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9737044037174867\n",
      "itr: 700 trn_loss 0.04841823219489617 val_loss 0.9540953478283023\n",
      "itr: 700 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.973701112937184\n",
      "itr: 710 trn_loss 0.04822139637638114 val_loss 1.0076157308578195\n",
      "itr: 710 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9737995983403192\n",
      "itr: 720 trn_loss 0.048015549208172796 val_loss 1.059062736358616\n",
      "itr: 720 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9740186740077673\n",
      "itr: 730 trn_loss 0.04783075896423288 val_loss 1.1062734876212836\n",
      "itr: 730 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9742061801955526\n",
      "itr: 740 trn_loss 0.04763301003587554 val_loss 1.1478931862834718\n",
      "itr: 740 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9743990875666224\n",
      "itr: 750 trn_loss 0.04744778566832955 val_loss 1.2068132482818033\n",
      "itr: 750 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9745485523985222\n",
      "itr: 760 trn_loss 0.047253468729059396 val_loss 1.2553624203370153\n",
      "itr: 760 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9747120564859864\n",
      "itr: 770 trn_loss 0.04706804568134255 val_loss 1.2621060035138851\n",
      "itr: 770 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9748785339905403\n",
      "itr: 780 trn_loss 0.04687787422228235 val_loss 1.2465890486397915\n",
      "itr: 780 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9750525155467018\n",
      "itr: 790 trn_loss 0.046692509452738895 val_loss 1.2208887195013678\n",
      "itr: 790 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9753008901002012\n",
      "itr: 800 trn_loss 0.04651854009310693 val_loss 1.2269966234644696\n",
      "itr: 800 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9754712836970688\n",
      "itr: 810 trn_loss 0.04633399165052091 val_loss 1.2656042244923513\n",
      "itr: 810 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9755763283696592\n",
      "itr: 820 trn_loss 0.04615653152754766 val_loss 1.3261547267638805\n",
      "itr: 820 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.975603235184564\n",
      "itr: 830 trn_loss 0.04598587854143168 val_loss 1.354013292211577\n",
      "itr: 830 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.975661270993424\n",
      "itr: 840 trn_loss 0.0458016201076967 val_loss 1.388431735233919\n",
      "itr: 840 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9757038413084798\n",
      "itr: 850 trn_loss 0.04563149846726292 val_loss 1.4027359253732468\n",
      "itr: 850 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9757759683070111\n",
      "itr: 860 trn_loss 0.04546096162738688 val_loss 1.4148378640325163\n",
      "itr: 860 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9756186586085731\n",
      "itr: 870 trn_loss 0.04529561160193267 val_loss 1.4732500556032027\n",
      "itr: 870 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.975665490162114\n",
      "itr: 880 trn_loss 0.04512381808119261 val_loss 1.5484993078935294\n",
      "itr: 880 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.975620681344038\n",
      "itr: 890 trn_loss 0.044951018340689555 val_loss 1.5804422514404193\n",
      "itr: 890 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9756238290356687\n",
      "itr: 900 trn_loss 0.044784806396491746 val_loss 1.6082009151666166\n",
      "itr: 900 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9755928482431553\n",
      "itr: 910 trn_loss 0.04462412979271233 val_loss 1.6592907376666788\n",
      "itr: 910 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9755408077673657\n",
      "itr: 920 trn_loss 0.04445595161661421 val_loss 1.6605821491104296\n",
      "itr: 920 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9755519428166637\n",
      "itr: 930 trn_loss 0.04429734713956745 val_loss 1.850767186953598\n",
      "itr: 930 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9753445743006071\n",
      "itr: 940 trn_loss 0.04413382833407496 val_loss 1.9120472177928454\n",
      "itr: 940 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9753560088707447\n",
      "itr: 950 trn_loss 0.04397440044687214 val_loss 1.8435676609038503\n",
      "itr: 950 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.97551122867764\n",
      "itr: 960 trn_loss 0.043817062865182346 val_loss 1.790072439422535\n",
      "itr: 960 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9755977889630284\n",
      "itr: 970 trn_loss 0.043657869409401064 val_loss 1.7618172868880575\n",
      "itr: 970 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9756273836552876\n",
      "itr: 980 trn_loss 0.04349954576360634 val_loss 1.711919907163943\n",
      "itr: 980 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9756926665299932\n",
      "itr: 990 trn_loss 0.04334930361081819 val_loss 1.6913048536461877\n",
      "itr: 990 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.975712773465556\n",
      "itr: 1000 trn_loss 0.04319847202384173 val_loss 1.690150533099379\n",
      "itr: 1000 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9757260357708711\n",
      "itr: 1010 trn_loss 0.04304704256529551 val_loss 1.689847785812085\n",
      "itr: 1010 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9757089861069005\n",
      "itr: 1020 trn_loss 0.04289858236677721 val_loss 1.6836170779896962\n",
      "itr: 1020 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9757081372589363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 1030 trn_loss 0.0427420968328177 val_loss 1.61677345280944\n",
      "itr: 1030 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9757943305120313\n",
      "itr: 1040 trn_loss 0.04259595588232411 val_loss 1.552633458183098\n",
      "itr: 1040 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9758863943289618\n",
      "itr: 1050 trn_loss 0.042450748059657775 val_loss 1.5611364607267575\n",
      "itr: 1050 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9759064523104639\n",
      "itr: 1060 trn_loss 0.04230173958094653 val_loss 1.5685439256655869\n",
      "itr: 1060 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9759245044938158\n",
      "itr: 1070 trn_loss 0.04215404662444983 val_loss 1.6101691258991686\n",
      "itr: 1070 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9758924418942421\n",
      "itr: 1080 trn_loss 0.04201072405233295 val_loss 1.6389583885411854\n",
      "itr: 1080 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9758587516179343\n",
      "itr: 1090 trn_loss 0.04186770073935195 val_loss 1.677225030041376\n",
      "itr: 1090 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9758139404801125\n",
      "itr: 1100 trn_loss 0.04172244445266344 val_loss 1.7534349172502512\n",
      "itr: 1100 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9757446247173185\n",
      "itr: 1110 trn_loss 0.04158003204675184 val_loss 1.7792340942492861\n",
      "itr: 1110 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9757305500953943\n",
      "itr: 1120 trn_loss 0.04144072499526612 val_loss 1.7899201895240644\n",
      "itr: 1120 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9757082210227446\n",
      "itr: 1130 trn_loss 0.04129951247547303 val_loss 1.7960288571217984\n",
      "itr: 1130 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9756543051819142\n",
      "itr: 1140 trn_loss 0.041162858849547075 val_loss 1.801752225777447\n",
      "itr: 1140 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9755623052972677\n",
      "itr: 1150 trn_loss 0.04102497219639704 val_loss 1.8413621213027298\n",
      "itr: 1150 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9754311958364955\n",
      "itr: 1160 trn_loss 0.04088413982042518 val_loss 1.8673060321201498\n",
      "itr: 1160 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9753904926251451\n",
      "itr: 1170 trn_loss 0.04075024946336838 val_loss 1.8913192376544727\n",
      "itr: 1170 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9753780115369929\n",
      "itr: 1180 trn_loss 0.040611708262273956 val_loss 1.9572562597737302\n",
      "itr: 1180 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9753233029297567\n",
      "itr: 1190 trn_loss 0.040482437610862704 val_loss 2.0231439551006787\n",
      "itr: 1190 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9752209216819623\n",
      "itr: 1200 trn_loss 0.040346404463631635 val_loss 2.040928412701878\n",
      "itr: 1200 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9751191166460293\n",
      "itr: 1210 trn_loss 0.040213862840166384 val_loss 2.0515540324683723\n",
      "itr: 1210 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9750564778524439\n",
      "itr: 1220 trn_loss 0.04007982702344461 val_loss 2.0141043796869282\n",
      "itr: 1220 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9750532464394986\n",
      "itr: 1230 trn_loss 0.03994811218879714 val_loss 2.031452286368504\n",
      "itr: 1230 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9750165184924025\n",
      "itr: 1240 trn_loss 0.03981453616248335 val_loss 2.0381050599746957\n",
      "itr: 1240 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9749979591896253\n",
      "itr: 1250 trn_loss 0.03968612168817332 val_loss 2.0800943577110518\n",
      "itr: 1250 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9749377742287622\n",
      "itr: 1260 trn_loss 0.03955542883595092 val_loss 2.0218183580475824\n",
      "itr: 1260 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9749898888060843\n",
      "itr: 1270 trn_loss 0.03942876264239258 val_loss 2.041018711039821\n",
      "itr: 1270 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9749884823610838\n",
      "itr: 1280 trn_loss 0.039298564888289284 val_loss 2.0210607408071155\n",
      "itr: 1280 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9750258642122558\n",
      "itr: 1290 trn_loss 0.039178170583128454 val_loss 2.064477094285303\n",
      "itr: 1290 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9750063703374933\n",
      "itr: 1300 trn_loss 0.03904923404900766 val_loss 2.0981497242503884\n",
      "itr: 1300 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9749695020243707\n",
      "itr: 1310 trn_loss 0.03892291038026812 val_loss 2.160126066904085\n",
      "itr: 1310 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.974912162780033\n",
      "itr: 1320 trn_loss 0.03879777378958732 val_loss 2.164548197419182\n",
      "itr: 1320 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9748847152226565\n",
      "itr: 1330 trn_loss 0.03866985348888341 val_loss 2.2231332889474054\n",
      "itr: 1330 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.974806868919736\n",
      "itr: 1340 trn_loss 0.03854393210243487 val_loss 2.263705435211356\n",
      "itr: 1340 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9747223173579627\n",
      "itr: 1350 trn_loss 0.03842386495344257 val_loss 2.2257810915841723\n",
      "itr: 1350 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9747766537965287\n",
      "itr: 1360 trn_loss 0.03830367922962718 val_loss 2.222756715297764\n",
      "itr: 1360 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9747917428762569\n",
      "itr: 1370 trn_loss 0.038180098515506064 val_loss 2.1881594285488712\n",
      "itr: 1370 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9748632945255209\n",
      "itr: 1380 trn_loss 0.03805778953640146 val_loss 2.1770093582678367\n",
      "itr: 1380 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9749180290969404\n",
      "itr: 1390 trn_loss 0.03793639895742503 val_loss 2.154709363812971\n",
      "itr: 1390 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9749624562745266\n",
      "itr: 1400 trn_loss 0.03781735712203587 val_loss 2.198272345812411\n",
      "itr: 1400 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9749348073439277\n",
      "itr: 1410 trn_loss 0.03769843148775008 val_loss 2.2311979636710504\n",
      "itr: 1410 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9749292471322247\n",
      "itr: 1420 trn_loss 0.037581262926209294 val_loss 2.2642820094670317\n",
      "itr: 1420 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9748952572029378\n",
      "itr: 1430 trn_loss 0.037461000633586955 val_loss 2.317839474795353\n",
      "itr: 1430 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9748115287257623\n",
      "itr: 1440 trn_loss 0.03734528260490019 val_loss 2.312049017534934\n",
      "itr: 1440 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9747410010725311\n",
      "itr: 1450 trn_loss 0.03722609735031956 val_loss 2.756597232389107\n",
      "itr: 1450 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9743635169950174\n",
      "itr: 1460 trn_loss 0.0371097029512984 val_loss 2.5348461731383094\n",
      "itr: 1460 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.974685619379912\n",
      "itr: 1470 trn_loss 0.03699423797626948 val_loss 2.3316462633707125\n",
      "itr: 1470 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9749948353521535\n",
      "itr: 1480 trn_loss 0.03687529203650775 val_loss 2.1484656391469836\n",
      "itr: 1480 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9752924535530069\n",
      "itr: 1490 trn_loss 0.03676188073590736 val_loss 1.9834505103166282\n",
      "itr: 1490 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9755168343058758\n",
      "itr: 1500 trn_loss 0.03664658708372761 val_loss 1.8349858864070814\n",
      "itr: 1500 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9757332668726028\n",
      "itr: 1510 trn_loss 0.036534413810230114 val_loss 1.7013348708082892\n",
      "itr: 1510 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9758894085309846\n",
      "itr: 1520 trn_loss 0.036419662059840256 val_loss 1.5878259571966222\n",
      "itr: 1520 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9760202741106102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 1530 trn_loss 0.0363096801058542 val_loss 1.5022870986977241\n",
      "itr: 1530 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9761477150451913\n",
      "itr: 1540 trn_loss 0.036199784051344365 val_loss 1.468841149448069\n",
      "itr: 1540 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9761899505196607\n",
      "itr: 1550 trn_loss 0.036088390537378064 val_loss 1.4150123960125853\n",
      "itr: 1550 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9762666100983557\n",
      "itr: 1560 trn_loss 0.035975605800229606 val_loss 1.3712072163535722\n",
      "itr: 1560 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9763211078695718\n",
      "itr: 1570 trn_loss 0.03586251087490504 val_loss 1.3317495277267906\n",
      "itr: 1570 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9763749898003575\n",
      "itr: 1580 trn_loss 0.035751352325856904 val_loss 1.3208564601012063\n",
      "itr: 1580 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9764138216251467\n",
      "itr: 1590 trn_loss 0.03563787830281912 val_loss 1.3516015125812286\n",
      "itr: 1590 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9764004607028665\n",
      "itr: 1600 trn_loss 0.03552923127364022 val_loss 1.3746528667361613\n",
      "itr: 1600 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9763642781102867\n",
      "itr: 1610 trn_loss 0.03542233165969608 val_loss 1.4171906866566308\n",
      "itr: 1610 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9762979000619839\n",
      "itr: 1620 trn_loss 0.035312561227211825 val_loss 1.4906912110390635\n",
      "itr: 1620 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9761512026022485\n",
      "itr: 1630 trn_loss 0.03520616243772177 val_loss 1.491410374236915\n",
      "itr: 1630 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9761206219938945\n",
      "itr: 1640 trn_loss 0.0350951783462323 val_loss 1.4401799377138735\n",
      "itr: 1640 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9761462429476575\n",
      "itr: 1650 trn_loss 0.03498539581914271 val_loss 1.4586517277063717\n",
      "itr: 1650 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9761403160672899\n",
      "itr: 1660 trn_loss 0.03487881957915919 val_loss 1.5280440326960496\n",
      "itr: 1660 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9760576865716144\n",
      "itr: 1670 trn_loss 0.03477457636650076 val_loss 1.5388145276693952\n",
      "itr: 1670 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9760654433050779\n",
      "itr: 1680 trn_loss 0.03467696354357256 val_loss 1.547947832535665\n",
      "itr: 1680 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9760627624522771\n",
      "itr: 1690 trn_loss 0.0345671628311264 val_loss 1.547701086334253\n",
      "itr: 1690 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9760458597956114\n",
      "itr: 1700 trn_loss 0.03445990008965541 val_loss 1.522646575330375\n",
      "itr: 1700 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9760547992066753\n",
      "itr: 1710 trn_loss 0.03435632882081424 val_loss 1.5285258077013597\n",
      "itr: 1710 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9760290309616517\n",
      "itr: 1720 trn_loss 0.03424971329445206 val_loss 1.542240516760478\n",
      "itr: 1720 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9760203294302754\n",
      "itr: 1730 trn_loss 0.03414532883773586 val_loss 1.5463925389217106\n",
      "itr: 1730 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9759931742262006\n",
      "itr: 1740 trn_loss 0.03404398539489166 val_loss 1.5798323918372483\n",
      "itr: 1740 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9759204249779427\n",
      "itr: 1750 trn_loss 0.03394129890058286 val_loss 1.6183358641555987\n",
      "itr: 1750 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9758646125674287\n",
      "itr: 1760 trn_loss 0.0338384976744155 val_loss 1.6619674276194945\n",
      "itr: 1760 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9758288772475755\n",
      "itr: 1770 trn_loss 0.03373482155024912 val_loss 1.6803005973293161\n",
      "itr: 1770 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9757918815230163\n",
      "itr: 1780 trn_loss 0.03363369765561402 val_loss 1.6830370938593546\n",
      "itr: 1780 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9757730812205225\n",
      "itr: 1790 trn_loss 0.03353621808435808 val_loss 1.690047119481659\n",
      "itr: 1790 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9757078513836877\n",
      "itr: 1800 trn_loss 0.033432777573194385 val_loss 1.679340566179123\n",
      "itr: 1800 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9756394826176181\n",
      "itr: 1810 trn_loss 0.03333289142251997 val_loss 1.7545200511642993\n",
      "itr: 1810 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9754716696860565\n",
      "itr: 1820 trn_loss 0.03323172452578582 val_loss 1.741528885796252\n",
      "itr: 1820 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9754462429155862\n",
      "itr: 1830 trn_loss 0.03313111726098229 val_loss 1.6926233564924449\n",
      "itr: 1830 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9754764963629803\n",
      "itr: 1840 trn_loss 0.03303026698216285 val_loss 1.732733779708557\n",
      "itr: 1840 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9754602488377357\n",
      "itr: 1850 trn_loss 0.03293057035162 val_loss 1.7748960497021788\n",
      "itr: 1850 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9754069784133433\n",
      "itr: 1860 trn_loss 0.032832401748715996 val_loss 1.8118634321129423\n",
      "itr: 1860 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9753445391817808\n",
      "itr: 1870 trn_loss 0.03273500100927539 val_loss 1.8403884021493289\n",
      "itr: 1870 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9752931778100657\n",
      "itr: 1880 trn_loss 0.03263804020595543 val_loss 1.870633099799081\n",
      "itr: 1880 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9752469525755222\n",
      "itr: 1890 trn_loss 0.032539718269783934 val_loss 1.92144208043044\n",
      "itr: 1890 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.975205349864433\n",
      "itr: 1900 trn_loss 0.03244081309729921 val_loss 1.955427237354742\n",
      "itr: 1900 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9751485835986164\n",
      "itr: 1910 trn_loss 0.03234448962575579 val_loss 1.9858070027855277\n",
      "itr: 1910 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9750733361968542\n",
      "itr: 1920 trn_loss 0.032252878474668185 val_loss 2.0479974817061937\n",
      "itr: 1920 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9750056135352682\n",
      "itr: 1930 trn_loss 0.03215304130215809 val_loss 2.0221016112707124\n",
      "itr: 1930 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9749978066411225\n",
      "itr: 1940 trn_loss 0.03205633792191451 val_loss 1.9649111006029307\n",
      "itr: 1940 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9750584138268179\n",
      "itr: 1950 trn_loss 0.0319616963251849 val_loss 1.9011993622810714\n",
      "itr: 1950 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9751177882701707\n",
      "itr: 1960 trn_loss 0.0318647879077953 val_loss 1.8725866272477274\n",
      "itr: 1960 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9751808871821064\n",
      "itr: 1970 trn_loss 0.031773757971486474 val_loss 1.9721767718578547\n",
      "itr: 1970 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9751169052716044\n",
      "itr: 1980 trn_loss 0.03167804088973056 val_loss 1.8873311592625233\n",
      "itr: 1980 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9751849264200879\n",
      "itr: 1990 trn_loss 0.03158042676689718 val_loss 1.8432682079213847\n",
      "itr: 1990 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9752171597149688\n",
      "itr: 2000 trn_loss 0.03148925640338868 val_loss 1.825588642549931\n",
      "itr: 2000 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9752316738307523\n",
      "itr: 2010 trn_loss 0.03139306652310072 val_loss 1.9966015031866005\n",
      "itr: 2010 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9750756560391229\n",
      "itr: 2020 trn_loss 0.03130153065453694 val_loss 2.0117028484581505\n",
      "itr: 2020 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9750318591558376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 2030 trn_loss 0.031207020136490127 val_loss 1.9900022324775395\n",
      "itr: 2030 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9750697372642254\n",
      "itr: 2040 trn_loss 0.031116373016544213 val_loss 2.0044199684430426\n",
      "itr: 2040 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9750748418230202\n",
      "itr: 2050 trn_loss 0.031023927881606645 val_loss 2.0456840329619586\n",
      "itr: 2050 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.975055278163408\n",
      "itr: 2060 trn_loss 0.030933296001154006 val_loss 2.0610839962886875\n",
      "itr: 2060 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9750231809806121\n",
      "itr: 2070 trn_loss 0.030839292019124664 val_loss 2.0281197345191635\n",
      "itr: 2070 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9750329411677683\n",
      "itr: 2080 trn_loss 0.030743547657942856 val_loss 2.006821320885515\n",
      "itr: 2080 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9750127395974545\n",
      "itr: 2090 trn_loss 0.03065081341008638 val_loss 1.9627038059539461\n",
      "itr: 2090 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9750428677487626\n",
      "itr: 2100 trn_loss 0.030562340765406797 val_loss 1.9618363575256048\n",
      "itr: 2100 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9750313354332675\n",
      "itr: 2110 trn_loss 0.030473980342334776 val_loss 1.969292133699314\n",
      "itr: 2110 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9750112944364038\n",
      "itr: 2120 trn_loss 0.030383088411795527 val_loss 2.0331744637940434\n",
      "itr: 2120 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9749159622358817\n",
      "itr: 2130 trn_loss 0.03029246901644059 val_loss 2.0558741110730865\n",
      "itr: 2130 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.974863976970393\n",
      "itr: 2140 trn_loss 0.03020303011228006 val_loss 2.029060723021808\n",
      "itr: 2140 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9748751617089616\n",
      "itr: 2150 trn_loss 0.030116325819588903 val_loss 1.9831373229341354\n",
      "itr: 2150 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9749142137124277\n",
      "itr: 2160 trn_loss 0.030022789974327578 val_loss 2.003312665347753\n",
      "itr: 2160 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9748672372359756\n",
      "itr: 2170 trn_loss 0.029931391215424208 val_loss 2.0471239550034688\n",
      "itr: 2170 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9748104625575593\n",
      "itr: 2180 trn_loss 0.029840719508447196 val_loss 2.1189805683303313\n",
      "itr: 2180 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9747303796082304\n",
      "itr: 2190 trn_loss 0.029757428519645628 val_loss 2.2580115094907676\n",
      "itr: 2190 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9745906775238722\n",
      "itr: 2200 trn_loss 0.029672967114136712 val_loss 2.200307556551182\n",
      "itr: 2200 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9746340261437841\n",
      "itr: 2210 trn_loss 0.029580275250733336 val_loss 2.0819264905704413\n",
      "itr: 2210 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9747455012683585\n",
      "itr: 2220 trn_loss 0.029495963511078235 val_loss 2.1336971870410464\n",
      "itr: 2220 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.974686410297559\n",
      "itr: 2230 trn_loss 0.02940470082340315 val_loss 2.1376889423741123\n",
      "itr: 2230 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9746477183129845\n",
      "itr: 2240 trn_loss 0.029318818151591253 val_loss 2.1890487083387273\n",
      "itr: 2240 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9745645919227414\n",
      "itr: 2250 trn_loss 0.029232109869271748 val_loss 2.1385163970522787\n",
      "itr: 2250 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9746008871898484\n",
      "itr: 2260 trn_loss 0.029145899608974117 val_loss 2.1710618711470997\n",
      "itr: 2260 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9745562576269\n",
      "itr: 2270 trn_loss 0.029058898729863187 val_loss 2.1217907408141583\n",
      "itr: 2270 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9745933863235912\n",
      "itr: 2280 trn_loss 0.028974711529640214 val_loss 2.115586889320328\n",
      "itr: 2280 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9745929824751676\n",
      "itr: 2290 trn_loss 0.02888480164045454 val_loss 2.11071064400707\n",
      "itr: 2290 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9745829570986683\n",
      "itr: 2300 trn_loss 0.028802059002795914 val_loss 2.1199511974576204\n",
      "itr: 2300 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9745352866081465\n",
      "itr: 2310 trn_loss 0.028718407354125473 val_loss 2.0794857641628166\n",
      "itr: 2310 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9745696784700217\n",
      "itr: 2320 trn_loss 0.028633676212519648 val_loss 2.1747279197487934\n",
      "itr: 2320 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9744701983015474\n",
      "itr: 2330 trn_loss 0.02854592855600518 val_loss 2.2793099907499883\n",
      "itr: 2330 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9744579614532652\n",
      "itr: 2340 trn_loss 0.028462560052011622 val_loss 2.1324391687627733\n",
      "itr: 2340 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9746401865481731\n",
      "itr: 2350 trn_loss 0.028375641330719126 val_loss 2.0364379100247714\n",
      "itr: 2350 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9748186790227351\n",
      "itr: 2360 trn_loss 0.02829187496173403 val_loss 1.9337486158827983\n",
      "itr: 2360 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9750469556402674\n",
      "itr: 2370 trn_loss 0.028211009363379492 val_loss 1.8650449512320797\n",
      "itr: 2370 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9752282527939836\n",
      "itr: 2380 trn_loss 0.02812662221582627 val_loss 1.846054041199509\n",
      "itr: 2380 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9753189529052102\n",
      "itr: 2390 trn_loss 0.028040657232965054 val_loss 1.811283043455443\n",
      "itr: 2390 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9753571073774151\n",
      "itr: 2400 trn_loss 0.02796140142922293 val_loss 1.8456263882798205\n",
      "itr: 2400 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9753141510990547\n",
      "itr: 2410 trn_loss 0.02787836928498002 val_loss 1.9518942662670242\n",
      "itr: 2410 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9751885332322675\n",
      "itr: 2420 trn_loss 0.02779081642361523 val_loss 1.9167216743761921\n",
      "itr: 2420 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9752107439330123\n",
      "itr: 2430 trn_loss 0.027711208999880405 val_loss 1.989175974643346\n",
      "itr: 2430 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9751002947590561\n",
      "itr: 2440 trn_loss 0.02763001383184289 val_loss 2.049261420353755\n",
      "itr: 2440 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9749912285895775\n",
      "itr: 2450 trn_loss 0.027550139676542025 val_loss 1.9818703258495074\n",
      "itr: 2450 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9750573215566543\n",
      "itr: 2460 trn_loss 0.02746425097091163 val_loss 1.9953585318738707\n",
      "itr: 2460 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9750588337495149\n",
      "itr: 2470 trn_loss 0.02738166456169293 val_loss 2.0156050072921357\n",
      "itr: 2470 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9750167190951902\n",
      "itr: 2480 trn_loss 0.0273004427343684 val_loss 1.9640308809411264\n",
      "itr: 2480 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9750319534471152\n",
      "itr: 2490 trn_loss 0.02721855266427908 val_loss 1.9519478016017442\n",
      "itr: 2490 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9749973547992574\n",
      "itr: 2500 trn_loss 0.027138341617951138 val_loss 1.966846662722698\n",
      "itr: 2500 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9749807118657947\n",
      "itr: 2510 trn_loss 0.027059307337324277 val_loss 1.9371599298381845\n",
      "itr: 2510 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.975043028529023\n",
      "itr: 2520 trn_loss 0.026979162518536264 val_loss 1.9745014853773153\n",
      "itr: 2520 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.97504114204842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 2530 trn_loss 0.02690056026354009 val_loss 1.935331304147628\n",
      "itr: 2530 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9750974156933858\n",
      "itr: 2540 trn_loss 0.02681934791150594 val_loss 1.8219561987591868\n",
      "itr: 2540 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9752591709921808\n",
      "itr: 2550 trn_loss 0.02673735122399446 val_loss 1.7276929688578622\n",
      "itr: 2550 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9753950888481782\n",
      "itr: 2560 trn_loss 0.026656988435468654 val_loss 1.6598965351694026\n",
      "itr: 2560 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9754449535519224\n",
      "itr: 2570 trn_loss 0.026575303616528184 val_loss 1.5978446386676906\n",
      "itr: 2570 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.975494659761519\n",
      "itr: 2580 trn_loss 0.026499452531479742 val_loss 1.6292559131165039\n",
      "itr: 2580 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9754621000468111\n",
      "itr: 2590 trn_loss 0.026421125510776897 val_loss 1.6752876124542677\n",
      "itr: 2590 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9754134724777379\n",
      "itr: 2600 trn_loss 0.026340032183370252 val_loss 1.676664049782907\n",
      "itr: 2600 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9754276791430806\n",
      "itr: 2610 trn_loss 0.026260648432912034 val_loss 1.6489063519464926\n",
      "itr: 2610 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9754742848173346\n",
      "itr: 2620 trn_loss 0.02618277686558958 val_loss 1.5592418746555146\n",
      "itr: 2620 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9755886912908165\n",
      "itr: 2630 trn_loss 0.026108014900241835 val_loss 1.4988631579045322\n",
      "itr: 2630 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9756819952040322\n",
      "itr: 2640 trn_loss 0.026030297977664787 val_loss 1.4977655803648116\n",
      "itr: 2640 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9756886734225817\n",
      "itr: 2650 trn_loss 0.025953459821853483 val_loss 1.5052833314675822\n",
      "itr: 2650 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9756560361676039\n",
      "itr: 2660 trn_loss 0.025875678750892507 val_loss 1.5033371826355517\n",
      "itr: 2660 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9756604823135694\n",
      "itr: 2670 trn_loss 0.02579742901354449 val_loss 1.4478964430402095\n",
      "itr: 2670 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9757224553224468\n",
      "itr: 2680 trn_loss 0.02572091334248526 val_loss 1.428915311900709\n",
      "itr: 2680 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9757299214658461\n",
      "itr: 2690 trn_loss 0.025645017222568605 val_loss 1.4341414064239864\n",
      "itr: 2690 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9757221451452962\n",
      "itr: 2700 trn_loss 0.025567165615241385 val_loss 1.44635148853702\n",
      "itr: 2700 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9756861607180469\n",
      "itr: 2710 trn_loss 0.025491511897675682 val_loss 1.4821083665845631\n",
      "itr: 2710 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9756392848443777\n",
      "itr: 2720 trn_loss 0.02541374446053142 val_loss 1.428556816663549\n",
      "itr: 2720 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9756937156872563\n",
      "itr: 2730 trn_loss 0.025339056521488573 val_loss 1.4290772231395892\n",
      "itr: 2730 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9756847319683384\n",
      "itr: 2740 trn_loss 0.025259961242244236 val_loss 1.459396906373726\n",
      "itr: 2740 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.975647660882558\n",
      "itr: 2750 trn_loss 0.02518866790879637 val_loss 1.5253674396972299\n",
      "itr: 2750 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9755659873407653\n",
      "itr: 2760 trn_loss 0.025109617355712367 val_loss 1.51586683473889\n",
      "itr: 2760 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9755842663456414\n",
      "itr: 2770 trn_loss 0.025031913051809653 val_loss 1.4544214357307907\n",
      "itr: 2770 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9756586889275385\n",
      "itr: 2780 trn_loss 0.024958975454492487 val_loss 1.4292345836196563\n",
      "itr: 2780 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9756870215995734\n",
      "itr: 2790 trn_loss 0.02488219092575622 val_loss 1.4285017962247928\n",
      "itr: 2790 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.975678707289424\n",
      "itr: 2800 trn_loss 0.024808845180042016 val_loss 1.424082438497913\n",
      "itr: 2800 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.975666390473598\n",
      "itr: 2810 trn_loss 0.024735547947101925 val_loss 1.4064070108674513\n",
      "itr: 2810 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.975674629165191\n",
      "itr: 2820 trn_loss 0.024662723549018262 val_loss 1.4587190738149773\n",
      "itr: 2820 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9756337344230341\n",
      "itr: 2830 trn_loss 0.024590567399959167 val_loss 1.3763169340039751\n",
      "itr: 2830 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9757418578488644\n",
      "itr: 2840 trn_loss 0.02451293331953021 val_loss 1.284182157374513\n",
      "itr: 2840 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.975887478496702\n",
      "itr: 2850 trn_loss 0.024438107977317835 val_loss 1.233044713369392\n",
      "itr: 2850 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9759750614518566\n",
      "itr: 2860 trn_loss 0.02436373199008764 val_loss 1.1994559016508306\n",
      "itr: 2860 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9760152384598234\n",
      "itr: 2870 trn_loss 0.0242900965660923 val_loss 1.2387086075558464\n",
      "itr: 2870 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9759451167248946\n",
      "itr: 2880 trn_loss 0.024220584207463296 val_loss 1.319649152935821\n",
      "itr: 2880 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9758288636621769\n",
      "itr: 2890 trn_loss 0.024147672427849102 val_loss 1.2599320468139157\n",
      "itr: 2890 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9759174741640928\n",
      "itr: 2900 trn_loss 0.024073764385936974 val_loss 1.2751931956557785\n",
      "itr: 2900 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9759006044866362\n",
      "itr: 2910 trn_loss 0.023998915934537887 val_loss 1.2044494162036499\n",
      "itr: 2910 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9759868748427974\n",
      "itr: 2920 trn_loss 0.023926412131692623 val_loss 1.181410009565966\n",
      "itr: 2920 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9760065466858341\n",
      "itr: 2930 trn_loss 0.023853561546475378 val_loss 1.1088367386785298\n",
      "itr: 2930 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9760677269724662\n",
      "itr: 2940 trn_loss 0.023782884120071665 val_loss 1.0912585929745868\n",
      "itr: 2940 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9760841415787628\n",
      "itr: 2950 trn_loss 0.023710034271172256 val_loss 1.0388677247507365\n",
      "itr: 2950 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9761520582257114\n",
      "itr: 2960 trn_loss 0.023636348649673768 val_loss 0.9918270940908485\n",
      "itr: 2960 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9762035212950471\n",
      "itr: 2970 trn_loss 0.023563073343111713 val_loss 0.9695110471840525\n",
      "itr: 2970 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9762498380574491\n",
      "itr: 2980 trn_loss 0.023493265099065526 val_loss 0.949640925389155\n",
      "itr: 2980 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9762818612306929\n",
      "itr: 2990 trn_loss 0.0234226536197496 val_loss 0.9113984605624893\n",
      "itr: 2990 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9763203439995305\n",
      "itr: 3000 trn_loss 0.02334897469914234 val_loss 0.8844074339528041\n",
      "itr: 3000 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9763549784914842\n",
      "itr: 3010 trn_loss 0.023278071802329863 val_loss 0.8928003726177653\n",
      "itr: 3010 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9763040202942066\n",
      "itr: 3020 trn_loss 0.023211221293256312 val_loss 0.8836402551329054\n",
      "itr: 3020 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9762968055683291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 3030 trn_loss 0.023139932125835663 val_loss 0.8477648725505995\n",
      "itr: 3030 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.976367607618384\n",
      "itr: 3040 trn_loss 0.023069866612560188 val_loss 0.8351955560204084\n",
      "itr: 3040 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9764216675505154\n",
      "itr: 3050 trn_loss 0.022998144716923 val_loss 0.8281046879908468\n",
      "itr: 3050 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9764606595765155\n",
      "itr: 3060 trn_loss 0.022928260164128335 val_loss 0.8013876139480792\n",
      "itr: 3060 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9765054143128337\n",
      "itr: 3070 trn_loss 16.33170182179194 val_loss 2.22381747895708\n",
      "itr: 3070 trn_acc 0.9253548593749998 trn_single_acc 0.9903347851461171 val_acc 0.9436954519144483\n",
      "itr: 3080 trn_loss 11.738968238524993 val_loss 2.063551591046724\n",
      "itr: 3080 trn_acc 0.85536190364958 trn_single_acc 0.9828188036873883 val_acc 0.9470602090053368\n",
      "itr: 3090 trn_loss 4.267421674323958 val_loss 3.5395902689171495\n",
      "itr: 3090 trn_acc 0.9410373610605021 trn_single_acc 0.992775778882624 val_acc 0.9415570868455716\n",
      "itr: 3100 trn_loss 7.982390239269349 val_loss 3.2816950704937575\n",
      "itr: 3100 trn_acc 0.8180471583382091 trn_single_acc 0.9770123118538834 val_acc 0.9409859216275824\n",
      "itr: 3110 trn_loss 2.8260857158675465 val_loss 3.0022654169457015\n",
      "itr: 3110 trn_acc 0.9359516224835419 trn_single_acc 0.9919174285107228 val_acc 0.9446167978104664\n",
      "itr: 3120 trn_loss 1.003585366379129 val_loss 2.7565961331418327\n",
      "itr: 3120 trn_acc 0.9776677116366254 trn_single_acc 0.9971817815811221 val_acc 0.94837733797342\n",
      "itr: 3130 trn_loss 0.36632224408728165 val_loss 2.5282003417871453\n",
      "itr: 3130 trn_acc 0.992213212529595 trn_single_acc 0.9990173479978445 val_acc 0.9515637578854897\n",
      "itr: 3140 trn_loss 0.23017904989150395 val_loss 2.3194380344684773\n",
      "itr: 3140 trn_acc 0.9913224150914289 trn_single_acc 0.9992706729794668 val_acc 0.9542624553105181\n",
      "itr: 3150 trn_loss 0.09611628941960842 val_loss 2.1326620258452764\n",
      "itr: 3150 trn_acc 0.996974313230244 trn_single_acc 0.9997456993921576 val_acc 0.9567444205338609\n",
      "itr: 3160 trn_loss 0.04922953912577608 val_loss 1.9675062874760274\n",
      "itr: 3160 trn_acc 0.9989450082568901 trn_single_acc 0.9999113308607409 val_acc 0.9591569376040863\n",
      "itr: 3170 trn_loss 0.03276827824175796 val_loss 1.819425894004647\n",
      "itr: 3170 trn_acc 0.9996321471246941 trn_single_acc 0.999969082982838 val_acc 0.9610769932314187\n",
      "itr: 3180 trn_loss 0.02696318728897109 val_loss 1.6863438694321014\n",
      "itr: 3180 trn_acc 0.999871737633252 trn_single_acc 0.9999892199026834 val_acc 0.9628340290347722\n",
      "itr: 3190 trn_loss 0.0248886091974146 val_loss 1.5667051531651226\n",
      "itr: 3190 trn_acc 0.9999552776780387 trn_single_acc 0.9999962412124834 val_acc 0.9644491749727714\n",
      "itr: 3200 trn_loss 0.024117015893466952 val_loss 1.4589354238908239\n",
      "itr: 3200 trn_acc 0.9999844062905409 trn_single_acc 0.999998689391832 val_acc 0.965931792055725\n",
      "itr: 3210 trn_loss 0.023820134173963996 val_loss 1.3619123824239445\n",
      "itr: 3210 trn_acc 0.9999945628097104 trn_single_acc 0.9999995430191883 val_acc 0.9672709813670745\n",
      "itr: 3220 trn_loss 0.02367959917230482 val_loss 1.2746070618451244\n",
      "itr: 3220 trn_acc 0.9999981041689713 trn_single_acc 0.9999998406606434 val_acc 0.968476251747289\n",
      "itr: 3230 trn_loss 0.02359418444649485 val_loss 1.1954125876743145\n",
      "itr: 3230 trn_acc 0.9999993389645941 trn_single_acc 0.9999999444418016 val_acc 0.9695851468915451\n",
      "itr: 3240 trn_loss 0.02353220123930132 val_loss 1.123617346482147\n",
      "itr: 3240 trn_acc 0.9999997695112057 trn_single_acc 0.999999980628054 val_acc 0.970631462085966\n",
      "itr: 3250 trn_loss 0.023482517958899534 val_loss 1.0590235072940613\n",
      "itr: 3250 trn_acc 0.9999999196335267 trn_single_acc 0.99999999324542 val_acc 0.971582807673863\n",
      "itr: 3260 trn_loss 0.023432375665344275 val_loss 1.0010237138184928\n",
      "itr: 3260 trn_acc 0.9999999719779433 trn_single_acc 0.9999999976448235 val_acc 0.9724341907267434\n",
      "itr: 3270 trn_loss 0.02338799885360789 val_loss 0.9488342530172785\n",
      "itr: 3270 trn_acc 0.999999990229313 trn_single_acc 0.9999999991788007 val_acc 0.9731907735614178\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f58b1a16a3a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mcmd_lengths\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mcmd_lengths_np\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_samples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         }\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercent_fully_correct\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meval_itr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mval_loss_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_itr = -1\n",
    "bs = 64\n",
    "for itr in range(10000):\n",
    "    samples = np.random.choice(trn_samples, size = bs, replace = False)\n",
    "    if True:#itr == 0:\n",
    "        trn_feed_dict = {\n",
    "            cmd_ind : cmd_np[samples],\n",
    "            act_ind : act_np[samples],\n",
    "            mask_ph : mask_np[samples],\n",
    "            act_lengths : np.clip(struct_np[samples], a_min = 1, a_max = None),\n",
    "            cmd_lengths : cmd_lengths_np[samples],\n",
    "        }\n",
    "        \n",
    "    trn_feed_dict[learning_rate] = .01 / (np.sqrt(itr + 10))\n",
    "    _, trn_loss, acc_trn_single, acc_trn = sess.run(\n",
    "        [optimizer, loss, percent_correct, percent_fully_correct], trn_feed_dict)\n",
    "    if itr == 0:\n",
    "        trn_loss_avg = trn_loss\n",
    "        acc_trn_avg = acc_trn\n",
    "        acc_trn_single_avg = acc_trn_single\n",
    "    else:\n",
    "        trn_loss_avg = trn_loss_avg * .9 + trn_loss * .1\n",
    "        acc_trn_avg = acc_trn_avg * .9 + acc_trn * .1\n",
    "        acc_trn_single_avg = acc_trn_single_avg * .9 + acc_trn_single * .1\n",
    "    if itr % 10 == 0 and itr > 0:\n",
    "        # val_samples = np.random.choice(val_samples_all, size = bs, replace = False)\n",
    "        eval_itr += 1\n",
    "        val_feed_dict = {\n",
    "            cmd_ind : cmd_np[val_samples],\n",
    "            act_ind : act_np[val_samples],\n",
    "            mask_ph : mask_np[val_samples],\n",
    "            act_lengths : np.clip(struct_np[val_samples], a_min = 1, a_max = None),\n",
    "            cmd_lengths : cmd_lengths_np[val_samples]\n",
    "        }\n",
    "        val_loss, acc_val = sess.run([loss, percent_fully_correct], val_feed_dict)\n",
    "        if eval_itr == 0:\n",
    "            val_loss_avg = val_loss\n",
    "            acc_val_avg = acc_val\n",
    "        else:\n",
    "            val_loss_avg = val_loss_avg * .9 + val_loss * .1\n",
    "            acc_val_avg = acc_val_avg * .9 + acc_val * .1\n",
    "        print('itr:', itr, 'trn_loss', trn_loss_avg, 'val_loss', val_loss_avg)\n",
    "        print('itr:', itr, 'trn_acc', acc_trn_avg, \n",
    "              'trn_single_acc', acc_trn_single_avg, 'val_acc', acc_val_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_percent.shape, percent_fully_correct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "10, None, 7, 9, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_np.shape, act_np.shape, mask_np.shape, struct_np.shape, cmd_lengths_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cmd_ind.shape, act_ind.shape, mask_ph.shape, act_lengths.shape, cmd_lengths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_samples.shape, val_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "act_presoftmax = tf.stack(action_probabilities_presoftmax, 1)[:, :, :-1, :]\n",
    "#batch, subprogram, timestep, action_selection\n",
    "logprobabilities = tf.nn.log_softmax(act_presoftmax, -1)\n",
    "act_presoftmax_flat = tf.reshape(act_presoftmax, [-1, 9, num_act])\n",
    "mask_ph_flat = tf.reshape(mask_ph, [-1, max_actions_per_subprogram])\n",
    "act_ind_flat = tf.reshape(act_ind, [-1, max_actions_per_subprogram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "act_presoftmax_flat = tf.reshape(act_presoftmax, [-1, 9, num_act])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_actions_per_subprogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess.run(act_presoftmax, feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(*actions_ind[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subprogram_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprogram_last_layer[:,cmd_lengths,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.gather(\n",
    "    encoding_last_layer,\n",
    "    [1,2],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.gather_nd(\n",
    "    encoding_last_layer,\n",
    "    np.array([[0,1,2,3,4], [1,4,3,2,5]]).T,\n",
    "    name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmd_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_command(sub_cmd, num_repeat):\n",
    "    return sub_cmd * num_repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_command(cmd):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uni_commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uni_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uni_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
