{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import common dependencies\n",
    "import pandas as pd  # noqa\n",
    "import numpy as np\n",
    "import matplotlib  # noqa\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime  # noqa\n",
    "import PIL  # noqa\n",
    "import glob  # noqa\n",
    "import pickle  # noqa\n",
    "from pathlib import Path  # noqa\n",
    "from scipy import misc  # noqa\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import pdb\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "TRADE_COST_FRAC = .003\n",
    "EPSILON = 1e-10\n",
    "ADV_MULT = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uni_tokens = set()\n",
    "uni_commands = set()\n",
    "uni_actions = set()\n",
    "fname = 'tasks_with_length_tags.txt'\n",
    "with open(fname) as f:\n",
    "    content = f.readlines()\n",
    "content2 = [c.split(' ') for c in content]\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "commands = []\n",
    "actions = []\n",
    "content = [l.replace('\\n', '') for l in content]\n",
    "commands = [x.split(':::')[1].split(' ')[1:-1] for x in content]\n",
    "actions = [x.split(':::')[2].split(' ')[1:-2] for x in content]\n",
    "structures = [x.split(':::')[3].split(' ')[2:] for x in content]\n",
    "\n",
    "structures = [[int(l) for l in program] for program in structures]\n",
    "#actions = [[wd.replace('\\n', '') for wd in res] for res in actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 10, 49, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_actions_per_subprogram = max([max([s for s in struct]) for struct in structures]) + 1\n",
    "max_num_subprograms = max([len(s) for s in structures]) + 1\n",
    "max_cmd_len = max([len(s) for s in commands]) + 1\n",
    "max_act_len = max([len(a) for a in actions]) + 1\n",
    "cmd_lengths_list = [len(s)+1 for s in commands]\n",
    "cmd_lengths_np = np.array(cmd_lengths_list)\n",
    "max_num_subprograms, max_cmd_len, max_act_len, max_actions_per_subprogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_fmap_invmap(unique, num_unique):\n",
    "    fmap = dict(zip(unique, range(num_unique)))\n",
    "    invmap = dict(zip(range(num_unique), unique))\n",
    "    return fmap, invmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for li in content2:\n",
    "    for wd in li:\n",
    "        uni_tokens.add(wd)\n",
    "for li in commands:\n",
    "    for wd in li:\n",
    "        uni_commands.add(wd)\n",
    "for li in actions:\n",
    "    for wd in li:\n",
    "        uni_actions.add(wd)\n",
    "uni_commands.add('end_command')\n",
    "uni_actions.add('end_subprogram')\n",
    "uni_actions.add('end_action')\n",
    "num_cmd = len(uni_commands)\n",
    "num_act = len(uni_actions)\n",
    "command_map, command_invmap = build_fmap_invmap(uni_commands, num_cmd)\n",
    "action_map, action_invmap = build_fmap_invmap(uni_actions, num_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dense_scaled(prev_layer, layer_size, name=None, reuse=False, scale=1.0):\n",
    "    output = tf.layers.dense(prev_layer, layer_size, reuse=reuse) * scale\n",
    "    return output\n",
    "\n",
    "\n",
    "def dense_relu(dense_input, layer_size, scale=1.0):\n",
    "    dense = dense_scaled(dense_input, layer_size, scale=scale)\n",
    "    output = tf.nn.leaky_relu(dense)\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_grad_norm(opt_fcn, loss):\n",
    "    gvs = opt_fcn.compute_gradients(loss)\n",
    "    grad_norm = tf.sqrt(tf.reduce_sum(\n",
    "        [tf.reduce_sum(tf.square(grad)) for grad, var in gvs if grad is not None]))\n",
    "    return grad_norm\n",
    "\n",
    "\n",
    "def apply_clipped_optimizer(opt_fcn, loss, clip_norm=.1, clip_single=.03, clip_global_norm=False):\n",
    "    gvs = opt_fcn.compute_gradients(loss)\n",
    "\n",
    "    if clip_global_norm:\n",
    "        gs, vs = zip(*[(g, v) for g, v in gvs if g is not None])\n",
    "        capped_gs, grad_norm_total = tf.clip_by_global_norm([g for g in gs], clip_norm)\n",
    "        capped_gvs = list(zip(capped_gs, vs))\n",
    "    else:\n",
    "        grad_norm_total = tf.sqrt(\n",
    "            tf.reduce_sum([tf.reduce_sum(tf.square(grad)) for grad, var in gvs if grad is not None]))\n",
    "        capped_gvs = [(tf.clip_by_value(grad, -1 * clip_single, clip_single), var)\n",
    "                      for grad, var in gvs if grad is not None]\n",
    "        capped_gvs = [(tf.clip_by_norm(grad, clip_norm), var) for grad, var in capped_gvs if grad is not None]\n",
    "\n",
    "    optimizer = opt_fcn.apply_gradients(capped_gvs)\n",
    "\n",
    "    return optimizer, grad_norm_total\n",
    "\n",
    "\n",
    "def mlp(x, hidden_sizes, output_size=None, name='', reuse=False):\n",
    "    prev_layer = x\n",
    "\n",
    "    for idx, l in enumerate(hidden_sizes):\n",
    "        dense = dense_scaled(prev_layer, l, name='mlp' + name + '_' + str(idx))\n",
    "        prev_layer = tf.nn.leaky_relu(dense)\n",
    "\n",
    "    output = prev_layer\n",
    "\n",
    "    if output_size is not None:\n",
    "        output = dense_scaled(prev_layer, output_size, name='mlp' + name + 'final')\n",
    "\n",
    "    return output\n",
    "\n",
    "def mlp_with_adversaries(x, hidden_sizes, output_size=None, name='', reuse=False):\n",
    "    prev_layer = x\n",
    "    adv_phs = []\n",
    "    for idx, l in enumerate(hidden_sizes):\n",
    "        \n",
    "        adversary = tf.placeholder_with_default(tf.zeros_like(prev_layer), prev_layer.shape)\n",
    "        prev_layer = prev_layer + adversary\n",
    "        adv_phs.append(adversary)\n",
    "        \n",
    "        dense = dense_scaled(prev_layer, l, name='mlp' + name + '_' + str(idx))\n",
    "        prev_layer = tf.nn.leaky_relu(dense)\n",
    "\n",
    "    output = prev_layer\n",
    "\n",
    "    if output_size is not None:\n",
    "        output = dense_scaled(prev_layer, output_size, name='mlp' + name + 'final')\n",
    "\n",
    "    return output, adv_phs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "commands_ind = [[command_map[c] for c in cmd] + [0] * (max_cmd_len - len(cmd)) for cmd in commands]\n",
    "actions_ind = [[action_map[a] for a in act] + [0] * (max_act_len - len(act)) for act in actions]\n",
    "cmd_np = np.array(commands_ind)\n",
    "actions_structured = []\n",
    "mask_structured = []\n",
    "for row in range(len(structures)):\n",
    "    mask_row = []\n",
    "    action_row = []\n",
    "    act = actions_ind[row]\n",
    "    struct = structures[row]\n",
    "    start = 0\n",
    "    for step in struct:\n",
    "        end = start + step\n",
    "        a = act[start:end]\n",
    "        padding = max_actions_per_subprogram - step - 1\n",
    "        action_row.append(a + [action_map['end_action']] + [0] * padding)\n",
    "        start = end\n",
    "    actions_structured.append(\n",
    "        action_row + [[action_map['end_subprogram']] + [0] * (max_actions_per_subprogram - 1)] +\n",
    "        [[0] * max_actions_per_subprogram] * (max_num_subprograms - len(struct) - 1)\n",
    "    )\n",
    "act_np = np.array(actions_structured)\n",
    "struct_padded = [[sa + 1 for sa in s] + [1] + [0] * (max_num_subprograms - len(s) - 1) for s in structures]\n",
    "struct_np = np.array(struct_padded)\n",
    "\n",
    "mask_list = [[np.concatenate((np.ones(st), np.zeros(max_actions_per_subprogram - st)), 0) \n",
    "              for st in s] for s in struct_np]\n",
    "mask_np = np.array(mask_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\clip_ops.py:110: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "default_sizes = 128\n",
    "size_emb = 64\n",
    "num_layers_encoder = 6\n",
    "hidden_filters = 256\n",
    "num_layers_subprogram = 3\n",
    "hidden_filters_subprogram = 256\n",
    "init_mag = 1e-2\n",
    "cmd_mat = tf.Variable(init_mag*tf.random_normal([num_cmd, size_emb]))\n",
    "act_mat = tf.Variable(init_mag*tf.random_normal([num_act, size_emb]))\n",
    "act_st_emb = tf.Variable(init_mag*tf.random_normal([size_emb]))\n",
    "global_bs = None\n",
    "global_time_len = 7\n",
    "action_lengths = None\n",
    "max_num_actions= None\n",
    "# global_bs = 8\n",
    "global_time_len = 7\n",
    "max_num_actions = 9\n",
    "output_keep_prob = tf.placeholder_with_default(1.0, ())\n",
    "state_keep_prob = tf.placeholder_with_default(1.0, ())\n",
    "cmd_ind = tf.placeholder(tf.int32, shape=(global_bs, 10,))\n",
    "act_ind = tf.placeholder(tf.int32, shape=(global_bs, global_time_len, 9))\n",
    "mask_ph = tf.placeholder(tf.float32, shape=(global_bs, global_time_len, 9))\n",
    "cmd_lengths = tf.placeholder(tf.int32, shape=(global_bs,))\n",
    "act_lengths = tf.placeholder(tf.int32, shape=(global_bs, 7))\n",
    "learning_rate = tf.placeholder(tf.float32, shape = (None))\n",
    "\n",
    "cmd_emb = tf.nn.embedding_lookup(cmd_mat, cmd_ind)\n",
    "act_emb = tf.nn.embedding_lookup(act_mat, act_ind)\n",
    "tf_bs = tf.shape(act_ind)[0]\n",
    "act_st_emb_expanded = tf.tile(tf.reshape(\n",
    "    act_st_emb, [1, 1, 1, size_emb]), [tf_bs, global_time_len, 1, 1])\n",
    "act_emb_with_st = tf.concat((act_st_emb_expanded, act_emb), 2)\n",
    "\n",
    "first_cell_encoder = [tf.nn.rnn_cell.LSTMCell(\n",
    "    hidden_filters, forget_bias=1., name = 'layer1_'+d) for d in ['f', 'b']]\n",
    "hidden_cells_encoder = [[tf.nn.rnn_cell.LSTMCell(\n",
    "    hidden_filters,forget_bias=1., name = 'layer' + str(lidx) + '_' + d)  for d in ['f', 'b']]\n",
    "                        for lidx in range(num_layers_encoder - 1)]\n",
    "hidden_cells_encoder = [[tf.nn.rnn_cell.DropoutWrapper(cell,\n",
    "    output_keep_prob=output_keep_prob, state_keep_prob=state_keep_prob,\n",
    "    variational_recurrent=True, dtype=tf.float32) for cell in cells] for cells in hidden_cells_encoder[:-1]] + [hidden_cells_encoder[-1]]\n",
    "cells_encoder = [first_cell_encoder] + hidden_cells_encoder\n",
    "c1, c2 = zip(*cells_encoder)\n",
    "cells_encoder = [c1, c2]\n",
    "def encode(x, num_layers, cells, initial_states, lengths, name='',):\n",
    "    prev_layer = x\n",
    "    shortcut = x\n",
    "    hiddenlayers = []\n",
    "    returncells = []\n",
    "    cell_fw, cell_bw = cells\n",
    "    bs = tf.shape(x)[0]\n",
    "    for idx in range(num_layers):\n",
    "        prev_layer, c = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw = cell_fw[idx],\n",
    "                cell_bw = cell_bw[idx],\n",
    "                inputs = prev_layer,\n",
    "                sequence_length=lengths,\n",
    "                initial_state_fw=None,\n",
    "                initial_state_bw=None,\n",
    "                dtype=tf.float32,\n",
    "                scope='encoder'+str(idx)\n",
    "            )\n",
    "        prev_layer = tf.concat(prev_layer, 2)\n",
    "        prev_layer = tf.nn.leaky_relu(prev_layer)\n",
    "        returncells.append(c)\n",
    "        hiddenlayers.append(prev_layer)\n",
    "        if idx == num_layers - 1:\n",
    "            #pdb.set_trace()\n",
    "            output = tf.gather_nd(\n",
    "                        prev_layer,\n",
    "                        tf.stack([tf.range(bs), lengths], 1),\n",
    "                        name=None\n",
    "                    )\n",
    "            return prev_layer, returncells, hiddenlayers, output\n",
    "        prev_layer = tf.concat((prev_layer, shortcut), 2)\n",
    "encoding_last_layer, encoding_final_cells, encoding_hidden_layers, encoding_last_timestep = encode(\n",
    "    cmd_emb, num_layers_encoder, cells_encoder,None, lengths = cmd_lengths, name = 'encoder')\n",
    "# encoding_last_timestep = encoding_last_layer[:,cmd_lengths, :]\n",
    "hidden_filters_encoder = encoding_last_timestep.shape[-1].value\n",
    "first_cell_subprogram = tf.nn.rnn_cell.LSTMCell(\n",
    "    hidden_filters_subprogram, forget_bias=1., name = 'subpogramlayer1_')\n",
    "hidden_cells_subprogram = [tf.nn.rnn_cell.LSTMCell(\n",
    "    hidden_filters_subprogram,forget_bias=1., name = 'subpogramlayer' + str(lidx))\n",
    "                        for lidx in range(num_layers_subprogram - 1)]\n",
    "\n",
    "cells_subprogram_rnn = [first_cell_subprogram] + hidden_cells_subprogram\n",
    "\n",
    "attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "    num_units=hidden_filters_encoder, memory=encoding_last_layer,\n",
    "    memory_sequence_length=cmd_lengths)\n",
    "attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "    num_units=hidden_filters_encoder//2, memory=encoding_last_layer,\n",
    "    memory_sequence_length=cmd_lengths)\n",
    "cells_subprogram = [\n",
    "    tf.contrib.seq2seq.AttentionWrapper(\n",
    "        cell, attention_mechanism, attention_layer_size = hidden_filters_subprogram) \n",
    "    for cell in cells_subprogram_rnn]\n",
    "\n",
    "def subprogram(x, num_layers, cells, initial_states, lengths, reuse, name='',):\n",
    "    prev_layer = x\n",
    "    shortcut = x\n",
    "    hiddenlayers = []\n",
    "    returncells = []\n",
    "    bs = tf.shape(x)[0]\n",
    "    for idx in range(num_layers):\n",
    "        print(idx)\n",
    "        if idx == 0:\n",
    "            num_past_units = hidden_filters\n",
    "        else:\n",
    "            num_past_units = hidden_filters_subprogram\n",
    "        with tf.variable_scope(name + 'subprogram' + str(idx), reuse=reuse):\n",
    "#             self_attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "#                 num_units=num_past_units, memory=prev_layer,\n",
    "#                 memory_sequence_length=tf.expand_dims(tf.range(10), 0))\n",
    "#             cell_with_selfattention = tf.contrib.seq2seq.AttentionWrapper(\n",
    "#                     cells[idx], self_attention_mechanism, attention_layer_size = num_past_units)\n",
    "\n",
    "            prev_layer, c = tf.nn.dynamic_rnn(\n",
    "                    cell = cells[idx],\n",
    "                    inputs = prev_layer,\n",
    "                    sequence_length=lengths,\n",
    "                    initial_state = None,\n",
    "                    dtype=tf.float32,\n",
    "                )\n",
    "            prev_layer = tf.concat(prev_layer, 2)\n",
    "            prev_layer = tf.nn.leaky_relu(prev_layer)\n",
    "            returncells.append(c)\n",
    "            hiddenlayers.append(prev_layer)\n",
    "            if idx == num_layers - 1:\n",
    "                output = tf.gather_nd(\n",
    "                            prev_layer,\n",
    "                            tf.stack([tf.range(bs), lengths], 1),\n",
    "                            name=None\n",
    "                        )\n",
    "                return prev_layer, returncells, hiddenlayers, output\n",
    "            prev_layer = tf.concat((prev_layer, shortcut), 2)\n",
    "encodings = [encoding_last_timestep]\n",
    "last_encoding = encoding_last_timestep\n",
    "initial_cmb_encoding = last_encoding\n",
    "loss = 0\n",
    "action_probabilities_presoftmax = []\n",
    "for sub_idx in range(max_num_subprograms): \n",
    "    from_last_layer = tf.tile(tf.expand_dims(tf.concat((\n",
    "        last_encoding, initial_cmb_encoding), 1), 1), [1, max_num_actions + 1, 1])\n",
    "    autoregressive = act_emb_with_st[:,sub_idx, :, :]\n",
    "    x_input = tf.concat((from_last_layer, autoregressive), -1)\n",
    "    subprogram_last_layer, _, subprogram_hidden_layers, subprogram_output = subprogram(\n",
    "        x_input, num_layers_subprogram, cells_subprogram,None, \n",
    "        lengths = act_lengths[:, sub_idx], reuse = (sub_idx > 0), name = 'subprogram')\n",
    "    action_prob_flat = mlp(\n",
    "        tf.reshape(subprogram_last_layer, [-1, hidden_filters_subprogram]),\n",
    "        [], output_size = num_act, name = 'action_choice_mlp', reuse = (sub_idx > 0))\n",
    "    action_prob_expanded = tf.reshape(action_prob_flat, [-1, max_num_actions + 1, num_act])\n",
    "    action_probabilities_layer = tf.nn.softmax(action_prob_expanded, axis=-1)\n",
    "    action_probabilities_presoftmax.append(action_prob_expanded)\n",
    "    delta = mlp(\n",
    "        subprogram_output, [64], output_size = hidden_filters_encoder, name = 'global_transform',\n",
    "        reuse = (sub_idx > 0)\n",
    "    )\n",
    "    last_encoding = last_encoding + delta\n",
    "    encodings.append(last_encoding)\n",
    "act_presoftmax = tf.stack(action_probabilities_presoftmax, 1)[:, :, :-1, :]\n",
    "#batch, subprogram, timestep, action_selection\n",
    "logprobabilities = tf.nn.log_softmax(act_presoftmax, -1)\n",
    "act_presoftmax_flat = tf.reshape(act_presoftmax, [-1, 9, num_act])\n",
    "mask_ph_flat = tf.reshape(mask_ph, [-1, max_actions_per_subprogram])\n",
    "act_ind_flat = tf.reshape(act_ind, [-1, max_actions_per_subprogram])\n",
    "ppl_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "    logits = act_presoftmax_flat,\n",
    "    targets = act_ind_flat,\n",
    "    weights = mask_ph_flat,\n",
    "    average_across_timesteps=False,\n",
    "    average_across_batch=False,\n",
    "    softmax_loss_function=None,\n",
    "    name=None\n",
    ")\n",
    "ppl_loss_avg = tf.reduce_mean(tf.pow(ppl_loss, 1.5)) * 100\n",
    "\n",
    "tfvars = tf.trainable_variables()\n",
    "weight_norm = tf.reduce_mean([tf.reduce_sum(tf.square(var)) for var in tfvars])*1e-5\n",
    "\n",
    "action_taken = tf.argmax(act_presoftmax, -1, output_type=tf.int32)\n",
    "correct_mat = tf.cast(tf.equal(action_taken, act_ind), tf.float32) * mask_ph\n",
    "correct_percent = tf.reduce_sum(correct_mat, [1, 2])/tf.reduce_sum(mask_ph, [1, 2])\n",
    "percent_correct = tf.reduce_mean(correct_percent)\n",
    "percent_fully_correct = tf.reduce_mean(tf.cast(tf.equal(correct_percent, 1), tf.float32))\n",
    "\n",
    "loss = ppl_loss_avg + weight_norm\n",
    "\n",
    "opt_fcn = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "#opt_fcn = tf.train.MomentumOptimizer(learning_rate=learning_rate, use_nesterov=True, momentum=.8)\n",
    "optimizer, grad_norm_total = apply_clipped_optimizer(opt_fcn, loss)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'LeakyRelu_5/Maximum:0' shape=(?, 10, 512) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2091,), (18819,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "trn_percent = .1\n",
    "num_samples = mask_np.shape[0]\n",
    "ordered_samples = np.arange(num_samples)\n",
    "np.random.shuffle(ordered_samples)\n",
    "trn_samples = ordered_samples[:int(np.ceil(num_samples*trn_percent))]\n",
    "val_samples_all = ordered_samples[int(np.ceil(num_samples*trn_percent)):]\n",
    "val_samples = val_samples_all\n",
    "trn_samples.shape, val_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 10 trn_loss 125.63178126295718 val_loss 74.49046\n",
      "itr: 10 trn_acc 0.0 trn_single_acc 0.23163052923205987 val_acc 0.0\n",
      "itr: 20 trn_loss 89.52098678706643 val_loss 74.00071029663086\n",
      "itr: 20 trn_acc 0.0 trn_single_acc 0.32212166276457915 val_acc 0.0\n",
      "itr: 30 trn_loss 72.27933313532839 val_loss 72.57512771606446\n",
      "itr: 30 trn_acc 0.00023066015625000007 trn_single_acc 0.3923362940624975 val_acc 3.1882672919891776e-05\n",
      "itr: 40 trn_loss 60.04697753956069 val_loss 70.57124539184571\n",
      "itr: 40 trn_acc 0.00036519184847447233 trn_single_acc 0.4560885899498661 val_acc 3.932196254027076e-05\n",
      "itr: 50 trn_loss 52.26999728087269 val_loss 68.34460753173829\n",
      "itr: 50 trn_acc 0.0001273345240633146 trn_single_acc 0.5020944749707751 val_acc 4.070354474242777e-05\n",
      "itr: 60 trn_loss 48.148354998644045 val_loss 66.09733054016114\n",
      "itr: 60 trn_acc 0.00032916442822127247 trn_single_acc 0.5267567671604656 val_acc 3.663319026818499e-05\n",
      "itr: 70 trn_loss 45.259468962846064 val_loss 63.83911535418702\n",
      "itr: 70 trn_acc 0.00039953816436860175 trn_single_acc 0.5445278246655059 val_acc 3.296987124136649e-05\n",
      "itr: 80 trn_loss 42.16098529732981 val_loss 61.53884715983277\n",
      "itr: 80 trn_acc 0.00013931034391246146 trn_single_acc 0.5599317596572078 val_acc 2.967288411722984e-05\n",
      "itr: 90 trn_loss 39.223904362053716 val_loss 59.348141459962775\n",
      "itr: 90 trn_acc 4.8574513405191617e-05 trn_single_acc 0.5710543982711241 val_acc 4.796070953024318e-05\n",
      "itr: 100 trn_loss 37.34886270784019 val_loss 57.146303464479196\n",
      "itr: 100 trn_acc 1.693688556273876e-05 trn_single_acc 0.580279723049068 val_acc 6.973353122193716e-05\n",
      "itr: 110 trn_loss 36.069039470499746 val_loss 55.06142737523343\n",
      "itr: 110 trn_acc 0.00034407638191629304 trn_single_acc 0.5897821504604421 val_acc 9.464285101963522e-05\n",
      "itr: 120 trn_loss 34.46798085861601 val_loss 53.05456556727552\n",
      "itr: 120 trn_acc 0.00011997201612182495 trn_single_acc 0.5980354386170533 val_acc 0.0001436301285718987\n",
      "itr: 130 trn_loss 33.96417739088619 val_loss 51.15680058464465\n",
      "itr: 130 trn_acc 0.00097952306168701 trn_single_acc 0.6081715760306576 val_acc 0.0002196013541992107\n",
      "itr: 140 trn_loss 32.89629802845853 val_loss 49.39140090642921\n",
      "itr: 140 trn_acc 0.0008569643544410029 trn_single_acc 0.6171519112003928 val_acc 0.0002773478967134445\n",
      "itr: 150 trn_loss 32.02897228477482 val_loss 47.72492469518082\n",
      "itr: 150 trn_acc 0.0008762647208902926 trn_single_acc 0.6244684784526819 val_acc 0.0003824575760864576\n",
      "itr: 160 trn_loss 31.731978407251372 val_loss 46.18471600862173\n",
      "itr: 160 trn_acc 0.0009119401667759393 trn_single_acc 0.6289732935512882 val_acc 0.00043454605696231373\n",
      "itr: 170 trn_loss 31.267326201632937 val_loss 44.76470460520829\n",
      "itr: 170 trn_acc 0.0007167852849722187 trn_single_acc 0.634936849526696 val_acc 0.00048142568975058424\n",
      "itr: 180 trn_loss 30.51612344436757 val_loss 43.44475346231441\n",
      "itr: 180 trn_acc 0.0010431678523944972 trn_single_acc 0.6439999383243783 val_acc 0.0005820689219142547\n",
      "itr: 190 trn_loss 30.171861127265192 val_loss 42.19620096764547\n",
      "itr: 190 trn_acc 0.0011330532684416307 trn_single_acc 0.6438101415743697 val_acc 0.0006301376003017025\n",
      "itr: 200 trn_loss 29.362788087721732 val_loss 41.06243559195026\n",
      "itr: 200 trn_acc 0.0008311729997060594 trn_single_acc 0.6517173890199489 val_acc 0.0007159096414102611\n",
      "itr: 210 trn_loss 29.19511570769348 val_loss 39.97159463590465\n",
      "itr: 210 trn_acc 0.0010171199995219968 trn_single_acc 0.6542861721848924 val_acc 0.0007984182557727543\n",
      "itr: 220 trn_loss 28.700480447128122 val_loss 38.937920890087625\n",
      "itr: 220 trn_acc 0.0011040626624840928 trn_single_acc 0.66469604770819 val_acc 0.000835479555503933\n",
      "itr: 230 trn_loss 28.010021774280915 val_loss 37.97105446025855\n",
      "itr: 230 trn_acc 0.0007438931160682314 trn_single_acc 0.6675773294299285 val_acc 0.0009007174010922685\n",
      "itr: 240 trn_loss 27.51777233145332 val_loss 37.02466808466727\n",
      "itr: 240 trn_acc 0.00041071561982742417 trn_single_acc 0.6722723033722475 val_acc 0.0010285105795055783\n",
      "itr: 250 trn_loss 27.070469235106945 val_loss 36.113637280839214\n",
      "itr: 250 trn_acc 0.0012609452550836307 trn_single_acc 0.679153622059976 val_acc 0.0011116417758888146\n",
      "itr: 260 trn_loss 26.4902517517245 val_loss 35.229001992086346\n",
      "itr: 260 trn_acc 0.0017710619632659323 trn_single_acc 0.6886499999681398 val_acc 0.0012024011847280986\n",
      "itr: 270 trn_loss 25.640068535212784 val_loss 34.371541017120876\n",
      "itr: 270 trn_acc 0.001408618856656384 trn_single_acc 0.6913352267527864 val_acc 0.0012840846526834542\n",
      "itr: 280 trn_loss 24.971838747692267 val_loss 33.53420590802354\n",
      "itr: 280 trn_acc 0.003918203966650019 trn_single_acc 0.6979168409140714 val_acc 0.0013947962153968074\n",
      "itr: 290 trn_loss 24.59290435035404 val_loss 32.704211754599115\n",
      "itr: 290 trn_acc 0.0033063837545070367 trn_single_acc 0.7022756844121575 val_acc 0.0014731815123796635\n",
      "itr: 300 trn_loss 23.969493937448714 val_loss 31.91662158042094\n",
      "itr: 300 trn_acc 0.00275923480372162 trn_single_acc 0.7051814377583527 val_acc 0.0016181211860543643\n",
      "itr: 310 trn_loss 23.613076790103563 val_loss 31.154468203811955\n",
      "itr: 310 trn_acc 0.0026871855919187843 trn_single_acc 0.7135770581885378 val_acc 0.001759194447091176\n",
      "itr: 320 trn_loss 22.881542056092933 val_loss 30.408697210457127\n",
      "itr: 320 trn_acc 0.005755447360136938 trn_single_acc 0.7147709050755303 val_acc 0.0018330226083764023\n",
      "itr: 330 trn_loss 22.35419220187727 val_loss 29.701928212678016\n",
      "itr: 330 trn_acc 0.005436285325188338 trn_single_acc 0.7166787388636 val_acc 0.001973860836640172\n",
      "itr: 340 trn_loss 22.01600614566549 val_loss 29.016733522208554\n",
      "itr: 340 trn_acc 0.005214691029703317 trn_single_acc 0.7241339035158261 val_acc 0.00204216369106487\n",
      "itr: 350 trn_loss 21.732223511445774 val_loss 28.359030278019926\n",
      "itr: 350 trn_acc 0.004877371895559166 trn_single_acc 0.7271513381547952 val_acc 0.0022045980532611805\n",
      "itr: 360 trn_loss 21.08409500223715 val_loss 27.729028586887853\n",
      "itr: 360 trn_acc 0.0059160743969874025 trn_single_acc 0.7313637633404084 val_acc 0.002510202323464638\n",
      "itr: 370 trn_loss 20.9504963349404 val_loss 27.13081292302329\n",
      "itr: 370 trn_acc 0.008476860434444618 trn_single_acc 0.7329318256157249 val_acc 0.002790559967295604\n",
      "itr: 380 trn_loss 20.654635483567077 val_loss 26.56205993882643\n",
      "itr: 380 trn_acc 0.00769988420682131 trn_single_acc 0.7356351417796791 val_acc 0.0029366062528815365\n",
      "itr: 390 trn_loss 20.422614381688277 val_loss 26.024927431271912\n",
      "itr: 390 trn_acc 0.008492210184106957 trn_single_acc 0.7369848535510691 val_acc 0.0032062061679595556\n",
      "itr: 400 trn_loss 20.12892990795457 val_loss 25.514248347812693\n",
      "itr: 400 trn_acc 0.0086116288023395 trn_single_acc 0.7362186491396169 val_acc 0.0033850807631522874\n",
      "itr: 410 trn_loss 19.68972856261602 val_loss 25.015934589386404\n",
      "itr: 410 trn_acc 0.010779066428379348 trn_single_acc 0.7414493330204341 val_acc 0.003572636762366634\n",
      "itr: 420 trn_loss 19.4962531207231 val_loss 24.5443889286045\n",
      "itr: 420 trn_acc 0.011677112832434718 trn_single_acc 0.743886286621203 val_acc 0.003810516290684886\n",
      "itr: 430 trn_loss 19.08853718494304 val_loss 24.103131340675695\n",
      "itr: 430 trn_acc 0.011274055663066286 trn_single_acc 0.7456347708719073 val_acc 0.003976783846605134\n",
      "itr: 440 trn_loss 18.989907218341745 val_loss 23.68439726239426\n",
      "itr: 440 trn_acc 0.010940011160948527 trn_single_acc 0.7458193674994436 val_acc 0.004131738447581213\n",
      "itr: 450 trn_loss 18.791721674637348 val_loss 23.295083512717333\n",
      "itr: 450 trn_acc 0.012689214731354249 trn_single_acc 0.7482704096015782 val_acc 0.004271197588459684\n",
      "itr: 460 trn_loss 18.58579658661981 val_loss 22.91831430481718\n",
      "itr: 460 trn_acc 0.010778427902528793 trn_single_acc 0.7512498179032673 val_acc 0.004471103698357373\n",
      "itr: 470 trn_loss 18.483471797720483 val_loss 22.54876642841261\n",
      "itr: 470 trn_acc 0.01168126084770593 trn_single_acc 0.7498295545288098 val_acc 0.0046882156621018915\n",
      "itr: 480 trn_loss 18.069355495963435 val_loss 22.207822395129455\n",
      "itr: 480 trn_acc 0.014179325383826186 trn_single_acc 0.7523798054420794 val_acc 0.004798595945069182\n",
      "itr: 490 trn_loss 18.021745298068417 val_loss 21.879624956336723\n",
      "itr: 490 trn_acc 0.01499392443462785 trn_single_acc 0.7548329418062034 val_acc 0.005099861786167909\n",
      "itr: 500 trn_loss 17.622988926828164 val_loss 21.560537201303635\n",
      "itr: 500 trn_acc 0.01391387572464019 trn_single_acc 0.7579652520172416 val_acc 0.005333804624886295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 510 trn_loss 17.56041935700394 val_loss 21.243894835238216\n",
      "itr: 510 trn_acc 0.017853963415834676 trn_single_acc 0.7580018519788491 val_acc 0.005655942481110377\n",
      "itr: 520 trn_loss 17.426591662982094 val_loss 20.97429031722953\n",
      "itr: 520 trn_acc 0.015808268666560706 trn_single_acc 0.761185230129637 val_acc 0.005807708340227499\n",
      "itr: 530 trn_loss 17.357311754893775 val_loss 20.698482226668688\n",
      "itr: 530 trn_acc 0.015145011481994349 trn_single_acc 0.7606581409560621 val_acc 0.0060452594532131206\n",
      "itr: 540 trn_loss 16.96373215880756 val_loss 20.423650483494008\n",
      "itr: 540 trn_acc 0.01901837620774463 trn_single_acc 0.76291124892843 val_acc 0.006338762138655102\n",
      "itr: 550 trn_loss 16.86116815413222 val_loss 20.164762264673417\n",
      "itr: 550 trn_acc 0.019141250719273233 trn_single_acc 0.7661773648073196 val_acc 0.006677307392093821\n",
      "itr: 560 trn_loss 16.652980072369687 val_loss 19.927474003599144\n",
      "itr: 560 trn_acc 0.018787485480249826 trn_single_acc 0.7647955519945759 val_acc 0.006912919037729458\n",
      "itr: 570 trn_loss 16.572293069959233 val_loss 19.68126858016794\n",
      "itr: 570 trn_acc 0.015570873268936164 trn_single_acc 0.7620500394269861 val_acc 0.007109028163424095\n",
      "itr: 580 trn_loss 16.470576301585147 val_loss 19.455724806257592\n",
      "itr: 580 trn_acc 0.014009516751235577 trn_single_acc 0.766835248616962 val_acc 0.0074130570333042385\n",
      "itr: 590 trn_loss 16.266918748191 val_loss 19.229986462594724\n",
      "itr: 590 trn_acc 0.020436969035266272 trn_single_acc 0.7719402489023809 val_acc 0.007628231441900609\n",
      "itr: 600 trn_loss 16.04655990914036 val_loss 19.01425424242168\n",
      "itr: 600 trn_acc 0.021057984502729277 trn_single_acc 0.7740375865699812 val_acc 0.007896281339310536\n",
      "itr: 610 trn_loss 16.016133472754728 val_loss 18.805567923404123\n",
      "itr: 610 trn_acc 0.022036042654858503 trn_single_acc 0.7759249772777144 val_acc 0.008217232930734392\n",
      "itr: 620 trn_loss 15.860294509890982 val_loss 18.597700568929923\n",
      "itr: 620 trn_acc 0.019244063848795 trn_single_acc 0.7735355531876074 val_acc 0.008458265390015814\n",
      "itr: 630 trn_loss 15.535279580640383 val_loss 18.41292402705158\n",
      "itr: 630 trn_acc 0.021764106728826393 trn_single_acc 0.7786973018092508 val_acc 0.008643311892614223\n",
      "itr: 640 trn_loss 15.521978773342394 val_loss 18.222177698259998\n",
      "itr: 640 trn_acc 0.021272710389455232 trn_single_acc 0.7777355935290645 val_acc 0.008926756893544309\n",
      "itr: 650 trn_loss 15.34522712149926 val_loss 18.038036329190835\n",
      "itr: 650 trn_acc 0.022445620096778692 trn_single_acc 0.7799951817735237 val_acc 0.009128719574167353\n",
      "itr: 660 trn_loss 15.231851115422526 val_loss 17.863601859784932\n",
      "itr: 660 trn_acc 0.021501436308281385 trn_single_acc 0.7771521008057131 val_acc 0.009384878916401287\n",
      "itr: 670 trn_loss 15.095689624549415 val_loss 17.69231545005156\n",
      "itr: 670 trn_acc 0.023084867058209186 trn_single_acc 0.7820545736793749 val_acc 0.00955697075011607\n",
      "itr: 680 trn_loss 14.81322337732411 val_loss 17.531635884111346\n",
      "itr: 680 trn_acc 0.02102749050725664 trn_single_acc 0.7816254722741484 val_acc 0.009780932576050842\n",
      "itr: 690 trn_loss 14.746443510593227 val_loss 17.368367830215593\n",
      "itr: 690 trn_acc 0.021541290770928433 trn_single_acc 0.7837456329647776 val_acc 0.010088773766687945\n",
      "itr: 700 trn_loss 14.733022993330543 val_loss 17.216483401625187\n",
      "itr: 700 trn_acc 0.021291313075466604 trn_single_acc 0.7841380149002841 val_acc 0.010344575728802177\n",
      "itr: 710 trn_loss 14.43620640510842 val_loss 17.07417618695095\n",
      "itr: 710 trn_acc 0.0230687258445063 trn_single_acc 0.7868593017237826 val_acc 0.010686386796082518\n",
      "itr: 720 trn_loss 14.337600286661473 val_loss 16.926509220569088\n",
      "itr: 720 trn_acc 0.02420298797732264 trn_single_acc 0.7870876374666782 val_acc 0.010914310072879907\n",
      "itr: 730 trn_loss 14.300222499994923 val_loss 16.793599223347385\n",
      "itr: 730 trn_acc 0.023611409790566924 trn_single_acc 0.7853899710525293 val_acc 0.011140696131456719\n",
      "itr: 740 trn_loss 14.244324909173482 val_loss 16.65606020860542\n",
      "itr: 740 trn_acc 0.02652902780035238 trn_single_acc 0.787262097118602 val_acc 0.011206285419257426\n",
      "itr: 750 trn_loss 14.074051455769592 val_loss 16.530982866638922\n",
      "itr: 750 trn_acc 0.02466605385970266 trn_single_acc 0.7878998409683711 val_acc 0.011552259802542875\n",
      "itr: 760 trn_loss 13.989787127111732 val_loss 16.421096707660578\n",
      "itr: 760 trn_acc 0.022902280096895593 trn_single_acc 0.7860403833699142 val_acc 0.011736105997612552\n",
      "itr: 770 trn_loss 13.878092596843258 val_loss 16.30729333038207\n",
      "itr: 770 trn_acc 0.023504343717278216 trn_single_acc 0.787390116748496 val_acc 0.01190156757317526\n",
      "itr: 780 trn_loss 13.95133441653706 val_loss 16.191253919676626\n",
      "itr: 780 trn_acc 0.025604583916196064 trn_single_acc 0.7862894830901335 val_acc 0.012156758631609765\n",
      "itr: 790 trn_loss 13.794720602709075 val_loss 16.1125975065908\n",
      "itr: 790 trn_acc 0.027616284126965054 trn_single_acc 0.7892270755210722 val_acc 0.012301410053231914\n",
      "itr: 800 trn_loss 13.891919976401896 val_loss 15.99501870991121\n",
      "itr: 800 trn_acc 0.029411777885201697 trn_single_acc 0.7901873136939493 val_acc 0.012511303016446769\n",
      "itr: 810 trn_loss 13.57669838492582 val_loss 15.894808068168137\n",
      "itr: 810 trn_acc 0.029748745103501686 trn_single_acc 0.7919870087113927 val_acc 0.012833051187309093\n",
      "itr: 820 trn_loss 13.502854892755236 val_loss 15.782016123198247\n",
      "itr: 820 trn_acc 0.029797197015355863 trn_single_acc 0.7949323905750408 val_acc 0.013101369431626023\n",
      "itr: 830 trn_loss 13.176103279068798 val_loss 15.688081459578374\n",
      "itr: 830 trn_acc 0.027328137003572908 trn_single_acc 0.7977141586459583 val_acc 0.01334285585151126\n",
      "itr: 840 trn_loss 13.404813668735695 val_loss 15.590155443289728\n",
      "itr: 840 trn_acc 0.03069959706991677 trn_single_acc 0.7944030504552066 val_acc 0.013427349125439018\n",
      "itr: 850 trn_loss 13.282115312351483 val_loss 15.501243639652895\n",
      "itr: 850 trn_acc 0.02526404756530274 trn_single_acc 0.7954809637825933 val_acc 0.013662806439483843\n",
      "itr: 860 trn_loss 13.283674919987117 val_loss 15.413249536155135\n",
      "itr: 860 trn_acc 0.024055707117947595 trn_single_acc 0.7960805833056708 val_acc 0.013800325185583249\n",
      "itr: 870 trn_loss 13.192290771647423 val_loss 15.326972189961497\n",
      "itr: 870 trn_acc 0.023086147036122814 trn_single_acc 0.7950980587397376 val_acc 0.013977229784154489\n",
      "itr: 880 trn_loss 12.996458344135398 val_loss 15.238975212054214\n",
      "itr: 880 trn_acc 0.02630673639467767 trn_single_acc 0.7959025233850839 val_acc 0.014062051086327668\n",
      "itr: 890 trn_loss 12.844757151884513 val_loss 15.154463295707192\n",
      "itr: 890 trn_acc 0.033308323708655614 trn_single_acc 0.798921941204878 val_acc 0.014329686336548242\n",
      "itr: 900 trn_loss 12.797328529395829 val_loss 15.077316240581053\n",
      "itr: 900 trn_acc 0.03426392484917365 trn_single_acc 0.8005107930548807 val_acc 0.014480223683563869\n",
      "itr: 910 trn_loss 12.75372619007984 val_loss 14.992062338690918\n",
      "itr: 910 trn_acc 0.033065693489988494 trn_single_acc 0.803746101323529 val_acc 0.014679472717387676\n",
      "itr: 920 trn_loss 12.711384913888612 val_loss 14.91271608635869\n",
      "itr: 920 trn_acc 0.03448469913511705 trn_single_acc 0.8040227767054959 val_acc 0.014906620820829154\n",
      "itr: 930 trn_loss 12.634409180427735 val_loss 14.838839687988935\n",
      "itr: 930 trn_acc 0.029074591282609814 trn_single_acc 0.8022529628827987 val_acc 0.014988837211253239\n",
      "itr: 940 trn_loss 12.465457756814105 val_loss 14.75508704014341\n",
      "itr: 940 trn_acc 0.032471716753715456 trn_single_acc 0.8053460328713998 val_acc 0.015211617821981304\n",
      "itr: 950 trn_loss 12.358831985600633 val_loss 14.682153256783366\n",
      "itr: 950 trn_acc 0.03385320671888267 trn_single_acc 0.8063827727438654 val_acc 0.015337727441963368\n",
      "itr: 960 trn_loss 12.217159251383764 val_loss 14.59996555332793\n",
      "itr: 960 trn_acc 0.03432740489666973 trn_single_acc 0.8067154257147329 val_acc 0.01549373631886555\n",
      "itr: 970 trn_loss 12.228609405914998 val_loss 14.519833690683125\n",
      "itr: 970 trn_acc 0.03405894271197378 trn_single_acc 0.8070703269606336 val_acc 0.01573510600815934\n",
      "itr: 980 trn_loss 12.139758848977351 val_loss 14.444702833974432\n",
      "itr: 980 trn_acc 0.03577090531470379 trn_single_acc 0.8068717180194229 val_acc 0.01597359393111517\n",
      "itr: 990 trn_loss 12.227855325745779 val_loss 14.385754439309899\n",
      "itr: 990 trn_acc 0.03391844536843948 trn_single_acc 0.8062922892849591 val_acc 0.016119153886183946\n",
      "itr: 1000 trn_loss 12.174466036763254 val_loss 14.34296003557056\n",
      "itr: 1000 trn_acc 0.033219448743950185 trn_single_acc 0.8091074742873973 val_acc 0.01637237474841909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 1010 trn_loss 12.12480796758569 val_loss 14.275310512573807\n",
      "itr: 1010 trn_acc 0.039277784344413084 trn_single_acc 0.807747520321002 val_acc 0.01643554640283915\n",
      "itr: 1020 trn_loss 12.128787025924932 val_loss 14.205957027844747\n",
      "itr: 1020 trn_acc 0.03145187867970042 trn_single_acc 0.8068393842193091 val_acc 0.016593362778163544\n",
      "itr: 1030 trn_loss 11.915416933099 val_loss 14.138219773088837\n",
      "itr: 1030 trn_acc 0.033262085712658644 trn_single_acc 0.8081275859869874 val_acc 0.016825731707873874\n",
      "itr: 1040 trn_loss 11.826477342293735 val_loss 14.072746413333665\n",
      "itr: 1040 trn_acc 0.03193562640294982 trn_single_acc 0.8110154812476069 val_acc 0.01702423623644972\n",
      "itr: 1050 trn_loss 11.691220572550362 val_loss 14.019135311886773\n",
      "itr: 1050 trn_acc 0.032567679961468796 trn_single_acc 0.8127885433735931 val_acc 0.017186948863658284\n",
      "itr: 1060 trn_loss 11.67727223991911 val_loss 13.967565484921728\n",
      "itr: 1060 trn_acc 0.0344785218360941 trn_single_acc 0.8127587342906839 val_acc 0.01743966586857406\n",
      "itr: 1070 trn_loss 11.616314996007167 val_loss 13.90443015224987\n",
      "itr: 1070 trn_acc 0.035087875470528704 trn_single_acc 0.8108661883220264 val_acc 0.01759803199740679\n",
      "itr: 1080 trn_loss 11.606749378116632 val_loss 13.838578964417461\n",
      "itr: 1080 trn_acc 0.03663484886400264 trn_single_acc 0.8130378496259385 val_acc 0.017777757978192844\n",
      "itr: 1090 trn_loss 11.50440808491585 val_loss 13.78114614350672\n",
      "itr: 1090 trn_acc 0.03597510839855908 trn_single_acc 0.8129719354858501 val_acc 0.018061728263573537\n",
      "itr: 1100 trn_loss 11.475506059143003 val_loss 13.738849152294478\n",
      "itr: 1100 trn_acc 0.03784835273821299 trn_single_acc 0.8146665027803618 val_acc 0.018248222344824695\n",
      "itr: 1110 trn_loss 11.436450080235938 val_loss 13.689847030491544\n",
      "itr: 1110 trn_acc 0.03559032890209905 trn_single_acc 0.812171040573361 val_acc 0.01841075326386901\n",
      "itr: 1120 trn_loss 11.491716415701712 val_loss 13.623533473835213\n",
      "itr: 1120 trn_acc 0.03813718562907361 trn_single_acc 0.8113943175923076 val_acc 0.01870581695035528\n",
      "itr: 1130 trn_loss 11.392209531556052 val_loss 13.571579181932648\n",
      "itr: 1130 trn_acc 0.032898842834402656 trn_single_acc 0.8116318253095581 val_acc 0.018982001776356377\n",
      "itr: 1140 trn_loss 11.28553916300539 val_loss 13.514686289831914\n",
      "itr: 1140 trn_acc 0.03904389381075083 trn_single_acc 0.8157055797898728 val_acc 0.019087096200757214\n",
      "itr: 1150 trn_loss 11.156716259813011 val_loss 13.446641275350675\n",
      "itr: 1150 trn_acc 0.039865574414753274 trn_single_acc 0.816499457234561 val_acc 0.019139170963799645\n",
      "itr: 1160 trn_loss 11.049616718117846 val_loss 13.392408551929378\n",
      "itr: 1160 trn_acc 0.03916561246507979 trn_single_acc 0.8154516147697788 val_acc 0.019276372442456207\n",
      "itr: 1170 trn_loss 11.013905138699345 val_loss 13.336716147206412\n",
      "itr: 1170 trn_acc 0.036811216219260146 trn_single_acc 0.8146509796762208 val_acc 0.019389226265083662\n",
      "itr: 1180 trn_loss 10.977275103695794 val_loss 13.29404966706829\n",
      "itr: 1180 trn_acc 0.0388121441112342 trn_single_acc 0.8162274783350648 val_acc 0.019527991170284968\n",
      "itr: 1190 trn_loss 10.899148909358573 val_loss 13.249206525160044\n",
      "itr: 1190 trn_acc 0.0388489979382978 trn_single_acc 0.8189061587261071 val_acc 0.019583800409374675\n",
      "itr: 1200 trn_loss 10.900620147695543 val_loss 13.203630240624998\n",
      "itr: 1200 trn_acc 0.04105117032752692 trn_single_acc 0.8169818057589131 val_acc 0.019756245627228653\n",
      "itr: 1210 trn_loss 10.841842290823068 val_loss 13.146207051157225\n",
      "itr: 1210 trn_acc 0.03863601020126899 trn_single_acc 0.8167646134330439 val_acc 0.019980525498888704\n",
      "itr: 1220 trn_loss 10.850896286051793 val_loss 13.086706051241698\n",
      "itr: 1220 trn_acc 0.03953745223350992 trn_single_acc 0.817761603526913 val_acc 0.020198318645627926\n",
      "itr: 1230 trn_loss 10.897424708429428 val_loss 13.033987708995948\n",
      "itr: 1230 trn_acc 0.03843673853905852 trn_single_acc 0.8180834470672854 val_acc 0.020489980423693325\n",
      "itr: 1240 trn_loss 10.708901168316508 val_loss 12.991113592484782\n",
      "itr: 1240 trn_acc 0.04378624217070549 trn_single_acc 0.8213132736851991 val_acc 0.02061963161311549\n",
      "itr: 1250 trn_loss 10.640156428745554 val_loss 12.93832648631858\n",
      "itr: 1250 trn_acc 0.048092868694094314 trn_single_acc 0.8221172286363684 val_acc 0.020831965629595536\n",
      "itr: 1260 trn_loss 10.546552128804539 val_loss 12.883670301767534\n",
      "itr: 1260 trn_acc 0.043178303103378446 trn_single_acc 0.8217440426334182 val_acc 0.020975242271427528\n",
      "itr: 1270 trn_loss 10.42483325527351 val_loss 12.832891772567343\n",
      "itr: 1270 trn_acc 0.04325975887186037 trn_single_acc 0.8232440378738506 val_acc 0.02117327042466779\n",
      "itr: 1280 trn_loss 10.39541449587504 val_loss 12.779017344456118\n",
      "itr: 1280 trn_acc 0.040833600612073674 trn_single_acc 0.823431493349957 val_acc 0.021383378473338897\n",
      "itr: 1290 trn_loss 10.36690453252284 val_loss 12.7328674334358\n",
      "itr: 1290 trn_acc 0.04076289638852176 trn_single_acc 0.8235909179925492 val_acc 0.021508710295633154\n",
      "itr: 1300 trn_loss 10.311137133461084 val_loss 12.687398511594907\n",
      "itr: 1300 trn_acc 0.0474055254267077 trn_single_acc 0.8268454548430891 val_acc 0.02170652937353463\n",
      "itr: 1310 trn_loss 10.264145290211554 val_loss 12.637979434513786\n",
      "itr: 1310 trn_acc 0.04880190692353034 trn_single_acc 0.8258371515024872 val_acc 0.021969586981482606\n",
      "itr: 1320 trn_loss 10.15700809215427 val_loss 12.597142153904205\n",
      "itr: 1320 trn_acc 0.04722446764328321 trn_single_acc 0.8250809458106039 val_acc 0.022174456304145426\n",
      "itr: 1330 trn_loss 10.086681381433701 val_loss 12.553504789404897\n",
      "itr: 1330 trn_acc 0.047418095587022306 trn_single_acc 0.8265023832946233 val_acc 0.022417290175705467\n",
      "itr: 1340 trn_loss 10.07676717772555 val_loss 12.503588155984428\n",
      "itr: 1340 trn_acc 0.05155853652445189 trn_single_acc 0.8303308660206211 val_acc 0.022736802546455843\n",
      "itr: 1350 trn_loss 10.285930096228308 val_loss 12.4685755813833\n",
      "itr: 1350 trn_acc 0.047310658432544145 trn_single_acc 0.8267820274681281 val_acc 0.022960598258621442\n",
      "itr: 1360 trn_loss 10.125328450145382 val_loss 12.436324975912207\n",
      "itr: 1360 trn_acc 0.04915057254642592 trn_single_acc 0.8273321107019954 val_acc 0.023045111250978963\n",
      "itr: 1370 trn_loss 9.997120957621583 val_loss 12.38761454405585\n",
      "itr: 1370 trn_acc 0.047758180956915926 trn_single_acc 0.8280264744044059 val_acc 0.02330184151420199\n",
      "itr: 1380 trn_loss 9.992128437122396 val_loss 12.351900365193478\n",
      "itr: 1380 trn_acc 0.045631636527524104 trn_single_acc 0.8278833449277896 val_acc 0.023543526259266166\n",
      "itr: 1390 trn_loss 10.02927268624724 val_loss 12.31501173934918\n",
      "itr: 1390 trn_acc 0.046020919051439804 trn_single_acc 0.8274925403143725 val_acc 0.023686649600150732\n",
      "itr: 1400 trn_loss 10.07313252825605 val_loss 12.2900400873564\n",
      "itr: 1400 trn_acc 0.044094189672293164 trn_single_acc 0.8276899561930988 val_acc 0.023831401869192016\n",
      "itr: 1410 trn_loss 9.913534278617139 val_loss 12.249351279426426\n",
      "itr: 1410 trn_acc 0.05102357601772123 trn_single_acc 0.829314260870625 val_acc 0.023993561622084044\n",
      "itr: 1420 trn_loss 9.863824740230218 val_loss 12.199604788263569\n",
      "itr: 1420 trn_acc 0.04499748818051095 trn_single_acc 0.8286071474289908 val_acc 0.024128877891523418\n",
      "itr: 1430 trn_loss 9.749816554390957 val_loss 12.153591739094194\n",
      "itr: 1430 trn_acc 0.04939202466141751 trn_single_acc 0.8311433468686382 val_acc 0.02440476214744697\n",
      "itr: 1440 trn_loss 9.672719895153003 val_loss 12.10982012774581\n",
      "itr: 1440 trn_acc 0.05304362640490193 trn_single_acc 0.832984594462686 val_acc 0.02472213696710512\n",
      "itr: 1450 trn_loss 9.646144344314374 val_loss 12.065919661754675\n",
      "itr: 1450 trn_acc 0.05539969424570214 trn_single_acc 0.8316608697968594 val_acc 0.02496526408587913\n",
      "itr: 1460 trn_loss 9.674813330086762 val_loss 12.018277261466658\n",
      "itr: 1460 trn_acc 0.05370388365363962 trn_single_acc 0.8313995305885126 val_acc 0.02516282347644884\n",
      "itr: 1470 trn_loss 9.639902893103633 val_loss 11.982914913127608\n",
      "itr: 1470 trn_acc 0.053968362144116086 trn_single_acc 0.8332710184048602 val_acc 0.02528217526053356\n",
      "itr: 1480 trn_loss 9.599121142898612 val_loss 11.943657960083891\n",
      "itr: 1480 trn_acc 0.05232004832130292 trn_single_acc 0.8335476243162324 val_acc 0.02548523999847442\n",
      "itr: 1490 trn_loss 9.561202342995664 val_loss 11.913075809705385\n",
      "itr: 1490 trn_acc 0.056847869378769164 trn_single_acc 0.8357985852389467 val_acc 0.025487329692519937\n",
      "itr: 1500 trn_loss 9.525394597527258 val_loss 11.880286352361557\n",
      "itr: 1500 trn_acc 0.052002554088700935 trn_single_acc 0.835965691279078 val_acc 0.025738957976589115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 1510 trn_loss 9.478699329058054 val_loss 11.842848613731846\n",
      "itr: 1510 trn_acc 0.051631493764301636 trn_single_acc 0.8355011310875781 val_acc 0.026066385318597715\n",
      "itr: 1520 trn_loss 9.371360088904023 val_loss 11.806923718636737\n",
      "itr: 1520 trn_acc 0.051129252899128966 trn_single_acc 0.8358007795320364 val_acc 0.026249480531895665\n",
      "itr: 1530 trn_loss 9.446797210680943 val_loss 11.782019386934197\n",
      "itr: 1530 trn_acc 0.04377838148848733 trn_single_acc 0.8338876621618493 val_acc 0.026393011207536912\n",
      "itr: 1540 trn_loss 9.489340505502781 val_loss 11.76207655392315\n",
      "itr: 1540 trn_acc 0.04559099899368036 trn_single_acc 0.8335026063449418 val_acc 0.026453109640022565\n",
      "itr: 1550 trn_loss 9.430561116964846 val_loss 11.753941885133619\n",
      "itr: 1550 trn_acc 0.05497963514413902 trn_single_acc 0.8357809401631267 val_acc 0.026730376832014728\n",
      "itr: 1560 trn_loss 9.363947541359263 val_loss 11.722207955867082\n",
      "itr: 1560 trn_acc 0.0513872437747943 trn_single_acc 0.8348599438674437 val_acc 0.0268523866480527\n",
      "itr: 1570 trn_loss 9.296726110501389 val_loss 11.680922924593117\n",
      "itr: 1570 trn_acc 0.05092585189769934 trn_single_acc 0.8350861029089384 val_acc 0.027041902166241798\n",
      "itr: 1580 trn_loss 9.311756623418226 val_loss 11.639781849785173\n",
      "itr: 1580 trn_acc 0.050388807573109685 trn_single_acc 0.8363953703143583 val_acc 0.02735062448379492\n",
      "itr: 1590 trn_loss 9.217031751273714 val_loss 11.607472232464128\n",
      "itr: 1590 trn_acc 0.04892345007230971 trn_single_acc 0.8368760263547784 val_acc 0.027490316218409794\n",
      "itr: 1600 trn_loss 9.17411242434482 val_loss 11.575704119935489\n",
      "itr: 1600 trn_acc 0.053145192325835686 trn_single_acc 0.8387032997370777 val_acc 0.027430056641644707\n",
      "itr: 1610 trn_loss 9.139033936195819 val_loss 11.560842104853561\n",
      "itr: 1610 trn_acc 0.05336646527226813 trn_single_acc 0.8399910167594762 val_acc 0.027540550144147698\n",
      "itr: 1620 trn_loss 9.079516305552094 val_loss 11.535191739888226\n",
      "itr: 1620 trn_acc 0.05277241033705091 trn_single_acc 0.8391013397157332 val_acc 0.027756897444991907\n",
      "itr: 1630 trn_loss 9.078262582193656 val_loss 11.501804344616446\n",
      "itr: 1630 trn_acc 0.051525765320546285 trn_single_acc 0.8396886197893787 val_acc 0.027999433988751748\n",
      "itr: 1640 trn_loss 8.988347304930828 val_loss 11.466277402128679\n",
      "itr: 1640 trn_acc 0.0590057979116924 trn_single_acc 0.8412083438204709 val_acc 0.028472778377910058\n",
      "itr: 1650 trn_loss 8.918999042956482 val_loss 11.434368289845498\n",
      "itr: 1650 trn_acc 0.054450146411376625 trn_single_acc 0.8419420192220115 val_acc 0.0288562779229697\n",
      "itr: 1660 trn_loss 8.976435016458204 val_loss 11.409236571792345\n",
      "itr: 1660 trn_acc 0.057670082050263555 trn_single_acc 0.8435181957164464 val_acc 0.02890916973517684\n",
      "itr: 1670 trn_loss 8.996017005597418 val_loss 11.38837078357063\n",
      "itr: 1670 trn_acc 0.05624916934113117 trn_single_acc 0.8419873586597671 val_acc 0.02915869595259143\n",
      "itr: 1680 trn_loss 9.008008665672032 val_loss 11.356802038648626\n",
      "itr: 1680 trn_acc 0.056263474454427 trn_single_acc 0.8419885906382746 val_acc 0.029505486264673295\n",
      "itr: 1690 trn_loss 9.00617294290354 val_loss 11.322868107824535\n",
      "itr: 1690 trn_acc 0.055087815958157066 trn_single_acc 0.8407769828616799 val_acc 0.029737890861792047\n",
      "itr: 1700 trn_loss 8.874245201372842 val_loss 11.294786905410001\n",
      "itr: 1700 trn_acc 0.05988608040321236 trn_single_acc 0.8427158550034731 val_acc 0.030053330825891508\n",
      "itr: 1710 trn_loss 8.885338009882384 val_loss 11.26527025863121\n",
      "itr: 1710 trn_acc 0.050124127377601524 trn_single_acc 0.8407773414659562 val_acc 0.030294716388398184\n",
      "itr: 1720 trn_loss 8.885428733223437 val_loss 11.237196754496605\n",
      "itr: 1720 trn_acc 0.052025011402817685 trn_single_acc 0.8410981158970706 val_acc 0.030581042570245662\n",
      "itr: 1730 trn_loss 8.731441845861971 val_loss 11.211814538611154\n",
      "itr: 1730 trn_acc 0.054142086477197315 trn_single_acc 0.8438239864836372 val_acc 0.03079622610125458\n",
      "itr: 1740 trn_loss 8.780562567187383 val_loss 11.202802827334267\n",
      "itr: 1740 trn_acc 0.05642288484998784 trn_single_acc 0.841836944265401 val_acc 0.030958008382143222\n",
      "itr: 1750 trn_loss 8.85997906000877 val_loss 11.216123733260508\n",
      "itr: 1750 trn_acc 0.05553403888559194 trn_single_acc 0.8425713387733875 val_acc 0.030986709286351482\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-e5a01c62fdd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mcmd_lengths\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mcmd_lengths_np\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_samples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         }\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpercent_fully_correct\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_feed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0meval_itr\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mval_loss_avg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_itr = -1\n",
    "bs = 256\n",
    "for itr in range(100000):\n",
    "    samples = np.random.choice(trn_samples, size = bs, replace = False)\n",
    "    if True:#itr == 0:\n",
    "        trn_feed_dict = {\n",
    "            cmd_ind : cmd_np[samples],\n",
    "            act_ind : act_np[samples],\n",
    "            mask_ph : mask_np[samples],\n",
    "            act_lengths : np.clip(struct_np[samples], a_min = 1, a_max = None),\n",
    "            cmd_lengths : cmd_lengths_np[samples],\n",
    "        }\n",
    "        \n",
    "    trn_feed_dict[learning_rate] = .01 / (np.power(itr + 10, .6))\n",
    "    _, trn_loss, acc_trn_single, acc_trn = sess.run(\n",
    "        [optimizer, loss, percent_correct, percent_fully_correct], trn_feed_dict)\n",
    "    if itr == 0:\n",
    "        trn_loss_avg = trn_loss\n",
    "        acc_trn_avg = acc_trn\n",
    "        acc_trn_single_avg = acc_trn_single\n",
    "    else:\n",
    "        trn_loss_avg = trn_loss_avg * .9 + trn_loss * .1\n",
    "        acc_trn_avg = acc_trn_avg * .9 + acc_trn * .1\n",
    "        acc_trn_single_avg = acc_trn_single_avg * .9 + acc_trn_single * .1\n",
    "    if itr % 10 == 0 and itr > 0:\n",
    "        # val_samples = np.random.choice(val_samples_all, size = bs, replace = False)\n",
    "        eval_itr += 1\n",
    "        val_feed_dict = {\n",
    "            cmd_ind : cmd_np[val_samples],\n",
    "            act_ind : act_np[val_samples],\n",
    "            mask_ph : mask_np[val_samples],\n",
    "            act_lengths : np.clip(struct_np[val_samples], a_min = 1, a_max = None),\n",
    "            cmd_lengths : cmd_lengths_np[val_samples]\n",
    "        }\n",
    "        val_loss, acc_val = sess.run([loss, percent_fully_correct], val_feed_dict)\n",
    "        if eval_itr == 0:\n",
    "            val_loss_avg = val_loss\n",
    "            acc_val_avg = acc_val\n",
    "        else:\n",
    "            val_loss_avg = val_loss_avg * .9 + val_loss * .1\n",
    "            acc_val_avg = acc_val_avg * .9 + acc_val * .1\n",
    "        print('itr:', itr, 'trn_loss', trn_loss_avg, 'val_loss', val_loss_avg)\n",
    "        print('itr:', itr, 'trn_acc', acc_trn_avg, \n",
    "              'trn_single_acc', acc_trn_single_avg, 'val_acc', acc_val_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_percent.shape, percent_fully_correct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "10, None, 7, 9, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmd_np.shape, act_np.shape, mask_np.shape, struct_np.shape, cmd_lengths_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cmd_ind.shape, act_ind.shape, mask_ph.shape, act_lengths.shape, cmd_lengths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_samples.shape, val_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "act_presoftmax = tf.stack(action_probabilities_presoftmax, 1)[:, :, :-1, :]\n",
    "#batch, subprogram, timestep, action_selection\n",
    "logprobabilities = tf.nn.log_softmax(act_presoftmax, -1)\n",
    "act_presoftmax_flat = tf.reshape(act_presoftmax, [-1, 9, num_act])\n",
    "mask_ph_flat = tf.reshape(mask_ph, [-1, max_actions_per_subprogram])\n",
    "act_ind_flat = tf.reshape(act_ind, [-1, max_actions_per_subprogram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "act_presoftmax_flat = tf.reshape(act_presoftmax, [-1, 9, num_act])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_actions_per_subprogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess.run(act_presoftmax, feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(*actions_ind[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "command_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subprogram_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subprogram_last_layer[:,cmd_lengths,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoding_last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.gather(\n",
    "    encoding_last_layer,\n",
    "    [1,2],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.gather_nd(\n",
    "    encoding_last_layer,\n",
    "    np.array([[0,1,2,3,4], [1,4,3,2,5]]).T,\n",
    "    name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmd_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_command(sub_cmd, num_repeat):\n",
    "    return sub_cmd * num_repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_command(cmd):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uni_commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uni_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uni_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
