{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import common dependencies\n",
    "import pandas as pd  # noqa\n",
    "import numpy as np\n",
    "import matplotlib  # noqa\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime  # noqa\n",
    "import PIL  # noqa\n",
    "import glob  # noqa\n",
    "import pickle  # noqa\n",
    "from pathlib import Path  # noqa\n",
    "from scipy import misc  # noqa\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import pdb\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "TRADE_COST_FRAC = .003\n",
    "EPSILON = 1e-10\n",
    "ADV_MULT = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uni_tokens = set()\n",
    "uni_commands = set()\n",
    "uni_actions = set()\n",
    "fname = 'tasks_with_length_tags.txt'\n",
    "with open(fname) as f:\n",
    "    content = f.readlines()\n",
    "content2 = [c.split(' ') for c in content]\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "commands = []\n",
    "actions = []\n",
    "content = [l.replace('\\n', '') for l in content]\n",
    "commands = [x.split(':::')[1].split(' ')[1:-1] for x in content]\n",
    "actions = [x.split(':::')[2].split(' ')[1:-2] for x in content]\n",
    "structures = [x.split(':::')[3].split(' ')[2:] for x in content]\n",
    "\n",
    "structures = [[int(l) for l in program] for program in structures]\n",
    "#actions = [[wd.replace('\\n', '') for wd in res] for res in actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 10, 49, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_actions_per_subprogram = max([max([s for s in struct]) for struct in structures]) + 1\n",
    "max_num_subprograms = max([len(s) for s in structures]) + 1\n",
    "max_cmd_len = max([len(s) for s in commands]) + 1\n",
    "max_act_len = max([len(a) for a in actions]) + 1\n",
    "cmd_lengths_list = [len(s)+1 for s in commands]\n",
    "cmd_lengths_np = np.array(cmd_lengths_list)\n",
    "max_num_subprograms, max_cmd_len, max_act_len, max_actions_per_subprogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_fmap_invmap(unique, num_unique):\n",
    "    fmap = dict(zip(unique, range(num_unique)))\n",
    "    invmap = dict(zip(range(num_unique), unique))\n",
    "    return fmap, invmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for li in content2:\n",
    "    for wd in li:\n",
    "        uni_tokens.add(wd)\n",
    "for li in commands:\n",
    "    for wd in li:\n",
    "        uni_commands.add(wd)\n",
    "for li in actions:\n",
    "    for wd in li:\n",
    "        uni_actions.add(wd)\n",
    "uni_commands.add('end_command')\n",
    "uni_actions.add('end_subprogram')\n",
    "uni_actions.add('end_action')\n",
    "num_cmd = len(uni_commands)\n",
    "num_act = len(uni_actions)\n",
    "command_map, command_invmap = build_fmap_invmap(uni_commands, num_cmd)\n",
    "action_map, action_invmap = build_fmap_invmap(uni_actions, num_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dense_scaled(prev_layer, layer_size, name=None, reuse=False, scale=1.0):\n",
    "    output = tf.layers.dense(prev_layer, layer_size, reuse=reuse) * scale\n",
    "    return output\n",
    "\n",
    "\n",
    "def dense_relu(dense_input, layer_size, scale=1.0):\n",
    "    dense = dense_scaled(dense_input, layer_size, scale=scale)\n",
    "    output = tf.nn.leaky_relu(dense)\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_grad_norm(opt_fcn, loss):\n",
    "    gvs = opt_fcn.compute_gradients(loss)\n",
    "    grad_norm = tf.sqrt(tf.reduce_sum(\n",
    "        [tf.reduce_sum(tf.square(grad)) for grad, var in gvs if grad is not None]))\n",
    "    return grad_norm\n",
    "\n",
    "\n",
    "def apply_clipped_optimizer(opt_fcn, loss, clip_norm=.1, clip_single=.03, clip_global_norm=False):\n",
    "    gvs = opt_fcn.compute_gradients(loss)\n",
    "\n",
    "    if clip_global_norm:\n",
    "        gs, vs = zip(*[(g, v) for g, v in gvs if g is not None])\n",
    "        capped_gs, grad_norm_total = tf.clip_by_global_norm([g for g in gs], clip_norm)\n",
    "        capped_gvs = list(zip(capped_gs, vs))\n",
    "    else:\n",
    "        grad_norm_total = tf.sqrt(\n",
    "            tf.reduce_sum([tf.reduce_sum(tf.square(grad)) for grad, var in gvs if grad is not None]))\n",
    "        capped_gvs = [(tf.clip_by_value(grad, -1 * clip_single, clip_single), var)\n",
    "                      for grad, var in gvs if grad is not None]\n",
    "        capped_gvs = [(tf.clip_by_norm(grad, clip_norm), var) for grad, var in capped_gvs if grad is not None]\n",
    "\n",
    "    optimizer = opt_fcn.apply_gradients(capped_gvs)\n",
    "\n",
    "    return optimizer, grad_norm_total\n",
    "\n",
    "\n",
    "def mlp(x, hidden_sizes, output_size=None, name='', reuse=False):\n",
    "    prev_layer = x\n",
    "\n",
    "    for idx, l in enumerate(hidden_sizes):\n",
    "        dense = dense_scaled(prev_layer, l, name='mlp' + name + '_' + str(idx))\n",
    "        prev_layer = tf.nn.leaky_relu(dense)\n",
    "\n",
    "    output = prev_layer\n",
    "\n",
    "    if output_size is not None:\n",
    "        output = dense_scaled(prev_layer, output_size, name='mlp' + name + 'final')\n",
    "\n",
    "    return output\n",
    "\n",
    "def mlp_with_adversaries(x, hidden_sizes, output_size=None, name='', reuse=False):\n",
    "    prev_layer = x\n",
    "    adv_phs = []\n",
    "    for idx, l in enumerate(hidden_sizes):\n",
    "        \n",
    "        adversary = tf.placeholder_with_default(tf.zeros_like(prev_layer), prev_layer.shape)\n",
    "        prev_layer = prev_layer + adversary\n",
    "        adv_phs.append(adversary)\n",
    "        \n",
    "        dense = dense_scaled(prev_layer, l, name='mlp' + name + '_' + str(idx))\n",
    "        prev_layer = tf.nn.leaky_relu(dense)\n",
    "\n",
    "    output = prev_layer\n",
    "\n",
    "    if output_size is not None:\n",
    "        output = dense_scaled(prev_layer, output_size, name='mlp' + name + 'final')\n",
    "\n",
    "    return output, adv_phs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "commands_ind = [[command_map[c] for c in cmd] + [0] * (max_cmd_len - len(cmd)) for cmd in commands]\n",
    "actions_ind = [[action_map[a] for a in act] + [0] * (max_act_len - len(act)) for act in actions]\n",
    "cmd_np = np.array(commands_ind)\n",
    "actions_structured = []\n",
    "mask_structured = []\n",
    "for row in range(len(structures)):\n",
    "    mask_row = []\n",
    "    action_row = []\n",
    "    act = actions_ind[row]\n",
    "    struct = structures[row]\n",
    "    start = 0\n",
    "    for step in struct:\n",
    "        end = start + step\n",
    "        a = act[start:end]\n",
    "        padding = max_actions_per_subprogram - step - 1\n",
    "        action_row.append(a + [action_map['end_action']] + [0] * padding)\n",
    "        start = end\n",
    "    actions_structured.append(\n",
    "        action_row + [[action_map['end_subprogram']] + [0] * (max_actions_per_subprogram - 1)] +\n",
    "        [[0] * max_actions_per_subprogram] * (max_num_subprograms - len(struct) - 1)\n",
    "    )\n",
    "act_np = np.array(actions_structured)\n",
    "struct_padded = [[sa + 1 for sa in s] + [1] + [0] * (max_num_subprograms - len(s) - 1) for s in structures]\n",
    "struct_np = np.array(struct_padded)\n",
    "\n",
    "mask_list = [[np.concatenate((np.ones(st), np.zeros(max_actions_per_subprogram - st)), 0) \n",
    "              for st in s] for s in struct_np]\n",
    "mask_np = np.array(mask_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\clip_ops.py:110: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "default_sizes = 128\n",
    "size_emb = 64\n",
    "num_layers_encoder = 6\n",
    "hidden_filters = 256\n",
    "num_layers_subprogram = 3\n",
    "hidden_filters_subprogram = 256\n",
    "init_mag = 1e-2\n",
    "cmd_mat = tf.Variable(init_mag*tf.random_normal([num_cmd, size_emb]))\n",
    "act_mat = tf.Variable(init_mag*tf.random_normal([num_act, size_emb]))\n",
    "act_st_emb = tf.Variable(init_mag*tf.random_normal([size_emb]))\n",
    "global_bs = None\n",
    "global_time_len = 7\n",
    "action_lengths = None\n",
    "max_num_actions= None\n",
    "# global_bs = 8\n",
    "global_time_len = 7\n",
    "max_num_actions = 9\n",
    "output_keep_prob = tf.placeholder_with_default(1.0, ())\n",
    "state_keep_prob = tf.placeholder_with_default(1.0, ())\n",
    "cmd_ind = tf.placeholder(tf.int32, shape=(global_bs, 10,))\n",
    "act_ind = tf.placeholder(tf.int32, shape=(global_bs, global_time_len, 9))\n",
    "mask_ph = tf.placeholder(tf.float32, shape=(global_bs, global_time_len, 9))\n",
    "cmd_lengths = tf.placeholder(tf.int32, shape=(global_bs,))\n",
    "act_lengths = tf.placeholder(tf.int32, shape=(global_bs, 7))\n",
    "learning_rate = tf.placeholder(tf.float32, shape = (None))\n",
    "\n",
    "cmd_emb = tf.nn.embedding_lookup(cmd_mat, cmd_ind)\n",
    "act_emb = tf.nn.embedding_lookup(act_mat, act_ind)\n",
    "tf_bs = tf.shape(act_ind)[0]\n",
    "act_st_emb_expanded = tf.tile(tf.reshape(\n",
    "    act_st_emb, [1, 1, 1, size_emb]), [tf_bs, global_time_len, 1, 1])\n",
    "act_emb_with_st = tf.concat((act_st_emb_expanded, act_emb), 2)\n",
    "\n",
    "first_cell_encoder = [tf.nn.rnn_cell.LSTMCell(\n",
    "    hidden_filters, forget_bias=1., name = 'layer1_'+d) for d in ['f', 'b']]\n",
    "hidden_cells_encoder = [[tf.nn.rnn_cell.LSTMCell(\n",
    "    hidden_filters,forget_bias=1., name = 'layer' + str(lidx) + '_' + d)  for d in ['f', 'b']]\n",
    "                        for lidx in range(num_layers_encoder - 1)]\n",
    "hidden_cells_encoder = [[tf.nn.rnn_cell.DropoutWrapper(cell,\n",
    "    output_keep_prob=output_keep_prob, state_keep_prob=state_keep_prob,\n",
    "    variational_recurrent=True, dtype=tf.float32) for cell in cells] for cells in hidden_cells_encoder[:-1]] + [hidden_cells_encoder[-1]]\n",
    "cells_encoder = [first_cell_encoder] + hidden_cells_encoder\n",
    "c1, c2 = zip(*cells_encoder)\n",
    "cells_encoder = [c1, c2]\n",
    "def encode(x, num_layers, cells, initial_states, lengths, name='',):\n",
    "    prev_layer = x\n",
    "    shortcut = x\n",
    "    hiddenlayers = []\n",
    "    returncells = []\n",
    "    cell_fw, cell_bw = cells\n",
    "    bs = tf.shape(x)[0]\n",
    "    for idx in range(num_layers):\n",
    "        prev_layer, c = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw = cell_fw[idx],\n",
    "                cell_bw = cell_bw[idx],\n",
    "                inputs = prev_layer,\n",
    "                sequence_length=lengths,\n",
    "                initial_state_fw=None,\n",
    "                initial_state_bw=None,\n",
    "                dtype=tf.float32,\n",
    "                scope='encoder'+str(idx)\n",
    "            )\n",
    "        prev_layer = tf.concat(prev_layer, 2)\n",
    "        prev_layer = tf.nn.leaky_relu(prev_layer)\n",
    "        returncells.append(c)\n",
    "        hiddenlayers.append(prev_layer)\n",
    "        if idx == num_layers - 1:\n",
    "            #pdb.set_trace()\n",
    "            output = tf.gather_nd(\n",
    "                        prev_layer,\n",
    "                        tf.stack([tf.range(bs), lengths], 1),\n",
    "                        name=None\n",
    "                    )\n",
    "            return prev_layer, returncells, hiddenlayers, output\n",
    "        prev_layer = tf.concat((prev_layer, shortcut), 2)\n",
    "encoding_last_layer, encoding_final_cells, encoding_hidden_layers, encoding_last_timestep = encode(\n",
    "    cmd_emb, num_layers_encoder, cells_encoder,None, lengths = cmd_lengths, name = 'encoder')\n",
    "# encoding_last_timestep = encoding_last_layer[:,cmd_lengths, :]\n",
    "hidden_filters_encoder = encoding_last_timestep.shape[-1].value\n",
    "first_cell_subprogram = tf.nn.rnn_cell.LSTMCell(\n",
    "    hidden_filters_subprogram, forget_bias=1., name = 'subpogramlayer1_')\n",
    "hidden_cells_subprogram = [tf.nn.rnn_cell.LSTMCell(\n",
    "    hidden_filters_subprogram,forget_bias=1., name = 'subpogramlayer' + str(lidx))\n",
    "                        for lidx in range(num_layers_subprogram - 1)]\n",
    "\n",
    "cells_subprogram_rnn = [first_cell_subprogram] + hidden_cells_subprogram\n",
    "\n",
    "attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "    num_units=hidden_filters_encoder, memory=encoding_last_layer,\n",
    "    memory_sequence_length=cmd_lengths)\n",
    "attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "    num_units=hidden_filters_encoder//2, memory=encoding_last_layer,\n",
    "    memory_sequence_length=cmd_lengths)\n",
    "cells_subprogram = [\n",
    "    tf.contrib.seq2seq.AttentionWrapper(\n",
    "        cell, attention_mechanism, attention_layer_size = hidden_filters_subprogram) \n",
    "    for cell in cells_subprogram_rnn]\n",
    "\n",
    "def subprogram(x, num_layers, cells, initial_states, lengths, reuse, name='',):\n",
    "    prev_layer = x\n",
    "    shortcut = x\n",
    "    hiddenlayers = []\n",
    "    returncells = []\n",
    "    bs = tf.shape(x)[0]\n",
    "    for idx in range(num_layers):\n",
    "        print(idx)\n",
    "        if idx == 0:\n",
    "            num_past_units = hidden_filters\n",
    "        else:\n",
    "            num_past_units = hidden_filters_subprogram\n",
    "        with tf.variable_scope(name + 'subprogram' + str(idx), reuse=reuse):\n",
    "#             self_attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "#                 num_units=num_past_units, memory=prev_layer,\n",
    "#                 memory_sequence_length=tf.expand_dims(tf.range(10), 0))\n",
    "#             cell_with_selfattention = tf.contrib.seq2seq.AttentionWrapper(\n",
    "#                     cells[idx], self_attention_mechanism, attention_layer_size = num_past_units)\n",
    "\n",
    "            prev_layer, c = tf.nn.dynamic_rnn(\n",
    "                    cell = cells[idx],\n",
    "                    inputs = prev_layer,\n",
    "                    sequence_length=lengths,\n",
    "                    initial_state = None,\n",
    "                    dtype=tf.float32,\n",
    "                )\n",
    "            prev_layer = tf.concat(prev_layer, 2)\n",
    "            prev_layer = tf.nn.leaky_relu(prev_layer)\n",
    "            returncells.append(c)\n",
    "            hiddenlayers.append(prev_layer)\n",
    "            if idx == num_layers - 1:\n",
    "                output = tf.gather_nd(\n",
    "                            prev_layer,\n",
    "                            tf.stack([tf.range(bs), lengths], 1),\n",
    "                            name=None\n",
    "                        )\n",
    "                return prev_layer, returncells, hiddenlayers, output\n",
    "            prev_layer = tf.concat((prev_layer, shortcut), 2)\n",
    "encodings = [encoding_last_timestep]\n",
    "last_encoding = encoding_last_timestep\n",
    "initial_cmb_encoding = last_encoding\n",
    "loss = 0\n",
    "action_probabilities_presoftmax = []\n",
    "for sub_idx in range(max_num_subprograms): \n",
    "    from_last_layer = tf.tile(tf.expand_dims(tf.concat((\n",
    "        last_encoding, initial_cmb_encoding), 1), 1), [1, max_num_actions + 1, 1])\n",
    "    autoregressive = act_emb_with_st[:,sub_idx, :, :]\n",
    "    x_input = tf.concat((from_last_layer, autoregressive), -1)\n",
    "    subprogram_last_layer, _, subprogram_hidden_layers, subprogram_output = subprogram(\n",
    "        x_input, num_layers_subprogram, cells_subprogram,None, \n",
    "        lengths = act_lengths[:, sub_idx], reuse = (sub_idx > 0), name = 'subprogram')\n",
    "    action_prob_flat = mlp(\n",
    "        tf.reshape(subprogram_last_layer, [-1, hidden_filters_subprogram]),\n",
    "        [], output_size = num_act, name = 'action_choice_mlp', reuse = (sub_idx > 0))\n",
    "    action_prob_expanded = tf.reshape(action_prob_flat, [-1, max_num_actions + 1, num_act])\n",
    "    action_probabilities_layer = tf.nn.softmax(action_prob_expanded, axis=-1)\n",
    "    action_probabilities_presoftmax.append(action_prob_expanded)\n",
    "    delta = mlp(\n",
    "        subprogram_output, [64], output_size = hidden_filters_encoder, name = 'global_transform',\n",
    "        reuse = (sub_idx > 0)\n",
    "    )\n",
    "    last_encoding = last_encoding + delta\n",
    "    encodings.append(last_encoding)\n",
    "act_presoftmax = tf.stack(action_probabilities_presoftmax, 1)[:, :, :-1, :]\n",
    "#batch, subprogram, timestep, action_selection\n",
    "logprobabilities = tf.nn.log_softmax(act_presoftmax, -1)\n",
    "act_presoftmax_flat = tf.reshape(act_presoftmax, [-1, 9, num_act])\n",
    "mask_ph_flat = tf.reshape(mask_ph, [-1, max_actions_per_subprogram])\n",
    "act_ind_flat = tf.reshape(act_ind, [-1, max_actions_per_subprogram])\n",
    "ppl_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "    logits = act_presoftmax_flat,\n",
    "    targets = act_ind_flat,\n",
    "    weights = mask_ph_flat,\n",
    "    average_across_timesteps=False,\n",
    "    average_across_batch=False,\n",
    "    softmax_loss_function=None,\n",
    "    name=None\n",
    ")\n",
    "ppl_loss_avg = tf.reduce_mean(tf.pow(ppl_loss, 2.0)) * 100\n",
    "\n",
    "tfvars = tf.trainable_variables()\n",
    "weight_norm = tf.reduce_mean([tf.reduce_sum(tf.square(var)) for var in tfvars])*1e-3\n",
    "\n",
    "action_taken = tf.argmax(act_presoftmax, -1, output_type=tf.int32)\n",
    "correct_mat = tf.cast(tf.equal(action_taken, act_ind), tf.float32) * mask_ph\n",
    "correct_percent = tf.reduce_sum(correct_mat, [1, 2])/tf.reduce_sum(mask_ph, [1, 2])\n",
    "percent_correct = tf.reduce_mean(correct_percent)\n",
    "percent_fully_correct = tf.reduce_mean(tf.cast(tf.equal(correct_percent, 1), tf.float32))\n",
    "\n",
    "loss = ppl_loss_avg + weight_norm\n",
    "\n",
    "opt_fcn = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "#opt_fcn = tf.train.MomentumOptimizer(learning_rate=learning_rate, use_nesterov=True, momentum=.8)\n",
    "optimizer, grad_norm_total = apply_clipped_optimizer(opt_fcn, loss)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'LeakyRelu_5/Maximum:0' shape=(?, 10, 512) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2091,), (18819,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "trn_percent = .1\n",
    "num_samples = mask_np.shape[0]\n",
    "ordered_samples = np.arange(num_samples)\n",
    "np.random.shuffle(ordered_samples)\n",
    "trn_samples = ordered_samples[:int(np.ceil(num_samples*trn_percent))]\n",
    "val_samples_all = ordered_samples[int(np.ceil(num_samples*trn_percent)):]\n",
    "val_samples = val_samples_all\n",
    "trn_samples.shape, val_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 10 trn_loss 171.88877335730547 val_loss 97.90374\n",
      "itr: 10 trn_acc 0.0 trn_single_acc 0.26464871313560534 val_acc 0.0\n",
      "itr: 20 trn_loss 111.48723199601984 val_loss 94.89252624511718\n",
      "itr: 20 trn_acc 6.845724572766341e-05 trn_single_acc 0.3878960674802778 val_acc 0.0\n",
      "itr: 30 trn_loss 77.65564834730127 val_loss 90.65615676879882\n",
      "itr: 30 trn_acc 8.066253584870629e-05 trn_single_acc 0.48196618021995397 val_acc 0.0\n",
      "itr: 40 trn_loss 56.266743356132025 val_loss 85.6872863922119\n",
      "itr: 40 trn_acc 0.00012423900843029894 trn_single_acc 0.5483913732598724 val_acc 7.970667793415487e-05\n",
      "itr: 50 trn_loss 43.22734291933314 val_loss 80.63781930389403\n",
      "itr: 50 trn_acc 0.0009518420326866293 trn_single_acc 0.6042036293736127 val_acc 0.000459641850902699\n",
      "itr: 60 trn_loss 35.191476943770276 val_loss 75.54565259262085\n",
      "itr: 60 trn_acc 0.00288096822256608 trn_single_acc 0.643757840019386 val_acc 0.0006899941586307251\n",
      "itr: 70 trn_loss 28.869712011315986 val_loss 70.49640254179384\n",
      "itr: 70 trn_acc 0.004915843971808494 trn_single_acc 0.6705836522593581 val_acc 0.0008866836808563677\n",
      "itr: 80 trn_loss 24.03643836439806 val_loss 65.562466061113\n",
      "itr: 80 trn_acc 0.00562255179418427 trn_single_acc 0.6867027934268656 val_acc 0.0012443727045453855\n",
      "itr: 90 trn_loss 20.04333669695463 val_loss 60.828915377853264\n",
      "itr: 90 trn_acc 0.00801331021734937 trn_single_acc 0.7098557735413357 val_acc 0.001816040431859845\n",
      "itr: 100 trn_loss 17.51677488416631 val_loss 56.362633794901924\n",
      "itr: 100 trn_acc 0.009122206133366327 trn_single_acc 0.7249770845820079 val_acc 0.0025909165006006546\n",
      "itr: 110 trn_loss 15.546664534621517 val_loss 52.31838472815588\n",
      "itr: 110 trn_acc 0.010029940943154462 trn_single_acc 0.7372075259546585 val_acc 0.0031554605516306866\n",
      "itr: 120 trn_loss 14.184355964032708 val_loss 48.53863976119966\n",
      "itr: 120 trn_acc 0.011073900045403854 trn_single_acc 0.7465951250576817 val_acc 0.0038389049204449934\n",
      "itr: 130 trn_loss 13.202028017976001 val_loss 45.0743657623441\n",
      "itr: 130 trn_acc 0.01269725156797847 trn_single_acc 0.7550339204499077 val_acc 0.004714379919969537\n",
      "itr: 140 trn_loss 12.535153004745313 val_loss 42.03733524156013\n",
      "itr: 140 trn_acc 0.015589954782218213 trn_single_acc 0.7629951526518344 val_acc 0.00526318746140912\n",
      "itr: 150 trn_loss 12.679654876117382 val_loss 39.40099978869318\n",
      "itr: 150 trn_acc 0.015790351115050276 trn_single_acc 0.7625706641773324 val_acc 0.0054223461583076255\n",
      "itr: 160 trn_loss 12.223696008414056 val_loss 36.97574241541468\n",
      "itr: 160 trn_acc 0.017354890505753527 trn_single_acc 0.768523901179114 val_acc 0.005857846763862819\n",
      "itr: 170 trn_loss 11.689124272044063 val_loss 34.58078963383415\n",
      "itr: 170 trn_acc 0.018867949860759965 trn_single_acc 0.7739936214038288 val_acc 0.006425152031749771\n",
      "itr: 180 trn_loss 11.066540324710227 val_loss 32.31712767026763\n",
      "itr: 180 trn_acc 0.022503560002352863 trn_single_acc 0.7844758365695843 val_acc 0.007132336605194467\n",
      "itr: 190 trn_loss 10.432365030110017 val_loss 30.240234391766258\n",
      "itr: 190 trn_acc 0.02785726468671431 trn_single_acc 0.795022049620248 val_acc 0.008071688100936942\n",
      "itr: 200 trn_loss 9.834458990124878 val_loss 28.30000897962821\n",
      "itr: 200 trn_acc 0.031913836756781194 trn_single_acc 0.8054467907339387 val_acc 0.009326265374043225\n",
      "itr: 210 trn_loss 9.259329022023817 val_loss 26.550294107666367\n",
      "itr: 210 trn_acc 0.03743047337629499 trn_single_acc 0.8147888500645409 val_acc 0.010588229330675577\n",
      "itr: 220 trn_loss 8.64058746079095 val_loss 24.90028645974763\n",
      "itr: 220 trn_acc 0.04627587871093306 trn_single_acc 0.8257501653634864 val_acc 0.012420102029112078\n",
      "itr: 230 trn_loss 8.007040508894363 val_loss 23.32274999364347\n",
      "itr: 230 trn_acc 0.05723767768369698 trn_single_acc 0.8376057650774068 val_acc 0.015078405203581242\n",
      "itr: 240 trn_loss 7.457954780046195 val_loss 21.83565550575373\n",
      "itr: 240 trn_acc 0.06712837967140112 trn_single_acc 0.8466429148071423 val_acc 0.01888434335186516\n",
      "itr: 250 trn_loss 7.1957064394097126 val_loss 20.531708695260146\n",
      "itr: 250 trn_acc 0.06816576465276997 trn_single_acc 0.8502092890581892 val_acc 0.02141697290177138\n",
      "itr: 260 trn_loss 6.745817599150785 val_loss 19.380696849019046\n",
      "itr: 260 trn_acc 0.07686541936502632 trn_single_acc 0.8578889803040235 val_acc 0.022442287572935146\n",
      "itr: 270 trn_loss 6.556584562318105 val_loss 18.296790715447706\n",
      "itr: 270 trn_acc 0.0855750512088489 trn_single_acc 0.8616738148186059 val_acc 0.026468317689342727\n",
      "itr: 280 trn_loss 6.118182666460477 val_loss 17.235086640469707\n",
      "itr: 280 trn_acc 0.10245336172464337 trn_single_acc 0.8699828036445959 val_acc 0.029985468967416967\n",
      "itr: 290 trn_loss 5.720261607517174 val_loss 16.253115356641242\n",
      "itr: 290 trn_acc 0.11751700308878144 trn_single_acc 0.8773975654513665 val_acc 0.03412332658498801\n",
      "itr: 300 trn_loss 5.399119427139016 val_loss 15.353899974236395\n",
      "itr: 300 trn_acc 0.12769787456016893 trn_single_acc 0.8831115517003758 val_acc 0.03832563854333148\n",
      "itr: 310 trn_loss 5.123028060175142 val_loss 14.552408997961438\n",
      "itr: 310 trn_acc 0.1335709226317893 trn_single_acc 0.8874978832721987 val_acc 0.041411614540902246\n",
      "itr: 320 trn_loss 4.876701185357496 val_loss 13.804542813970349\n",
      "itr: 320 trn_acc 0.1431301011628744 trn_single_acc 0.8917774232184372 val_acc 0.0445237609359808\n",
      "itr: 330 trn_loss 4.651484221763197 val_loss 13.109929377681347\n",
      "itr: 330 trn_acc 0.1541911708298628 trn_single_acc 0.8958859063258358 val_acc 0.0485521758448947\n",
      "itr: 340 trn_loss 4.512689064073021 val_loss 12.493778969210087\n",
      "itr: 340 trn_acc 0.1633800516666544 trn_single_acc 0.8983607724117411 val_acc 0.052496575997936896\n",
      "itr: 350 trn_loss 4.339985541594631 val_loss 11.929206227089495\n",
      "itr: 350 trn_acc 0.16781703067948064 trn_single_acc 0.9008592785223757 val_acc 0.055796788203717626\n",
      "itr: 360 trn_loss 4.2010304054556835 val_loss 11.425144161127983\n",
      "itr: 360 trn_acc 0.1877463344829094 trn_single_acc 0.9042204037320292 val_acc 0.05905392339944961\n",
      "itr: 370 trn_loss 3.97738329261768 val_loss 10.968576734074635\n",
      "itr: 370 trn_acc 0.20752411303592488 trn_single_acc 0.9090452702241825 val_acc 0.06196409005928149\n",
      "itr: 380 trn_loss 3.764825972367661 val_loss 10.571892009504452\n",
      "itr: 380 trn_acc 0.2269401305819 trn_single_acc 0.9133392974306012 val_acc 0.06599139084525207\n",
      "itr: 390 trn_loss 3.6327247946058203 val_loss 10.218403684789969\n",
      "itr: 390 trn_acc 0.23678269982850575 trn_single_acc 0.9162610916636452 val_acc 0.06946186193919748\n",
      "itr: 400 trn_loss 3.4442665442152487 val_loss 9.887753892864318\n",
      "itr: 400 trn_acc 0.26015961199871696 trn_single_acc 0.9204816668440514 val_acc 0.07321762563970599\n",
      "itr: 410 trn_loss 3.3310439600558444 val_loss 9.63157819115418\n",
      "itr: 410 trn_acc 0.2748472326369492 trn_single_acc 0.9231235062331067 val_acc 0.07724078019428474\n",
      "itr: 420 trn_loss 3.2278641952067653 val_loss 9.452364358931462\n",
      "itr: 420 trn_acc 0.2893359917780017 trn_single_acc 0.9252758304928592 val_acc 0.08078191298217972\n",
      "itr: 430 trn_loss 3.1402940113324656 val_loss 9.249692124393297\n",
      "itr: 430 trn_acc 0.29897537914912453 trn_single_acc 0.9270865950371117 val_acc 0.08485633314822376\n",
      "itr: 440 trn_loss 3.090912313527812 val_loss 9.154032184720082\n",
      "itr: 440 trn_acc 0.30548689211325103 trn_single_acc 0.9280334373187973 val_acc 0.08844360498643751\n",
      "itr: 450 trn_loss 3.100183302223053 val_loss 9.006588847076932\n",
      "itr: 450 trn_acc 0.3022610775317018 trn_single_acc 0.9281067310461331 val_acc 0.08962634463361056\n",
      "itr: 460 trn_loss 3.1883560282131724 val_loss 8.851696459027563\n",
      "itr: 460 trn_acc 0.31261504952560143 trn_single_acc 0.9290227667507232 val_acc 0.09372497730777697\n",
      "itr: 470 trn_loss 3.0562086001085556 val_loss 8.812103480910451\n",
      "itr: 470 trn_acc 0.3250622659434113 trn_single_acc 0.9308144141854985 val_acc 0.09641475722187194\n",
      "itr: 480 trn_loss 2.8575592048375733 val_loss 8.712103412284128\n",
      "itr: 480 trn_acc 0.3623211400362535 trn_single_acc 0.9354933527463338 val_acc 0.1006581849902281\n",
      "itr: 490 trn_loss 2.693413715631527 val_loss 8.687138550639455\n",
      "itr: 490 trn_acc 0.39190603379836675 trn_single_acc 0.9388011311243403 val_acc 0.10442944619501313\n",
      "itr: 500 trn_loss 2.6176550446128233 val_loss 8.687409871661934\n",
      "itr: 500 trn_acc 0.40992686691746894 trn_single_acc 0.9407730855275892 val_acc 0.10874817895988835\n",
      "itr: 510 trn_loss 2.5015261705280656 val_loss 8.700608295659071\n",
      "itr: 510 trn_acc 0.43750388194034384 trn_single_acc 0.944105712816015 val_acc 0.11367122496456099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 520 trn_loss 2.4705969514507835 val_loss 8.750074615293604\n",
      "itr: 520 trn_acc 0.4425666702079551 trn_single_acc 0.9448892689551943 val_acc 0.11829857675990635\n",
      "itr: 530 trn_loss 2.3531891003838195 val_loss 8.794410148454185\n",
      "itr: 530 trn_acc 0.4833554818484945 trn_single_acc 0.9486575223025012 val_acc 0.12410515083755112\n",
      "itr: 540 trn_loss 2.2274683807935243 val_loss 8.878780149691531\n",
      "itr: 540 trn_acc 0.5200701952478195 trn_single_acc 0.9524685021146867 val_acc 0.12986244440572015\n",
      "itr: 550 trn_loss 2.1275235018882115 val_loss 9.070551297014859\n",
      "itr: 550 trn_acc 0.5460898904485838 trn_single_acc 0.9555627793452967 val_acc 0.1355116219564964\n",
      "itr: 560 trn_loss 2.041377678138553 val_loss 9.25078845704726\n",
      "itr: 560 trn_acc 0.5709554962763145 trn_single_acc 0.9578663033621357 val_acc 0.14239193808239906\n",
      "itr: 570 trn_loss 1.9280465336020631 val_loss 9.470078545973884\n",
      "itr: 570 trn_acc 0.6112949724565573 trn_single_acc 0.9615643658951337 val_acc 0.149179366033097\n",
      "itr: 580 trn_loss 1.8357569455058191 val_loss 9.719492791443635\n",
      "itr: 580 trn_acc 0.6348342246131808 trn_single_acc 0.9640191071346578 val_acc 0.15796088141276232\n",
      "itr: 590 trn_loss 1.7088109456517215 val_loss 9.998941789581988\n",
      "itr: 590 trn_acc 0.6764625496532773 trn_single_acc 0.9680736228145701 val_acc 0.1661830719894808\n",
      "itr: 600 trn_loss 1.8622999980168036 val_loss 10.350245520932628\n",
      "itr: 600 trn_acc 0.6671656227193978 trn_single_acc 0.9677726445360604 val_acc 0.1712024712491013\n",
      "itr: 610 trn_loss 1.8297272581324815 val_loss 10.574030821439464\n",
      "itr: 610 trn_acc 0.6596982393943964 trn_single_acc 0.9683209872451558 val_acc 0.1809221211607436\n",
      "itr: 620 trn_loss 1.5860822469379805 val_loss 10.812059353217638\n",
      "itr: 620 trn_acc 0.7016784447568499 trn_single_acc 0.9724969661462429 val_acc 0.189600724297927\n",
      "itr: 630 trn_loss 1.433251193148954 val_loss 10.926200345538696\n",
      "itr: 630 trn_acc 0.7297571598285961 trn_single_acc 0.9756049540029107 val_acc 0.20008961407974318\n",
      "itr: 640 trn_loss 1.2743506824437538 val_loss 11.05123654191134\n",
      "itr: 640 trn_acc 0.763069732429747 trn_single_acc 0.9789045178991987 val_acc 0.21187298937411567\n",
      "itr: 650 trn_loss 1.1189801507992372 val_loss 11.232146041254142\n",
      "itr: 650 trn_acc 0.8086256103506668 trn_single_acc 0.9827279549955867 val_acc 0.22370019653831238\n",
      "itr: 660 trn_loss 0.9771216945330526 val_loss 11.560884752863592\n",
      "itr: 660 trn_acc 0.8494581794611484 trn_single_acc 0.9861121230892422 val_acc 0.2354818323200433\n",
      "itr: 670 trn_loss 0.8990068157275616 val_loss 11.807798730427574\n",
      "itr: 670 trn_acc 0.8682036206587912 trn_single_acc 0.9878080076742933 val_acc 0.2464838390599628\n",
      "itr: 680 trn_loss 0.8044756145797052 val_loss 12.030468869591848\n",
      "itr: 680 trn_acc 0.8957729681978458 trn_single_acc 0.9900119440065016 val_acc 0.2595048306927925\n",
      "itr: 690 trn_loss 0.7277094827525395 val_loss 12.448372109280614\n",
      "itr: 690 trn_acc 0.9139370278668275 trn_single_acc 0.991736875508807 val_acc 0.26967210101001077\n",
      "itr: 700 trn_loss 0.6821408636355097 val_loss 12.736433991980483\n",
      "itr: 700 trn_acc 0.9239110377192007 trn_single_acc 0.992725027289063 val_acc 0.28101191991787505\n",
      "itr: 710 trn_loss 0.6483462377478859 val_loss 13.063356693856655\n",
      "itr: 710 trn_acc 0.9287949138748801 trn_single_acc 0.9933850210661839 val_acc 0.29095738298494833\n",
      "itr: 720 trn_loss 0.6083472312213425 val_loss 13.377005460506146\n",
      "itr: 720 trn_acc 0.9413237232577574 trn_single_acc 0.9942832694817355 val_acc 0.29989767223715086\n",
      "itr: 730 trn_loss 0.5838042102736677 val_loss 13.698236837368128\n",
      "itr: 730 trn_acc 0.9458489546036426 trn_single_acc 0.9946905053440637 val_acc 0.310292623789185\n",
      "itr: 740 trn_loss 0.6337016657990269 val_loss 14.052090788824186\n",
      "itr: 740 trn_acc 0.9267112613142513 trn_single_acc 0.9934730204079727 val_acc 0.31805394502080114\n",
      "itr: 750 trn_loss 0.6084654831979239 val_loss 14.188098846324825\n",
      "itr: 750 trn_acc 0.9310820916739049 trn_single_acc 0.9937644975096034 val_acc 0.3267129756056962\n",
      "itr: 760 trn_loss 0.5345345256862268 val_loss 14.358912699027549\n",
      "itr: 760 trn_acc 0.9530569340168776 trn_single_acc 0.995403099497137 val_acc 0.3356645074823824\n",
      "itr: 770 trn_loss 0.49490092122703716 val_loss 14.619669645372353\n",
      "itr: 770 trn_acc 0.9605994710617555 trn_single_acc 0.9961069141749415 val_acc 0.3444966972475643\n",
      "itr: 780 trn_loss 0.5321549937383401 val_loss 14.869239660510411\n",
      "itr: 780 trn_acc 0.9558222487731944 trn_single_acc 0.9958512556277195 val_acc 0.3510056317393835\n",
      "itr: 790 trn_loss 0.4778486882436139 val_loss 15.03196741259902\n",
      "itr: 790 trn_acc 0.9676515246825421 trn_single_acc 0.996745678127271 val_acc 0.3582718265543749\n",
      "itr: 800 trn_loss 0.444134268725867 val_loss 15.234827075452888\n",
      "itr: 800 trn_acc 0.9720320439516733 trn_single_acc 0.9972176281745898 val_acc 0.36409935586068365\n",
      "itr: 810 trn_loss 0.4389723767636327 val_loss 15.474692947314338\n",
      "itr: 810 trn_acc 0.9686557291194411 trn_single_acc 0.9969651991317745 val_acc 0.37145370154750346\n",
      "itr: 820 trn_loss 0.40310304245788353 val_loss 15.852232998591205\n",
      "itr: 820 trn_acc 0.9758178485952209 trn_single_acc 0.9975321729581765 val_acc 0.37742964544152025\n",
      "itr: 830 trn_loss 0.36974252682696757 val_loss 16.249205934388822\n",
      "itr: 830 trn_acc 0.9821034160086345 trn_single_acc 0.9980465897366751 val_acc 0.38192590580810437\n",
      "itr: 840 trn_loss 0.35732723489914153 val_loss 16.7225729233474\n",
      "itr: 840 trn_acc 0.9820192584454486 trn_single_acc 0.9980683762853143 val_acc 0.3876251265981436\n",
      "itr: 850 trn_loss 0.36715929543078335 val_loss 17.27899223623727\n",
      "itr: 850 trn_acc 0.9781136158660889 trn_single_acc 0.9978268749716819 val_acc 0.39108589758682016\n",
      "itr: 860 trn_loss 0.3669385241406369 val_loss 17.542850024027118\n",
      "itr: 860 trn_acc 0.9793596101458538 trn_single_acc 0.9980927313897617 val_acc 0.3964642626363733\n",
      "itr: 870 trn_loss 0.3391231711471391 val_loss 17.966558605303604\n",
      "itr: 870 trn_acc 0.9855560975387395 trn_single_acc 0.9986320936626016 val_acc 0.4005555488752155\n",
      "itr: 880 trn_loss 0.32081645708878775 val_loss 18.39955362367949\n",
      "itr: 880 trn_acc 0.9895259342040356 trn_single_acc 0.9989871441203236 val_acc 0.40559271974147815\n",
      "itr: 890 trn_loss 0.297319872209391 val_loss 18.835638910725606\n",
      "itr: 890 trn_acc 0.9934243217730745 trn_single_acc 0.9992887319010189 val_acc 0.4098498553286325\n",
      "itr: 900 trn_loss 0.29180147547981206 val_loss 19.60412999707004\n",
      "itr: 900 trn_acc 0.9933589791755999 trn_single_acc 0.9993128329508003 val_acc 0.41313396068665365\n",
      "itr: 910 trn_loss 0.36385279931608117 val_loss 19.764724725939697\n",
      "itr: 910 trn_acc 0.9803381476536812 trn_single_acc 0.9984299788818106 val_acc 0.41621718560683413\n",
      "itr: 920 trn_loss 0.33577886748730645 val_loss 19.86996631126809\n",
      "itr: 920 trn_acc 0.9849686222714935 trn_single_acc 0.998819044360874 val_acc 0.4202886499914022\n",
      "itr: 930 trn_loss 0.3102383761258402 val_loss 20.09560319454558\n",
      "itr: 930 trn_acc 0.9893199176031038 trn_single_acc 0.9991189912470072 val_acc 0.42375104528240787\n",
      "itr: 940 trn_loss 0.28302827347289267 val_loss 20.28581075076241\n",
      "itr: 940 trn_acc 0.9937952396443223 trn_single_acc 0.999465984942287 val_acc 0.4278980723164001\n",
      "itr: 950 trn_loss 0.2813584917305341 val_loss 20.66306005898695\n",
      "itr: 950 trn_acc 0.9935083393915548 trn_single_acc 0.9994969793722376 val_acc 0.43017973582221736\n",
      "itr: 960 trn_loss 0.30424063059393186 val_loss 20.767552545519894\n",
      "itr: 960 trn_acc 0.9886200280369966 trn_single_acc 0.9991651011875123 val_acc 0.43408242833859034\n",
      "itr: 970 trn_loss 0.2716543694877464 val_loss 21.14356332795521\n",
      "itr: 970 trn_acc 0.9943186039925176 trn_single_acc 0.9995680224080117 val_acc 0.43694657062512315\n",
      "itr: 980 trn_loss 0.26019651675033123 val_loss 21.473543314129415\n",
      "itr: 980 trn_acc 0.9955150183484749 trn_single_acc 0.9996386147783842 val_acc 0.4390991953762324\n",
      "itr: 990 trn_loss 0.25639422933297984 val_loss 21.698774470082196\n",
      "itr: 990 trn_acc 0.9957014974075094 trn_single_acc 0.9996345873918878 val_acc 0.4424128259198622\n",
      "itr: 1000 trn_loss 0.2551189643984701 val_loss 21.947162747256108\n",
      "itr: 1000 trn_acc 0.9955782924791643 trn_single_acc 0.9996676327338273 val_acc 0.4439072363057812\n",
      "itr: 1010 trn_loss 0.27357754896377384 val_loss 22.17295676458372\n",
      "itr: 1010 trn_acc 0.9916284498576826 trn_single_acc 0.999437811179273 val_acc 0.4453637944888246\n",
      "itr: 1020 trn_loss 0.30098693000879934 val_loss 22.20481289018834\n",
      "itr: 1020 trn_acc 0.9852364547910467 trn_single_acc 0.9989942427410465 val_acc 0.4458616894988274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 1030 trn_loss 0.27702852956137347 val_loss 22.215241139438547\n",
      "itr: 1030 trn_acc 0.9905962055579426 trn_single_acc 0.9993543909876081 val_acc 0.4492111166573812\n",
      "itr: 1040 trn_loss 0.25724483700971146 val_loss 22.441582702374575\n",
      "itr: 1040 trn_acc 0.9947631302292819 trn_single_acc 0.9996509213518112 val_acc 0.450891842865102\n",
      "itr: 1050 trn_loss 0.24607124729552263 val_loss 22.636242982246984\n",
      "itr: 1050 trn_acc 0.9966859094336059 trn_single_acc 0.9997717069264075 val_acc 0.4515914890973143\n",
      "itr: 1060 trn_loss 0.2549512275773132 val_loss 22.8668974010633\n",
      "itr: 1060 trn_acc 0.9942661551609125 trn_single_acc 0.9995691194790693 val_acc 0.4534486538596486\n",
      "itr: 1070 trn_loss 0.24056080013323886 val_loss 22.998868969245546\n",
      "itr: 1070 trn_acc 0.9962487170745742 trn_single_acc 0.9997261331081523 val_acc 0.4561084633851828\n",
      "itr: 1080 trn_loss 0.2256935349100349 val_loss 23.264576554742867\n",
      "itr: 1080 trn_acc 0.9984433760868051 trn_single_acc 0.9998882171922494 val_acc 0.4584332131551011\n",
      "itr: 1090 trn_loss 0.21411246827606037 val_loss 23.592382395667507\n",
      "itr: 1090 trn_acc 0.9994572388021248 trn_single_acc 0.9999610237449634 val_acc 0.46202397553977104\n",
      "itr: 1100 trn_loss 0.20481688001895063 val_loss 24.03661705649138\n",
      "itr: 1100 trn_acc 0.999792221802732 trn_single_acc 0.9999833247208934 val_acc 0.46502185352614583\n",
      "itr: 1110 trn_loss 0.19825897428578304 val_loss 24.52271689411617\n",
      "itr: 1110 trn_acc 0.9999275522222897 trn_single_acc 0.9999941856896928 val_acc 0.4676986886975562\n",
      "itr: 1120 trn_loss 0.19347914675839561 val_loss 25.097320700615196\n",
      "itr: 1120 trn_acc 0.9999747390218792 trn_single_acc 0.9999979726753518 val_acc 0.47024599795795047\n",
      "itr: 1130 trn_loss 0.1897380310994539 val_loss 25.692567191955042\n",
      "itr: 1130 trn_acc 0.9999911920415533 trn_single_acc 0.9999992931156041 val_acc 0.47260765807559996\n",
      "itr: 1140 trn_loss 0.1866074057301742 val_loss 26.29651031711989\n",
      "itr: 1140 trn_acc 0.9999969288547883 trn_single_acc 0.9999997535246514 val_acc 0.4743665001696131\n",
      "itr: 1150 trn_loss 0.18385317454037148 val_loss 26.907515947395208\n",
      "itr: 1150 trn_acc 0.9999989291578782 trn_single_acc 0.9999999140593598 val_acc 0.4759069480215712\n",
      "itr: 1160 trn_loss 0.1813507781181095 val_loss 27.52031621727971\n",
      "itr: 1160 trn_acc 0.9999996266204392 trn_single_acc 0.9999999700343515 val_acc 0.477341174875069\n",
      "itr: 1170 trn_loss 0.17903009981564 val_loss 28.122388828339826\n",
      "itr: 1170 trn_acc 0.9999998698105972 trn_single_acc 0.9999999895516244 val_acc 0.4785416452238277\n",
      "itr: 1180 trn_loss 0.17685085442731413 val_loss 28.712651623972643\n",
      "itr: 1180 trn_acc 0.9999999546057621 trn_single_acc 0.9999999963568766 val_acc 0.4796220685377105\n",
      "itr: 1190 trn_loss 0.17478820956689983 val_loss 29.28942685159003\n",
      "itr: 1190 trn_acc 0.999999984172008 trn_single_acc 0.9999999987297215 val_acc 0.48050411570081564\n",
      "itr: 1200 trn_loss 0.17282533811771067 val_loss 29.850329442309935\n",
      "itr: 1200 trn_acc 0.9999999944811204 trn_single_acc 0.9999999995570813 val_acc 0.4812288793445479\n",
      "itr: 1210 trn_loss 0.1709500096221255 val_loss 30.394194737214683\n",
      "itr: 1210 trn_acc 0.9999999980756856 trn_single_acc 0.9999999998455638 val_acc 0.48185459785349827\n",
      "itr: 1220 trn_loss 0.16915305368735933 val_loss 30.919317652409234\n",
      "itr: 1220 trn_acc 0.999999999329033 trn_single_acc 0.9999999999461514 val_acc 0.4823699177445858\n",
      "itr: 1230 trn_loss 0.16742706328073503 val_loss 31.425324806235693\n",
      "itr: 1230 trn_acc 0.9999999997660483 trn_single_acc 0.9999999999812241 val_acc 0.48281245063023775\n",
      "itr: 1240 trn_loss 0.165765967769051 val_loss 31.911853070851382\n",
      "itr: 1240 trn_acc 0.999999999918426 trn_single_acc 0.9999999999934533 val_acc 0.4831310239160986\n",
      "itr: 1250 trn_loss 0.1641648108496011 val_loss 32.380452843722296\n",
      "itr: 1250 trn_acc 0.9999999999715569 trn_single_acc 0.9999999999977173 val_acc 0.48338585734888295\n",
      "itr: 1260 trn_loss 0.16261932313127397 val_loss 32.831075238183075\n",
      "itr: 1260 trn_acc 0.9999999999900824 trn_single_acc 0.999999999999204 val_acc 0.4835620698975716\n",
      "itr: 1270 trn_loss 0.16112574349235245 val_loss 33.26558436780715\n",
      "itr: 1270 trn_acc 0.999999999996542 trn_single_acc 0.9999999999997224 val_acc 0.4836781511587376\n",
      "itr: 1280 trn_loss 0.15968079893021608 val_loss 33.6843235537071\n",
      "itr: 1280 trn_acc 0.9999999999987943 trn_single_acc 0.9999999999999033 val_acc 0.48374011426113317\n",
      "itr: 1290 trn_loss 0.15828142018948943 val_loss 34.08721955221823\n",
      "itr: 1290 trn_acc 0.9999999999995796 trn_single_acc 0.9999999999999662 val_acc 0.48377993979104406\n",
      "itr: 1300 trn_loss 0.15692487170892055 val_loss 34.47481259943781\n",
      "itr: 1300 trn_acc 0.9999999999998535 trn_single_acc 0.9999999999999882 val_acc 0.4838051552598004\n",
      "itr: 1310 trn_loss 0.15560873482664647 val_loss 34.847167383805555\n",
      "itr: 1310 trn_acc 0.9999999999999489 trn_single_acc 0.9999999999999959 val_acc 0.4837959666571907\n",
      "itr: 1320 trn_loss 0.1543307010156087 val_loss 35.20575585172383\n",
      "itr: 1320 trn_acc 0.9999999999999821 trn_single_acc 0.9999999999999986 val_acc 0.4837133043576979\n",
      "itr: 1330 trn_loss 0.15308875441512826 val_loss 35.55113600080438\n",
      "itr: 1330 trn_acc 0.9999999999999938 trn_single_acc 0.9999999999999994 val_acc 0.4836123395177456\n",
      "itr: 1340 trn_loss 0.15188099069381125 val_loss 35.88484915243293\n",
      "itr: 1340 trn_acc 0.9999999999999978 trn_single_acc 0.9999999999999994 val_acc 0.48340456857199066\n",
      "itr: 1350 trn_loss 0.15070568510011073 val_loss 36.20671846997772\n",
      "itr: 1350 trn_acc 0.9999999999999992 trn_single_acc 0.9999999999999994 val_acc 0.48320694721264773\n",
      "itr: 1360 trn_loss 0.14956126645452966 val_loss 36.51778490422995\n",
      "itr: 1360 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.48297063669434015\n",
      "itr: 1370 trn_loss 0.14844629112165292 val_loss 36.8181109822884\n",
      "itr: 1370 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.48268356169048693\n",
      "itr: 1380 trn_loss 0.14735939648163102 val_loss 37.10828970644726\n",
      "itr: 1380 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4823401741217114\n",
      "itr: 1390 trn_loss 0.14629928707636594 val_loss 37.39032877779472\n",
      "itr: 1390 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4820364390638951\n",
      "itr: 1400 trn_loss 0.1452647738307673 val_loss 37.66412663426818\n",
      "itr: 1400 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.48178433252818736\n",
      "itr: 1410 trn_loss 0.14425476677106178 val_loss 37.92998249501128\n",
      "itr: 1410 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4815414953838052\n",
      "itr: 1420 trn_loss 0.1432682451363007 val_loss 38.18803061697012\n",
      "itr: 1420 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4812857456752892\n",
      "itr: 1430 trn_loss 0.14230424808212508 val_loss 38.438692048070955\n",
      "itr: 1430 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.48102900216721617\n",
      "itr: 1440 trn_loss 0.14136186327079084 val_loss 38.683386762331246\n",
      "itr: 1440 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.48076073673137837\n",
      "itr: 1450 trn_loss 0.14044022270490172 val_loss 38.922551977089334\n",
      "itr: 1450 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4804927290687157\n",
      "itr: 1460 trn_loss 0.13953852348824602 val_loss 39.15670325826223\n",
      "itr: 1460 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.48019838463150205\n",
      "itr: 1470 trn_loss 0.13865601648948853 val_loss 39.38477721039011\n",
      "itr: 1470 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.47991753337576454\n",
      "itr: 1480 trn_loss 0.13779198893086084 val_loss 39.60848096212943\n",
      "itr: 1480 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4795850609343749\n",
      "itr: 1490 trn_loss 0.13694572885060907 val_loss 39.82723224640965\n",
      "itr: 1490 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4792486394585522\n",
      "itr: 1500 trn_loss 0.13611659583251262 val_loss 40.04239796830189\n",
      "itr: 1500 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4789618013925569\n",
      "itr: 1510 trn_loss 0.1353039621178358 val_loss 40.2529473713008\n",
      "itr: 1510 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.47867707836275253\n",
      "itr: 1520 trn_loss 0.1345072549475089 val_loss 40.4596995244295\n",
      "itr: 1520 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.478431455144092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 1530 trn_loss 0.13372588743599143 val_loss 40.66171045272385\n",
      "itr: 1530 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4781519429523986\n",
      "itr: 1540 trn_loss 0.13295931582041418 val_loss 40.860177545268854\n",
      "itr: 1540 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.47792163699620144\n",
      "itr: 1550 trn_loss 0.13220704196601094 val_loss 41.055577637421656\n",
      "itr: 1550 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4776931066192971\n",
      "itr: 1560 trn_loss 0.13146859422998725 val_loss 41.248014487326955\n",
      "itr: 1560 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4774768017719197\n",
      "itr: 1570 trn_loss 0.13074351476622428 val_loss 41.43798049464895\n",
      "itr: 1570 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.47720242109805416\n",
      "itr: 1580 trn_loss 0.1300314087730324 val_loss 41.62531876720554\n",
      "itr: 1580 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.47696610599973865\n",
      "itr: 1590 trn_loss 0.12933185228885807 val_loss 41.81024058006019\n",
      "itr: 1590 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4766896573622739\n",
      "itr: 1600 trn_loss 0.1286444272730812 val_loss 41.99319815047214\n",
      "itr: 1600 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4764408535885557\n",
      "itr: 1610 trn_loss 0.1279687560835846 val_loss 42.17432343430188\n",
      "itr: 1610 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4762116134578953\n",
      "itr: 1620 trn_loss 0.127304476697847 val_loss 42.35283759355724\n",
      "itr: 1620 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4759627873076472\n",
      "itr: 1630 trn_loss 0.12665121807397203 val_loss 42.52910740780992\n",
      "itr: 1630 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.47571227500201524\n",
      "itr: 1640 trn_loss 0.12600864456559735 val_loss 42.70270080216076\n",
      "itr: 1640 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.47544430389429265\n",
      "itr: 1650 trn_loss 0.12537646127083288 val_loss 42.87488982839\n",
      "itr: 1650 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4751871886350972\n",
      "itr: 1660 trn_loss 0.12475440004333589 val_loss 43.04518092062425\n",
      "itr: 1660 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.47498235367222985\n",
      "itr: 1670 trn_loss 0.12414218604668265 val_loss 43.21494070636944\n",
      "itr: 1670 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4747289234025868\n",
      "itr: 1680 trn_loss 0.1235395234079187 val_loss 43.383642482290114\n",
      "itr: 1680 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.47446895363541775\n",
      "itr: 1690 trn_loss 0.12294613004790175 val_loss 43.552375478323796\n",
      "itr: 1690 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4741924708123118\n",
      "itr: 1700 trn_loss 0.12236175891829189 val_loss 43.719602301218956\n",
      "itr: 1700 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.47391175374702604\n",
      "itr: 1710 trn_loss 0.1217861539076859 val_loss 43.88558991655605\n",
      "itr: 1710 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.47365910838826886\n",
      "itr: 1720 trn_loss 0.1212190553149858 val_loss 44.05023183737603\n",
      "itr: 1720 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4734423550735509\n",
      "itr: 1730 trn_loss 0.1206602234580055 val_loss 44.21411238593823\n",
      "itr: 1730 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.47321008081173266\n",
      "itr: 1740 trn_loss 0.12010941375944191 val_loss 44.3768777830866\n",
      "itr: 1740 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4730010339760962\n",
      "itr: 1750 trn_loss 0.11956641644883141 val_loss 44.5380578026295\n",
      "itr: 1750 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4727225580046341\n",
      "itr: 1760 trn_loss 0.11903104344645138 val_loss 44.69838585720542\n",
      "itr: 1760 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4724666158762364\n",
      "itr: 1770 trn_loss 0.11850310528535274 val_loss 44.85805929541067\n",
      "itr: 1770 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4722150129443516\n",
      "itr: 1780 trn_loss 0.11798240752617688 val_loss 45.015859933252415\n",
      "itr: 1780 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4719248052566746\n",
      "itr: 1790 trn_loss 0.1174687568605625 val_loss 45.173364363511155\n",
      "itr: 1790 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.47166361833776527\n",
      "itr: 1800 trn_loss 0.11696196142217027 val_loss 45.330089893102425\n",
      "itr: 1800 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4714072950944199\n",
      "itr: 1810 trn_loss 0.11646185440005785 val_loss 45.48613577439765\n",
      "itr: 1810 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4711181528805102\n",
      "itr: 1820 trn_loss 0.11596823969753497 val_loss 45.641928172299686\n",
      "itr: 1820 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4708419836257462\n",
      "itr: 1830 trn_loss 0.11548097858119795 val_loss 45.796423938443745\n",
      "itr: 1830 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.47059343129645864\n",
      "itr: 1840 trn_loss 0.1149999121277247 val_loss 45.95047676554664\n",
      "itr: 1840 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4703431654296912\n",
      "itr: 1850 trn_loss 0.11452487545911322 val_loss 46.10358201257596\n",
      "itr: 1850 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.470107298641437\n",
      "itr: 1860 trn_loss 0.11405575377219679 val_loss 46.255782557656254\n",
      "itr: 1860 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.469857819273204\n",
      "itr: 1870 trn_loss 0.11359240282337549 val_loss 46.40885916334082\n",
      "itr: 1870 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4696226603336308\n",
      "itr: 1880 trn_loss 0.11313467836320754 val_loss 46.561397853329986\n",
      "itr: 1880 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.46936850725536117\n",
      "itr: 1890 trn_loss 0.11268243613409056 val_loss 46.712923320682535\n",
      "itr: 1890 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4691132007145098\n",
      "itr: 1900 trn_loss 0.11223552281538463 val_loss 46.86436849166604\n",
      "itr: 1900 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4688674835654984\n",
      "itr: 1910 trn_loss 0.11179384761763171 val_loss 47.01539120670842\n",
      "itr: 1910 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.46861976936097954\n",
      "itr: 1920 trn_loss 0.11135729302517164 val_loss 47.166372456520975\n",
      "itr: 1920 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.46841808159323944\n",
      "itr: 1930 trn_loss 0.11092575397649164 val_loss 47.31684298369603\n",
      "itr: 1930 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4682471901104368\n",
      "itr: 1940 trn_loss 0.11049910349175612 val_loss 47.46671309816823\n",
      "itr: 1940 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4680933877759144\n",
      "itr: 1950 trn_loss 0.11007722171022435 val_loss 47.615850961935394\n",
      "itr: 1950 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.46792839690443566\n",
      "itr: 1960 trn_loss 0.10965997506374996 val_loss 47.76532925075163\n",
      "itr: 1960 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.46777459136602306\n",
      "itr: 1970 trn_loss 0.10924726290638018 val_loss 47.914993759148146\n",
      "itr: 1970 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4676149113651248\n",
      "itr: 1980 trn_loss 0.10883899534500616 val_loss 48.0642380203183\n",
      "itr: 1980 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4674605718561529\n",
      "itr: 1990 trn_loss 0.10843507234410596 val_loss 48.21432361770053\n",
      "itr: 1990 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4673004112817513\n",
      "itr: 2000 trn_loss 0.10803541627059153 val_loss 48.36434587629181\n",
      "itr: 2000 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4671190704862178\n",
      "itr: 2010 trn_loss 0.10763992782076451 val_loss 48.514001758022985\n",
      "itr: 2010 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.46692929499982905\n",
      "itr: 2020 trn_loss 0.10724852761937245 val_loss 48.66392108600487\n",
      "itr: 2020 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4667053595212619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 2030 trn_loss 0.10686111136806807 val_loss 48.81341642857626\n",
      "itr: 2030 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.46646662131197936\n",
      "itr: 2040 trn_loss 0.10647761345479434 val_loss 48.96197056818934\n",
      "itr: 2040 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4662889532021971\n",
      "itr: 2050 trn_loss 0.10609793067445156 val_loss 49.110123944720016\n",
      "itr: 2050 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.46612905190339315\n",
      "itr: 2060 trn_loss 0.1057219862562618 val_loss 49.25869902888571\n",
      "itr: 2060 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.46597982698038787\n",
      "itr: 2070 trn_loss 0.1053496945558923 val_loss 49.406572183248116\n",
      "itr: 2070 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.46588272082825516\n",
      "itr: 2080 trn_loss 0.10498097646278307 val_loss 49.5540466787905\n",
      "itr: 2080 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.46577407027500883\n",
      "itr: 2090 trn_loss 0.10461574442853638 val_loss 49.70066380046223\n",
      "itr: 2090 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.465660343514842\n",
      "itr: 2100 trn_loss 0.1042539344248965 val_loss 49.84700405188574\n",
      "itr: 2100 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.46556330318477346\n",
      "itr: 2110 trn_loss 0.10389548692276175 val_loss 49.993831356658106\n",
      "itr: 2110 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4654228293468946\n",
      "itr: 2120 trn_loss 0.10354033194890222 val_loss 50.140271127486436\n",
      "itr: 2120 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4652485791060681\n",
      "itr: 2130 trn_loss 0.10318842129710097 val_loss 50.286537593839356\n",
      "itr: 2130 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.46509706764340597\n",
      "itr: 2140 trn_loss 0.10283969175918972 val_loss 50.43255004844468\n",
      "itr: 2140 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.4649553935729283\n",
      "itr: 2150 trn_loss 0.10249407864726487 val_loss 50.578336181295526\n",
      "itr: 2150 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.464796004385008\n",
      "itr: 2160 trn_loss 0.10215150826872244 val_loss 50.72400178496773\n",
      "itr: 2160 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.46462598534547117\n",
      "itr: 2170 trn_loss 0.10181192355020888 val_loss 50.87073917971315\n",
      "itr: 2170 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.46447296820988804\n",
      "itr: 2180 trn_loss 0.10147527031640756 val_loss 51.01643923329945\n",
      "itr: 2180 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.46431399777153626\n",
      "itr: 2190 trn_loss 0.1011414935863247 val_loss 51.16189041800173\n",
      "itr: 2190 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.46419217939334656\n",
      "itr: 2200 trn_loss 0.10081053517977655 val_loss 51.307410970928125\n",
      "itr: 2200 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.46407722909889415\n",
      "itr: 2210 trn_loss 0.10048233693782968 val_loss 51.45289362871813\n",
      "itr: 2210 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.46395251881756006\n",
      "itr: 2220 trn_loss 0.10015686544178731 val_loss 51.598619127906865\n",
      "itr: 2220 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.46377120076129696\n",
      "itr: 2230 trn_loss 0.09983407713190745 val_loss 51.74412983474508\n",
      "itr: 2230 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.46357613198616976\n"
     ]
    }
   ],
   "source": [
    "eval_itr = -1\n",
    "bs = 2091\n",
    "for itr in range(100000):\n",
    "    if itr == 0:\n",
    "        samples = np.random.choice(trn_samples, size = bs, replace = False)\n",
    "        trn_feed_dict = {\n",
    "            cmd_ind : cmd_np[samples],\n",
    "            act_ind : act_np[samples],\n",
    "            mask_ph : mask_np[samples],\n",
    "            act_lengths : np.clip(struct_np[samples], a_min = 1, a_max = None),\n",
    "            cmd_lengths : cmd_lengths_np[samples],\n",
    "        }\n",
    "        \n",
    "    trn_feed_dict[learning_rate] = .02 / (np.power(itr + 10, .6))\n",
    "    _, trn_loss, acc_trn_single, acc_trn = sess.run(\n",
    "        [optimizer, loss, percent_correct, percent_fully_correct], trn_feed_dict)\n",
    "    if itr == 0:\n",
    "        trn_loss_avg = trn_loss\n",
    "        acc_trn_avg = acc_trn\n",
    "        acc_trn_single_avg = acc_trn_single\n",
    "    else:\n",
    "        trn_loss_avg = trn_loss_avg * .9 + trn_loss * .1\n",
    "        acc_trn_avg = acc_trn_avg * .9 + acc_trn * .1\n",
    "        acc_trn_single_avg = acc_trn_single_avg * .9 + acc_trn_single * .1\n",
    "    if itr % 10 == 0 and itr > 0:\n",
    "        # val_samples = np.random.choice(val_samples_all, size = bs, replace = False)\n",
    "        eval_itr += 1\n",
    "        val_feed_dict = {\n",
    "            cmd_ind : cmd_np[val_samples],\n",
    "            act_ind : act_np[val_samples],\n",
    "            mask_ph : mask_np[val_samples],\n",
    "            act_lengths : np.clip(struct_np[val_samples], a_min = 1, a_max = None),\n",
    "            cmd_lengths : cmd_lengths_np[val_samples]\n",
    "        }\n",
    "        val_loss, acc_val = sess.run([loss, percent_fully_correct], val_feed_dict)\n",
    "        if eval_itr == 0:\n",
    "            val_loss_avg = val_loss\n",
    "            acc_val_avg = acc_val\n",
    "        else:\n",
    "            val_loss_avg = val_loss_avg * .9 + val_loss * .1\n",
    "            acc_val_avg = acc_val_avg * .9 + acc_val * .1\n",
    "        print('itr:', itr, 'trn_loss', trn_loss_avg, 'val_loss', val_loss_avg)\n",
    "        print('itr:', itr, 'trn_acc', acc_trn_avg, \n",
    "              'trn_single_acc', acc_trn_single_avg, 'val_acc', acc_val_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_percent.shape, percent_fully_correct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "10, None, 7, 9, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmd_np.shape, act_np.shape, mask_np.shape, struct_np.shape, cmd_lengths_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cmd_ind.shape, act_ind.shape, mask_ph.shape, act_lengths.shape, cmd_lengths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_samples.shape, val_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "act_presoftmax = tf.stack(action_probabilities_presoftmax, 1)[:, :, :-1, :]\n",
    "#batch, subprogram, timestep, action_selection\n",
    "logprobabilities = tf.nn.log_softmax(act_presoftmax, -1)\n",
    "act_presoftmax_flat = tf.reshape(act_presoftmax, [-1, 9, num_act])\n",
    "mask_ph_flat = tf.reshape(mask_ph, [-1, max_actions_per_subprogram])\n",
    "act_ind_flat = tf.reshape(act_ind, [-1, max_actions_per_subprogram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "act_presoftmax_flat = tf.reshape(act_presoftmax, [-1, 9, num_act])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_actions_per_subprogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess.run(act_presoftmax, feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(*actions_ind[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "command_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subprogram_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subprogram_last_layer[:,cmd_lengths,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoding_last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.gather(\n",
    "    encoding_last_layer,\n",
    "    [1,2],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.gather_nd(\n",
    "    encoding_last_layer,\n",
    "    np.array([[0,1,2,3,4], [1,4,3,2,5]]).T,\n",
    "    name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmd_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_command(sub_cmd, num_repeat):\n",
    "    return sub_cmd * num_repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_command(cmd):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uni_commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uni_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uni_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
