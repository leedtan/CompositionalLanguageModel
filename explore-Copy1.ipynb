{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lee/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import common dependencies\n",
    "import pandas as pd  # noqa\n",
    "import numpy as np\n",
    "import matplotlib  # noqa\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime  # noqa\n",
    "import PIL  # noqa\n",
    "import glob  # noqa\n",
    "import pickle  # noqa\n",
    "from pathlib import Path  # noqa\n",
    "from scipy import misc  # noqa\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import pdb\n",
    "TRADE_COST_FRAC = .003\n",
    "EPSILON = 1e-10\n",
    "ADV_MULT = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uni_tokens = set()\n",
    "uni_commands = set()\n",
    "uni_actions = set()\n",
    "fname = 'tasks_with_length_tags.txt'\n",
    "with open(fname) as f:\n",
    "    content = f.readlines()\n",
    "content2 = [c.split(' ') for c in content]\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "commands = []\n",
    "actions = []\n",
    "content = [l.replace('\\n', '') for l in content]\n",
    "commands = [x.split(':::')[1].split(' ')[1:-1] for x in content]\n",
    "actions = [x.split(':::')[2].split(' ')[1:-2] for x in content]\n",
    "structures = [x.split(':::')[3].split(' ')[2:] for x in content]\n",
    "\n",
    "structures = [[int(l) for l in program] for program in structures]\n",
    "#actions = [[wd.replace('\\n', '') for wd in res] for res in actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 10, 49, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_actions_per_subprogram = max([max([s for s in struct]) for struct in structures]) + 1\n",
    "max_num_subprograms = max([len(s) for s in structures]) + 1\n",
    "max_cmd_len = max([len(s) for s in commands]) + 1\n",
    "max_act_len = max([len(a) for a in actions]) + 1\n",
    "cmd_lengths_list = [len(s)+1 for s in commands]\n",
    "cmd_lengths_np = np.array(cmd_lengths_list)\n",
    "max_num_subprograms, max_cmd_len, max_act_len, max_actions_per_subprogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_fmap_invmap(unique, num_unique):\n",
    "    fmap = dict(zip(unique, range(num_unique)))\n",
    "    invmap = dict(zip(range(num_unique), unique))\n",
    "    return fmap, invmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for li in content2:\n",
    "    for wd in li:\n",
    "        uni_tokens.add(wd)\n",
    "for li in commands:\n",
    "    for wd in li:\n",
    "        uni_commands.add(wd)\n",
    "for li in actions:\n",
    "    for wd in li:\n",
    "        uni_actions.add(wd)\n",
    "uni_commands.add('end_command')\n",
    "uni_actions.add('end_subprogram')\n",
    "uni_actions.add('end_action')\n",
    "num_cmd = len(uni_commands)\n",
    "num_act = len(uni_actions)\n",
    "command_map, command_invmap = build_fmap_invmap(uni_commands, num_cmd)\n",
    "action_map, action_invmap = build_fmap_invmap(uni_actions, num_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dense_scaled(prev_layer, layer_size, name=None, reuse=False, scale=1.0):\n",
    "    output = tf.layers.dense(prev_layer, layer_size, reuse=reuse) * scale\n",
    "    return output\n",
    "\n",
    "\n",
    "def dense_relu(dense_input, layer_size, scale=1.0):\n",
    "    dense = dense_scaled(dense_input, layer_size, scale=scale)\n",
    "    output = tf.nn.leaky_relu(dense)\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_grad_norm(opt_fcn, loss):\n",
    "    gvs = opt_fcn.compute_gradients(loss)\n",
    "    grad_norm = tf.sqrt(tf.reduce_sum(\n",
    "        [tf.reduce_sum(tf.square(grad)) for grad, var in gvs if grad is not None]))\n",
    "    return grad_norm\n",
    "\n",
    "\n",
    "def apply_clipped_optimizer(opt_fcn, loss, clip_norm=.1, clip_single=.03, clip_global_norm=False):\n",
    "    gvs = opt_fcn.compute_gradients(loss)\n",
    "\n",
    "    if clip_global_norm:\n",
    "        gs, vs = zip(*[(g, v) for g, v in gvs if g is not None])\n",
    "        capped_gs, grad_norm_total = tf.clip_by_global_norm([g for g in gs], clip_norm)\n",
    "        capped_gvs = list(zip(capped_gs, vs))\n",
    "    else:\n",
    "        grad_norm_total = tf.sqrt(\n",
    "            tf.reduce_sum([tf.reduce_sum(tf.square(grad)) for grad, var in gvs if grad is not None]))\n",
    "        capped_gvs = [(tf.clip_by_value(grad, -1 * clip_single, clip_single), var)\n",
    "                      for grad, var in gvs if grad is not None]\n",
    "        capped_gvs = [(tf.clip_by_norm(grad, clip_norm), var) for grad, var in capped_gvs if grad is not None]\n",
    "\n",
    "    optimizer = opt_fcn.apply_gradients(capped_gvs)\n",
    "\n",
    "    return optimizer, grad_norm_total\n",
    "\n",
    "\n",
    "def mlp(x, hidden_sizes, output_size=None, name='', reuse=False):\n",
    "    prev_layer = x\n",
    "\n",
    "    for idx, l in enumerate(hidden_sizes):\n",
    "        dense = dense_scaled(prev_layer, l, name='mlp' + name + '_' + str(idx))\n",
    "        prev_layer = tf.nn.leaky_relu(dense)\n",
    "\n",
    "    output = prev_layer\n",
    "\n",
    "    if output_size is not None:\n",
    "        output = dense_scaled(prev_layer, output_size, name='mlp' + name + 'final')\n",
    "\n",
    "    return output\n",
    "\n",
    "def mlp_with_adversaries(x, hidden_sizes, output_size=None, name='', reuse=False):\n",
    "    prev_layer = x\n",
    "    adv_phs = []\n",
    "    for idx, l in enumerate(hidden_sizes):\n",
    "        \n",
    "        adversary = tf.placeholder_with_default(tf.zeros_like(prev_layer), prev_layer.shape)\n",
    "        prev_layer = prev_layer + adversary\n",
    "        adv_phs.append(adversary)\n",
    "        \n",
    "        dense = dense_scaled(prev_layer, l, name='mlp' + name + '_' + str(idx))\n",
    "        prev_layer = tf.nn.leaky_relu(dense)\n",
    "\n",
    "    output = prev_layer\n",
    "\n",
    "    if output_size is not None:\n",
    "        output = dense_scaled(prev_layer, output_size, name='mlp' + name + 'final')\n",
    "\n",
    "    return output, adv_phs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "commands_ind = [[command_map[c] for c in cmd] + [0] * (max_cmd_len - len(cmd)) for cmd in commands]\n",
    "actions_ind = [[action_map[a] for a in act] + [0] * (max_act_len - len(act)) for act in actions]\n",
    "cmd_np = np.array(commands_ind)\n",
    "actions_structured = []\n",
    "mask_structured = []\n",
    "for row in range(len(structures)):\n",
    "    mask_row = []\n",
    "    action_row = []\n",
    "    act = actions_ind[row]\n",
    "    struct = structures[row]\n",
    "    start = 0\n",
    "    for step in struct:\n",
    "        end = start + step\n",
    "        a = act[start:end]\n",
    "        padding = max_actions_per_subprogram - step - 1\n",
    "        action_row.append(a + [action_map['end_action']] + [0] * padding)\n",
    "        start = end\n",
    "    actions_structured.append(\n",
    "        action_row + [[action_map['end_subprogram']] + [0] * (max_actions_per_subprogram - 1)] +\n",
    "        [[0] * max_actions_per_subprogram] * (max_num_subprograms - len(struct) - 1)\n",
    "    )\n",
    "act_np = np.array(actions_structured)\n",
    "struct_padded = [[sa + 1 for sa in s] + [1] + [0] * (max_num_subprograms - len(s) - 1) for s in structures]\n",
    "struct_np = np.array(struct_padded)\n",
    "\n",
    "mask_list = [[np.concatenate((np.ones(st), np.zeros(max_actions_per_subprogram - st)), 0) \n",
    "              for st in s] for s in struct_np]\n",
    "mask_np = np.array(mask_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "default_sizes = 128\n",
    "size_emb = 64\n",
    "num_layers_encoder = 6\n",
    "hidden_filters = 128\n",
    "num_layers_subprogram = 3\n",
    "hidden_filters_subprogram = default_sizes\n",
    "cmd_mat = tf.Variable(1e-5*tf.random_normal([num_cmd, size_emb]))\n",
    "act_mat = tf.Variable(1e-5*tf.random_normal([num_act, size_emb]))\n",
    "global_bs = None\n",
    "global_time_len = 7\n",
    "action_lengths = None\n",
    "max_num_actions= None\n",
    "# global_bs = 8\n",
    "global_time_len = 7\n",
    "max_num_actions = 9\n",
    "cmd_ind = tf.placeholder(tf.int32, shape=(global_bs, 10,))\n",
    "act_ind = tf.placeholder(tf.int32, shape=(global_bs, global_time_len, 9))\n",
    "mask_ph = tf.placeholder(tf.float32, shape=(global_bs, global_time_len, 9))\n",
    "cmd_lengths = tf.placeholder(tf.int32, shape=(global_bs,))\n",
    "act_lengths = tf.placeholder(tf.int32, shape=(global_bs, 7))\n",
    "learning_rate = tf.placeholder(tf.float32, shape = (None))\n",
    "\n",
    "cmd_emb = tf.nn.embedding_lookup(cmd_mat, cmd_ind)\n",
    "act_emb = tf.nn.embedding_lookup(act_mat, act_ind)\n",
    "act_st_emb = tf.Variable(1e-5*tf.random_normal([size_emb]))\n",
    "tf_bs = tf.shape(act_ind)[0]\n",
    "act_st_emb_expanded = tf.tile(tf.reshape(\n",
    "    act_st_emb, [1, 1, 1, size_emb]), [tf_bs, global_time_len, 1, 1])\n",
    "act_emb_with_st = tf.concat((act_st_emb_expanded, act_emb), 2)\n",
    "\n",
    "first_cell_encoder = [tf.nn.rnn_cell.LSTMCell(\n",
    "    hidden_filters, forget_bias=1., name = 'layer1_'+d) for d in ['f', 'b']]\n",
    "hidden_cells_encoder = [[tf.nn.rnn_cell.LSTMCell(\n",
    "    hidden_filters,forget_bias=1., name = 'layer' + str(lidx) + '_' + d)  for d in ['f', 'b']]\n",
    "                        for lidx in range(num_layers_encoder - 1)]\n",
    "cells_encoder = [first_cell_encoder] + hidden_cells_encoder\n",
    "c1, c2 = zip(*cells_encoder)\n",
    "cells_encoder = [c1, c2]\n",
    "def encode(x, num_layers, cells, initial_states, lengths, name='',):\n",
    "    prev_layer = x\n",
    "    shortcut = x\n",
    "    hiddenlayers = []\n",
    "    returncells = []\n",
    "    cell_fw, cell_bw = cells\n",
    "    bs = tf.shape(x)[0]\n",
    "    for idx in range(num_layers):\n",
    "        prev_layer, c = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw = cell_fw[idx],\n",
    "                cell_bw = cell_bw[idx],\n",
    "                inputs = prev_layer,\n",
    "                sequence_length=lengths,\n",
    "                initial_state_fw=None,\n",
    "                initial_state_bw=None,\n",
    "                dtype=tf.float32,\n",
    "                scope='encoder'+str(idx)\n",
    "            )\n",
    "        prev_layer = tf.concat(prev_layer, 2)\n",
    "        prev_layer = tf.nn.leaky_relu(prev_layer)\n",
    "        returncells.append(c)\n",
    "        hiddenlayers.append(prev_layer)\n",
    "        if idx == num_layers - 1:\n",
    "            #pdb.set_trace()\n",
    "            output = tf.gather_nd(\n",
    "                        prev_layer,\n",
    "                        tf.stack([tf.range(bs), lengths], 1),\n",
    "                        name=None\n",
    "                    )\n",
    "            return prev_layer, returncells, hiddenlayers, output\n",
    "        prev_layer = tf.concat((prev_layer, shortcut), 2)\n",
    "encoding_last_layer, encoding_final_cells, encoding_hidden_layers, encoding_last_timestep = encode(\n",
    "    cmd_emb, num_layers_encoder, cells_encoder,None, lengths = cmd_lengths, name = 'encoder')\n",
    "# encoding_last_timestep = encoding_last_layer[:,cmd_lengths, :]\n",
    "hidden_filters_encoder = encoding_last_timestep.shape[-1].value\n",
    "first_cell_subprogram = tf.nn.rnn_cell.LSTMCell(\n",
    "    hidden_filters_subprogram, forget_bias=1., name = 'subpogramlayer1_')\n",
    "hidden_cells_subprogram = [tf.nn.rnn_cell.LSTMCell(\n",
    "    hidden_filters_subprogram,forget_bias=1., name = 'subpogramlayer' + str(lidx))\n",
    "                        for lidx in range(num_layers_subprogram - 1)]\n",
    "\n",
    "cells_subprogram_rnn = [first_cell_subprogram] + hidden_cells_subprogram\n",
    "\n",
    "attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "    num_units=hidden_filters_encoder, memory=encoding_last_layer,\n",
    "    memory_sequence_length=cmd_lengths)\n",
    "attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "    num_units=hidden_filters_encoder//2, memory=encoding_last_layer,\n",
    "    memory_sequence_length=cmd_lengths)\n",
    "cells_subprogram = [\n",
    "    tf.contrib.seq2seq.AttentionWrapper(\n",
    "        cell, attention_mechanism, attention_layer_size = hidden_filters_subprogram) \n",
    "    for cell in cells_subprogram_rnn]\n",
    "\n",
    "def subprogram(x, num_layers, cells, initial_states, lengths, reuse, name='',):\n",
    "    prev_layer = x\n",
    "    shortcut = x\n",
    "    hiddenlayers = []\n",
    "    returncells = []\n",
    "    bs = tf.shape(x)[0]\n",
    "    for idx in range(num_layers):\n",
    "        print(idx)\n",
    "        if idx == 0:\n",
    "            num_past_units = hidden_filters\n",
    "        else:\n",
    "            num_past_units = hidden_filters_subprogram\n",
    "        with tf.variable_scope(name + 'subprogram' + str(idx), reuse=reuse):\n",
    "            self_attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "                num_units=num_past_units, memory=prev_layer,\n",
    "                memory_sequence_length=lengths)\n",
    "            cell_with_selfattention = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                    cells[idx], self_attention_mechanism, attention_layer_size = num_past_units)\n",
    "\n",
    "            prev_layer, c = tf.nn.dynamic_rnn(\n",
    "                    cell = cell_with_selfattention,\n",
    "                    inputs = prev_layer,\n",
    "                    sequence_length=lengths,\n",
    "                    initial_state = None,\n",
    "                    dtype=tf.float32,\n",
    "                )\n",
    "            prev_layer = tf.concat(prev_layer, 2)\n",
    "            prev_layer = tf.nn.leaky_relu(prev_layer)\n",
    "            returncells.append(c)\n",
    "            hiddenlayers.append(prev_layer)\n",
    "            if idx == num_layers - 1:\n",
    "                output = tf.gather_nd(\n",
    "                            prev_layer,\n",
    "                            tf.stack([tf.range(bs), lengths], 1),\n",
    "                            name=None\n",
    "                        )\n",
    "                return prev_layer, returncells, hiddenlayers, output\n",
    "            prev_layer = tf.concat((prev_layer, shortcut), 2)\n",
    "encodings = [encoding_last_timestep]\n",
    "last_encoding = encoding_last_timestep\n",
    "initial_cmb_encoding = last_encoding\n",
    "loss = 0\n",
    "action_probabilities_presoftmax = []\n",
    "for sub_idx in range(max_num_subprograms): \n",
    "    from_last_layer = tf.tile(tf.expand_dims(tf.concat((\n",
    "        last_encoding, initial_cmb_encoding), 1), 1), [1, max_num_actions + 1, 1])\n",
    "    autoregressive = act_emb_with_st[:,sub_idx, :, :]\n",
    "    x_input = tf.concat((from_last_layer, autoregressive), -1)\n",
    "    subprogram_last_layer, _, subprogram_hidden_layers, subprogram_output = subprogram(\n",
    "        x_input, num_layers_subprogram, cells_subprogram,None, \n",
    "        lengths = act_lengths[:, sub_idx], reuse = (sub_idx > 0), name = 'subprogram')\n",
    "    action_prob_flat = mlp(\n",
    "        tf.reshape(subprogram_last_layer, [-1, hidden_filters_subprogram]),\n",
    "        [], output_size = num_act, name = 'action_choice_mlp', reuse = (sub_idx > 0))\n",
    "    action_prob_expanded = tf.reshape(action_prob_flat, [-1, max_num_actions + 1, num_act])\n",
    "    action_probabilities_layer = tf.nn.softmax(action_prob_expanded, axis=-1)\n",
    "    action_probabilities_presoftmax.append(action_prob_expanded)\n",
    "    delta = mlp(\n",
    "        subprogram_output, [64], output_size = hidden_filters_encoder, name = 'global_transform',\n",
    "        reuse = (sub_idx > 0)\n",
    "    )\n",
    "    last_encoding = last_encoding + delta\n",
    "    encodings.append(last_encoding)\n",
    "act_presoftmax = tf.stack(action_probabilities_presoftmax, 1)[:, :, :-1, :]\n",
    "#batch, subprogram, timestep, action_selection\n",
    "logprobabilities = tf.nn.log_softmax(act_presoftmax, -1)\n",
    "act_presoftmax_flat = tf.reshape(act_presoftmax, [-1, 9, num_act])\n",
    "mask_ph_flat = tf.reshape(mask_ph, [-1, max_actions_per_subprogram])\n",
    "act_ind_flat = tf.reshape(act_ind, [-1, max_actions_per_subprogram])\n",
    "ppl_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "    logits = act_presoftmax_flat,\n",
    "    targets = act_ind_flat,\n",
    "    weights = mask_ph_flat,\n",
    "    average_across_timesteps=False,\n",
    "    average_across_batch=False,\n",
    "    softmax_loss_function=None,\n",
    "    name=None\n",
    ")\n",
    "ppl_loss_avg = tf.reduce_mean(tf.pow(ppl_loss, 1.5)) * 100\n",
    "\n",
    "tfvars = tf.trainable_variables()\n",
    "weight_norm = tf.reduce_mean([tf.reduce_sum(tf.square(var)) for var in tfvars])*1e-4\n",
    "\n",
    "action_taken = tf.argmax(act_presoftmax, -1, output_type=tf.int32)\n",
    "correct_mat = tf.cast(tf.equal(action_taken, act_ind), tf.float32) * mask_ph\n",
    "correct_percent = tf.reduce_sum(correct_mat, [1, 2])/tf.reduce_sum(mask_ph, [1, 2])\n",
    "percent_correct = tf.reduce_mean(correct_percent)\n",
    "percent_fully_correct = tf.reduce_mean(tf.cast(tf.equal(correct_percent, 1), tf.float32))\n",
    "\n",
    "loss = ppl_loss_avg + weight_norm\n",
    "\n",
    "opt_fcn = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "optimizer, grad_norm_total = apply_clipped_optimizer(opt_fcn, loss)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'LeakyRelu_5/Maximum:0' shape=(?, 10, 256) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "trn_percent = .1\n",
    "num_samples = mask_np.shape[0]\n",
    "ordered_samples = np.arange(num_samples)\n",
    "np.random.shuffle(ordered_samples)\n",
    "trn_samples = ordered_samples[:int(np.ceil(num_samples*trn_percent))]\n",
    "val_samples_all = ordered_samples[int(np.ceil(num_samples*trn_percent)):]\n",
    "val_samples = val_samples_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 10 trn_loss 87.55928988940846 val_loss 74.59875\n",
      "itr: 10 trn_acc 0.0 trn_single_acc 0.2795148844536477 val_acc 0.0\n",
      "itr: 20 trn_loss 70.7651446882512 val_loss 73.21815032958985\n",
      "itr: 20 trn_acc 0.0014062500000000002 trn_single_acc 0.4547364735652163 val_acc 0.0002656889380887151\n",
      "itr: 30 trn_loss 61.79220419360663 val_loss 71.15014198303223\n",
      "itr: 30 trn_acc 0.005401512197015627 trn_single_acc 0.5710611911909429 val_acc 0.0008236356941051781\n",
      "itr: 40 trn_loss 54.58028836649759 val_loss 69.69479575347901\n",
      "itr: 40 trn_acc 0.008073142409536533 trn_single_acc 0.6198058226533321 val_acc 0.0015330251150298866\n",
      "itr: 50 trn_loss 45.49595709052744 val_loss 66.62024758987428\n",
      "itr: 50 trn_acc 0.009471091655187355 trn_single_acc 0.6484224374697176 val_acc 0.0029260322124930105\n",
      "itr: 60 trn_loss 36.47427163429604 val_loss 64.16535593330384\n",
      "itr: 60 trn_acc 0.03410656944874986 trn_single_acc 0.7064140586159777 val_acc 0.0056729102958296435\n",
      "itr: 70 trn_loss 27.946110045311716 val_loss 59.33933614426424\n",
      "itr: 70 trn_acc 0.09779727377317744 trn_single_acc 0.7903687723708799 val_acc 0.022141593828414803\n",
      "itr: 80 trn_loss 21.802589456450562 val_loss 54.527618182303634\n",
      "itr: 80 trn_acc 0.2876880546465142 trn_single_acc 0.8759708123190916 val_acc 0.06844223272915029\n",
      "itr: 90 trn_loss 24.12363075996104 val_loss 52.593729960264675\n",
      "itr: 90 trn_acc 0.3153125156186126 trn_single_acc 0.8784766059851551 val_acc 0.06631133093341002\n",
      "itr: 100 trn_loss 16.104724929846725 val_loss 48.707534397252125\n",
      "itr: 100 trn_acc 0.4423439416039674 trn_single_acc 0.914518340735255 val_acc 0.09779061794791051\n",
      "itr: 110 trn_loss 9.581899386045405 val_loss 44.262894992179625\n",
      "itr: 110 trn_acc 0.6533358060258446 trn_single_acc 0.956544540091251 val_acc 0.17208616066438046\n",
      "itr: 120 trn_loss 8.023434437780619 val_loss 40.04285101069238\n",
      "itr: 120 trn_acc 0.7761550796018803 trn_single_acc 0.9749071280923625 val_acc 0.24741699810441214\n",
      "itr: 130 trn_loss 7.963125889827677 val_loss 36.233571982144355\n",
      "itr: 130 trn_acc 0.7352689144172126 trn_single_acc 0.9639557901440348 val_acc 0.31500220163717163\n",
      "itr: 140 trn_loss 5.782041971267181 val_loss 33.243153168939685\n",
      "itr: 140 trn_acc 0.7274242032080143 trn_single_acc 0.9687062880295221 val_acc 0.3474533068900194\n",
      "itr: 150 trn_loss 3.6208312417223105 val_loss 30.105202025934876\n",
      "itr: 150 trn_acc 0.821605589628056 trn_single_acc 0.9798214203428077 val_acc 0.3980366326360303\n",
      "itr: 160 trn_loss 1.693075483300262 val_loss 27.13354388782217\n",
      "itr: 160 trn_acc 0.9158742587986389 trn_single_acc 0.991669572803107 val_acc 0.4568779531408904\n",
      "itr: 170 trn_loss 0.9287660464663121 val_loss 24.440812345524137\n",
      "itr: 170 trn_acc 0.9606507663794033 trn_single_acc 0.9962489705235149 val_acc 0.5104090329965554\n",
      "itr: 180 trn_loss 0.5857338707735021 val_loss 22.00631238700107\n",
      "itr: 180 trn_acc 0.9793946299770401 trn_single_acc 0.9978169861466883 val_acc 0.5593628129625858\n",
      "itr: 190 trn_loss 0.241604973683338 val_loss 19.80920832021077\n",
      "itr: 190 trn_acc 0.9920680128164611 trn_single_acc 0.9992168499142411 val_acc 0.6034265316663272\n",
      "itr: 200 trn_loss 0.14405583795885382 val_loss 17.830723291603086\n",
      "itr: 200 trn_acc 0.9947030370819505 trn_single_acc 0.9996219674185864 val_acc 0.6430838784996945\n",
      "itr: 210 trn_loss 0.23681807350646414 val_loss 16.049269808389862\n",
      "itr: 210 trn_acc 0.995180110107467 trn_single_acc 0.9996568619486066 val_acc 0.6787754906497251\n",
      "itr: 220 trn_loss 0.09115634452580551 val_loss 14.44574287173878\n",
      "itr: 220 trn_acc 0.9983194083108178 trn_single_acc 0.9998803551595011 val_acc 0.7108819973422752\n",
      "itr: 230 trn_loss 0.03954227021409464 val_loss 13.00232605228407\n",
      "itr: 230 trn_acc 0.9994140139113709 trn_single_acc 0.9999582824236487 val_acc 0.7397937976080476\n",
      "itr: 240 trn_loss 0.020733864240137734 val_loss 11.703392062671478\n",
      "itr: 240 trn_acc 0.9997956792846965 trn_single_acc 0.999985453980553 val_acc 0.765793162830916\n",
      "itr: 250 trn_loss 0.014067530710823466 val_loss 10.534100480400022\n",
      "itr: 250 trn_acc 0.9999287577717078 trn_single_acc 0.9999949281166295 val_acc 0.7892138465478244\n",
      "itr: 260 trn_loss 0.011612150055277224 val_loss 9.481724368707834\n",
      "itr: 260 trn_acc 0.9999751593709697 trn_single_acc 0.999998231543618 val_acc 0.810292461893042\n",
      "itr: 270 trn_loss 0.010661128762179848 val_loss 8.534576340475528\n",
      "itr: 270 trn_acc 0.9999913386082186 trn_single_acc 0.9999993833773874 val_acc 0.8292632157037378\n",
      "itr: 280 trn_loss 0.010265976465926408 val_loss 7.6821303403009615\n",
      "itr: 280 trn_acc 0.9999969799594245 trn_single_acc 0.9999997849969893 val_acc 0.8463368941333641\n",
      "itr: 290 trn_loss 0.010037343936344062 val_loss 6.914918538481412\n",
      "itr: 290 trn_acc 0.999998946976963 trn_single_acc 0.9999999250330857 val_acc 0.8617032047200277\n",
      "itr: 300 trn_loss 0.00998565504255764 val_loss 6.224416959740595\n",
      "itr: 300 trn_acc 0.99999963283357 trn_single_acc 0.9999999738606531 val_acc 0.8755328842480249\n",
      "itr: 310 trn_loss 0.009837538577099451 val_loss 5.602958893701026\n",
      "itr: 310 trn_acc 0.9999998719769818 trn_single_acc 0.9999999908857732 val_acc 0.8879795958232224\n",
      "itr: 320 trn_loss 0.009728349033998851 val_loss 5.043656274757912\n",
      "itr: 320 trn_acc 0.9999999553611337 trn_single_acc 0.9999999968220655 val_acc 0.8991763195065863\n",
      "itr: 330 trn_loss 0.009672001172698655 val_loss 4.540275916903135\n",
      "itr: 330 trn_acc 0.9999999844353897 trn_single_acc 0.9999999988919228 val_acc 0.9092533708216137\n",
      "itr: 340 trn_loss 0.009667448694065273 val_loss 4.087212298559919\n",
      "itr: 340 trn_acc 0.999999994572956 trn_single_acc 0.9999999996136373 val_acc 0.9183280337394524\n",
      "itr: 350 trn_loss 0.009587307470561461 val_loss 3.6794511666315275\n",
      "itr: 350 trn_acc 0.9999999981077069 trn_single_acc 0.9999999998652837 val_acc 0.9264952303655072\n",
      "itr: 360 trn_loss 0.009530527426575518 val_loss 3.31249291783206\n",
      "itr: 360 trn_acc 0.9999999993401982 trn_single_acc 0.9999999999530272 val_acc 0.9338403905946425\n",
      "itr: 370 trn_loss 0.009473755494704228 val_loss 2.9822205286678583\n",
      "itr: 370 trn_acc 0.9999999997699414 trn_single_acc 0.9999999999836215 val_acc 0.9404510348008643\n",
      "itr: 380 trn_loss 0.009423672740949907 val_loss 2.684942103288941\n",
      "itr: 380 trn_acc 0.9999999999197835 trn_single_acc 0.999999999994289 val_acc 0.9464059313207779\n",
      "itr: 390 trn_loss 0.009369617163700411 val_loss 2.4173868900983386\n",
      "itr: 390 trn_acc 0.9999999999720302 trn_single_acc 0.9999999999980087 val_acc 0.9517653381887001\n",
      "itr: 400 trn_loss 0.009327510333117068 val_loss 2.1765832173815833\n",
      "itr: 400 trn_acc 0.9999999999902476 trn_single_acc 0.9999999999993057 val_acc 0.9565888043698301\n",
      "itr: 410 trn_loss 0.009291856822188234 val_loss 1.9598548942498686\n",
      "itr: 410 trn_acc 0.9999999999965994 trn_single_acc 0.9999999999997577 val_acc 0.960929923932847\n",
      "itr: 420 trn_loss 0.009266780606795215 val_loss 1.7647949788109054\n",
      "itr: 420 trn_acc 0.9999999999988144 trn_single_acc 0.9999999999999155 val_acc 0.9648369315395623\n",
      "itr: 430 trn_loss 0.009236262143910622 val_loss 1.5892368056634598\n",
      "itr: 430 trn_acc 0.9999999999995867 trn_single_acc 0.9999999999999705 val_acc 0.968353238385606\n",
      "itr: 440 trn_loss 0.009174543936371163 val_loss 1.4312304941312552\n",
      "itr: 440 trn_acc 0.9999999999998558 trn_single_acc 0.9999999999999896 val_acc 0.9715179145470454\n",
      "itr: 450 trn_loss 0.009135768530533977 val_loss 1.28902105204726\n",
      "itr: 450 trn_acc 0.9999999999999497 trn_single_acc 0.9999999999999963 val_acc 0.9743661230923408\n",
      "itr: 460 trn_loss 0.00909769229869429 val_loss 1.161029029022587\n",
      "itr: 460 trn_acc 0.9999999999999825 trn_single_acc 0.9999999999999987 val_acc 0.9769295107831067\n",
      "itr: 470 trn_loss 0.009058152415587039 val_loss 1.0458327223599846\n",
      "itr: 470 trn_acc 0.9999999999999938 trn_single_acc 0.9999999999999994 val_acc 0.9792365597047961\n",
      "itr: 480 trn_loss 0.009021765802883707 val_loss 0.9421523826337661\n",
      "itr: 480 trn_acc 0.9999999999999978 trn_single_acc 0.9999999999999994 val_acc 0.9813129037343165\n",
      "itr: 490 trn_loss 0.008998226360952467 val_loss 0.8488367203004782\n",
      "itr: 490 trn_acc 0.9999999999999992 trn_single_acc 0.9999999999999994 val_acc 0.9831816133608848\n",
      "itr: 500 trn_loss 0.008969795698313113 val_loss 0.7648493644783758\n",
      "itr: 500 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9848634520247963\n",
      "itr: 510 trn_loss 0.00893113627999881 val_loss 0.6892572478672739\n",
      "itr: 510 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9863771068223167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 520 trn_loss 0.008912857808408182 val_loss 0.621221125942845\n",
      "itr: 520 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.987739396140085\n",
      "itr: 530 trn_loss 0.008864489681932344 val_loss 0.5599855164899339\n",
      "itr: 530 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9889654565260765\n",
      "itr: 540 trn_loss 0.008830805101315975 val_loss 0.5048704760154107\n",
      "itr: 540 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9900689108734688\n",
      "itr: 550 trn_loss 0.008810638527515206 val_loss 0.45526405705183914\n",
      "itr: 550 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.991062019786122\n",
      "itr: 560 trn_loss 0.008775609667017476 val_loss 0.4106159156360046\n",
      "itr: 560 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9919558178075097\n",
      "itr: 570 trn_loss 0.0087582629633152 val_loss 0.3704293154148295\n",
      "itr: 570 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9927602360267588\n",
      "itr: 580 trn_loss 0.008735154282397553 val_loss 0.33425840289977504\n",
      "itr: 580 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9934842124240829\n",
      "itr: 590 trn_loss 0.008698888600953983 val_loss 0.3017018908590435\n",
      "itr: 590 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9941357911816746\n",
      "itr: 600 trn_loss 0.00867271229837073 val_loss 0.27239857514921056\n",
      "itr: 600 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9947222120635072\n",
      "itr: 610 trn_loss 0.008648998745930118 val_loss 0.24602375183454062\n",
      "itr: 610 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9952499908571565\n",
      "itr: 620 trn_loss 0.008627918106555402 val_loss 0.2222901171426556\n",
      "itr: 620 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9957196750371269\n",
      "itr: 630 trn_loss 0.008592547751100877 val_loss 0.2009225644675417\n",
      "itr: 630 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9961477075334142\n",
      "itr: 640 trn_loss 0.008573291807010107 val_loss 0.18168739589743524\n",
      "itr: 640 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9965329367800728\n",
      "itr: 650 trn_loss 0.008551396495988137 val_loss 0.16437289079823533\n",
      "itr: 650 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9968796431020656\n",
      "itr: 660 trn_loss 0.008530100094632473 val_loss 0.14878755027769602\n",
      "itr: 660 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.997191678791859\n",
      "itr: 670 trn_loss 0.008491998964402236 val_loss 0.13475886486591637\n",
      "itr: 670 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9974725109126731\n",
      "itr: 680 trn_loss 0.008476047689352333 val_loss 0.12213054059554702\n",
      "itr: 680 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9977252598214058\n",
      "itr: 690 trn_loss 0.008451492883412376 val_loss 0.11076255634674043\n",
      "itr: 690 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9979527338392652\n",
      "itr: 700 trn_loss 0.008435357173874931 val_loss 0.10052958275600025\n",
      "itr: 700 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9981574604553387\n",
      "itr: 710 trn_loss 0.008410345824863386 val_loss 0.09131834180927648\n",
      "itr: 710 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9983417144098048\n",
      "itr: 720 trn_loss 0.008384566646887564 val_loss 0.08302559164264536\n",
      "itr: 720 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9985075429688243\n",
      "itr: 730 trn_loss 0.008362153101930352 val_loss 0.0755598328897244\n",
      "itr: 730 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9986567886719419\n",
      "itr: 740 trn_loss 0.008347326452457159 val_loss 0.06883869227891144\n",
      "itr: 740 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9987911098047476\n",
      "itr: 750 trn_loss 0.008322970602166727 val_loss 0.06278673401884714\n",
      "itr: 750 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9989119988242728\n",
      "itr: 760 trn_loss 0.00830022929544478 val_loss 0.057337693849168536\n",
      "itr: 760 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9990207989418456\n",
      "itr: 770 trn_loss 0.00828473283157859 val_loss 0.05243192406352965\n",
      "itr: 770 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.999118719047661\n",
      "itr: 780 trn_loss 0.008263919568406889 val_loss 0.04801590377634711\n",
      "itr: 780 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9992068471428949\n",
      "itr: 790 trn_loss 0.008242865925145105 val_loss 0.04404097366299582\n",
      "itr: 790 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9992861624286054\n",
      "itr: 800 trn_loss 0.008227935508564896 val_loss 0.040460547015515144\n",
      "itr: 800 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9993575461857448\n",
      "itr: 810 trn_loss 0.008202068216424871 val_loss 0.03723571682090805\n",
      "itr: 810 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9994217915671703\n",
      "itr: 820 trn_loss 0.008176394019090266 val_loss 0.03433140278561634\n",
      "itr: 820 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9994796124104532\n",
      "itr: 830 trn_loss 0.008157667485572135 val_loss 0.03171591846528997\n",
      "itr: 830 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9995316511694079\n",
      "itr: 840 trn_loss 0.008145849573031641 val_loss 0.029360778376907256\n",
      "itr: 840 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9995784860524671\n",
      "itr: 850 trn_loss 0.008125181834532178 val_loss 0.027240702654823794\n",
      "itr: 850 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9996206374472204\n",
      "itr: 860 trn_loss 0.008109162876903867 val_loss 0.02533149885020119\n",
      "itr: 860 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9996585737024983\n",
      "itr: 870 trn_loss 0.008086776974349189 val_loss 0.023611462956352195\n",
      "itr: 870 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9996927163322485\n",
      "itr: 880 trn_loss 0.0080732766173995 val_loss 0.022060128740832057\n",
      "itr: 880 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9997234446990236\n",
      "itr: 890 trn_loss 0.008058955283170738 val_loss 0.020662519042073054\n",
      "itr: 890 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9997511002291213\n",
      "itr: 900 trn_loss 0.00804425697263737 val_loss 0.019403108019571035\n",
      "itr: 900 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9997759902062091\n",
      "itr: 910 trn_loss 0.008027924985194433 val_loss 0.018270552099293944\n",
      "itr: 910 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9997930744512743\n",
      "itr: 920 trn_loss 0.008007464086512933 val_loss 0.01725167487089021\n",
      "itr: 920 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.999808450271833\n",
      "itr: 930 trn_loss 0.007987709431576305 val_loss 0.01633042828983828\n",
      "itr: 930 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9998222885103357\n",
      "itr: 940 trn_loss 0.00796896253964232 val_loss 0.015498202827543895\n",
      "itr: 940 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9998400596593021\n",
      "itr: 950 trn_loss 0.00795964759351351 val_loss 0.014750597174737645\n",
      "itr: 950 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.999850736959058\n",
      "itr: 960 trn_loss 0.007939562460417661 val_loss 0.014077665660477096\n",
      "itr: 960 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9998603465288383\n",
      "itr: 970 trn_loss 0.007919343772898768 val_loss 0.013468811813321484\n",
      "itr: 970 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9998689951416405\n",
      "itr: 980 trn_loss 0.007905269577501105 val_loss 0.012916715077304936\n",
      "itr: 980 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9998820956274764\n",
      "itr: 990 trn_loss 0.007894242130853048 val_loss 0.012420478450576153\n",
      "itr: 990 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9998885693304148\n",
      "itr: 1000 trn_loss 0.007875447307350094 val_loss 0.011974160995173174\n",
      "itr: 1000 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9998943956630594\n",
      "itr: 1010 trn_loss 0.00785747541326916 val_loss 0.011568011363078104\n",
      "itr: 1010 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9998996393624395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 1020 trn_loss 0.007841958357490282 val_loss 0.011200414373483313\n",
      "itr: 1020 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999096754261956\n",
      "itr: 1030 trn_loss 0.007832498798148192 val_loss 0.010868763106917785\n",
      "itr: 1030 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999133911492621\n",
      "itr: 1040 trn_loss 0.00781914936307458 val_loss 0.010569909280856353\n",
      "itr: 1040 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.999916735300022\n",
      "itr: 1050 trn_loss 0.007803515434937496 val_loss 0.010301908854085119\n",
      "itr: 1050 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999197450357059\n",
      "itr: 1060 trn_loss 0.007793241300042704 val_loss 0.010063431936595956\n",
      "itr: 1060 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999224537978214\n",
      "itr: 1070 trn_loss 0.007776807164116287 val_loss 0.009847737464094865\n",
      "itr: 1070 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999248916837253\n",
      "itr: 1080 trn_loss 0.007760718550296589 val_loss 0.009647924479351677\n",
      "itr: 1080 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999270857810388\n",
      "itr: 1090 trn_loss 0.007742523588810899 val_loss 0.00946382915177009\n",
      "itr: 1090 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999290604686211\n",
      "itr: 1100 trn_loss 0.007734839228694413 val_loss 0.009296613613051726\n",
      "itr: 1100 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999308376874451\n",
      "itr: 1110 trn_loss 0.00771802391488801 val_loss 0.009146244937657614\n",
      "itr: 1110 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999324371843866\n",
      "itr: 1120 trn_loss 0.007704141696525122 val_loss 0.009011780656781166\n",
      "itr: 1120 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.999933876731634\n",
      "itr: 1130 trn_loss 0.007691704681012837 val_loss 0.008889726288532945\n",
      "itr: 1130 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999351723241567\n",
      "itr: 1140 trn_loss 0.007675701168896231 val_loss 0.008777523392736077\n",
      "itr: 1140 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999363383574271\n",
      "itr: 1150 trn_loss 0.007665855319461144 val_loss 0.008675456494211399\n",
      "itr: 1150 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999373877873705\n",
      "itr: 1160 trn_loss 0.007651870253938907 val_loss 0.008582822729008713\n",
      "itr: 1160 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999383322743195\n",
      "itr: 1170 trn_loss 0.007638160594048484 val_loss 0.008501469398758399\n",
      "itr: 1170 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999391823125736\n",
      "itr: 1180 trn_loss 0.007625166067865456 val_loss 0.008493355971772898\n",
      "itr: 1180 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999399473470023\n",
      "itr: 1190 trn_loss 0.007607841828338695 val_loss 0.00843915909501859\n",
      "itr: 1190 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999406358779882\n",
      "itr: 1200 trn_loss 0.007595920260441326 val_loss 0.008384312197324172\n",
      "itr: 1200 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999412555558754\n",
      "itr: 1210 trn_loss 0.007584081153036817 val_loss 0.008331736181458714\n",
      "itr: 1210 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.999941813265974\n",
      "itr: 1220 trn_loss 0.007573101935557883 val_loss 0.008299826499176814\n",
      "itr: 1220 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999423152050626\n",
      "itr: 1230 trn_loss 0.007559212834195073 val_loss 0.00827665930585813\n",
      "itr: 1230 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999427669502424\n",
      "itr: 1240 trn_loss 0.007545411799490949 val_loss 0.008225555237130089\n",
      "itr: 1240 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999431735209042\n",
      "itr: 1250 trn_loss 0.0075397128843268674 val_loss 0.008244419298466232\n",
      "itr: 1250 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999435394344999\n",
      "itr: 1260 trn_loss 0.0075237550978603214 val_loss 0.00829862340709226\n",
      "itr: 1260 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999438687567359\n",
      "itr: 1270 trn_loss 0.0075106254573824765 val_loss 0.008373353844916732\n",
      "itr: 1270 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999441651467484\n",
      "itr: 1280 trn_loss 0.007502003593671626 val_loss 0.008451474278601328\n",
      "itr: 1280 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999444318977597\n",
      "itr: 1290 trn_loss 0.007486245540445192 val_loss 0.008519359740107346\n",
      "itr: 1290 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999446719736699\n",
      "itr: 1300 trn_loss 0.007478454687958807 val_loss 0.008578903395672818\n",
      "itr: 1300 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.999944888041989\n",
      "itr: 1310 trn_loss 0.00746225483903588 val_loss 0.00861656148172037\n",
      "itr: 1310 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999450825034761\n",
      "itr: 1320 trn_loss 0.007451222487894232 val_loss 0.008665989337294844\n",
      "itr: 1320 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999452575188146\n",
      "itr: 1330 trn_loss 0.007439964531093211 val_loss 0.008701403884228659\n",
      "itr: 1330 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999454150326192\n",
      "itr: 1340 trn_loss 0.007429246583038722 val_loss 0.008739655232253604\n",
      "itr: 1340 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999455567950434\n",
      "itr: 1350 trn_loss 0.007414909445302418 val_loss 0.008773529823115111\n",
      "itr: 1350 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999456843812251\n",
      "itr: 1360 trn_loss 0.007404742832150431 val_loss 0.008815831154278494\n",
      "itr: 1360 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999457992087887\n",
      "itr: 1370 trn_loss 0.007395368520036107 val_loss 0.008854562380634168\n",
      "itr: 1370 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999459025535958\n",
      "itr: 1380 trn_loss 0.007384195237612374 val_loss 0.008895413358857412\n",
      "itr: 1380 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999459955639223\n",
      "itr: 1390 trn_loss 0.007373737474527393 val_loss 0.008930668075248758\n",
      "itr: 1390 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999460792732161\n",
      "itr: 1400 trn_loss 0.007360183998206739 val_loss 0.008958356963275517\n",
      "itr: 1400 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999461546115805\n",
      "itr: 1410 trn_loss 0.0073507149859001015 val_loss 0.00898283262849925\n",
      "itr: 1410 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999462224161085\n",
      "itr: 1420 trn_loss 0.0073399755301803956 val_loss 0.0090135915038086\n",
      "itr: 1420 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999462834401838\n",
      "itr: 1430 trn_loss 0.00733152712606826 val_loss 0.009038075026014182\n",
      "itr: 1430 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999463383618514\n",
      "itr: 1440 trn_loss 0.0073192006068425195 val_loss 0.009054497673635541\n",
      "itr: 1440 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999463877913524\n",
      "itr: 1450 trn_loss 0.0073102910915852455 val_loss 0.009074378817103677\n",
      "itr: 1450 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999464322779033\n",
      "itr: 1460 trn_loss 0.007300538047649029 val_loss 0.009098880045553185\n",
      "itr: 1460 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.999946472315799\n",
      "itr: 1470 trn_loss 0.0072884820297802275 val_loss 0.00912089296693218\n",
      "itr: 1470 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999465083499052\n",
      "itr: 1480 trn_loss 0.007275760110015062 val_loss 0.009134559636693707\n",
      "itr: 1480 trn_acc 0.9999999999999994 trn_single_acc 0.9999999999999994 val_acc 0.9999465407806007\n"
     ]
    }
   ],
   "source": [
    "eval_itr = -1\n",
    "bs = 64\n",
    "for itr in range(10000):\n",
    "    samples = np.random.choice(trn_samples, size = bs, replace = False)\n",
    "    if True:#itr == 0:\n",
    "        trn_feed_dict = {\n",
    "            cmd_ind : cmd_np[samples],\n",
    "            act_ind : act_np[samples],\n",
    "            mask_ph : mask_np[samples],\n",
    "            act_lengths : np.clip(struct_np[samples], a_min = 1, a_max = None),\n",
    "            cmd_lengths : cmd_lengths_np[samples],\n",
    "        }\n",
    "        \n",
    "    trn_feed_dict[learning_rate] = .01 / (np.sqrt(itr + 10))\n",
    "    _, trn_loss, acc_trn_single, acc_trn = sess.run(\n",
    "        [optimizer, loss, percent_correct, percent_fully_correct], trn_feed_dict)\n",
    "    if itr == 0:\n",
    "        trn_loss_avg = trn_loss\n",
    "        acc_trn_avg = acc_trn\n",
    "        acc_trn_single_avg = acc_trn_single\n",
    "    else:\n",
    "        trn_loss_avg = trn_loss_avg * .9 + trn_loss * .1\n",
    "        acc_trn_avg = acc_trn_avg * .9 + acc_trn * .1\n",
    "        acc_trn_single_avg = acc_trn_single_avg * .9 + acc_trn_single * .1\n",
    "    if itr % 10 == 0 and itr > 0:\n",
    "        # val_samples = np.random.choice(val_samples_all, size = bs, replace = False)\n",
    "        eval_itr += 1\n",
    "        val_feed_dict = {\n",
    "            cmd_ind : cmd_np[val_samples],\n",
    "            act_ind : act_np[val_samples],\n",
    "            mask_ph : mask_np[val_samples],\n",
    "            act_lengths : np.clip(struct_np[val_samples], a_min = 1, a_max = None),\n",
    "            cmd_lengths : cmd_lengths_np[val_samples]\n",
    "        }\n",
    "        val_loss, acc_val = sess.run([loss, percent_fully_correct], val_feed_dict)\n",
    "        if eval_itr == 0:\n",
    "            val_loss_avg = val_loss\n",
    "            acc_val_avg = acc_val\n",
    "        else:\n",
    "            val_loss_avg = val_loss_avg * .9 + val_loss * .1\n",
    "            acc_val_avg = acc_val_avg * .9 + acc_val * .1\n",
    "        print('itr:', itr, 'trn_loss', trn_loss_avg, 'val_loss', val_loss_avg)\n",
    "        print('itr:', itr, 'trn_acc', acc_trn_avg, \n",
    "              'trn_single_acc', acc_trn_single_avg, 'val_acc', acc_val_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_percent.shape, percent_fully_correct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "10, None, 7, 9, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_np.shape, act_np.shape, mask_np.shape, struct_np.shape, cmd_lengths_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cmd_ind.shape, act_ind.shape, mask_ph.shape, act_lengths.shape, cmd_lengths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_samples.shape, val_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "act_presoftmax = tf.stack(action_probabilities_presoftmax, 1)[:, :, :-1, :]\n",
    "#batch, subprogram, timestep, action_selection\n",
    "logprobabilities = tf.nn.log_softmax(act_presoftmax, -1)\n",
    "act_presoftmax_flat = tf.reshape(act_presoftmax, [-1, 9, num_act])\n",
    "mask_ph_flat = tf.reshape(mask_ph, [-1, max_actions_per_subprogram])\n",
    "act_ind_flat = tf.reshape(act_ind, [-1, max_actions_per_subprogram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "act_presoftmax_flat = tf.reshape(act_presoftmax, [-1, 9, num_act])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_actions_per_subprogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess.run(act_presoftmax, feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(*actions_ind[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subprogram_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprogram_last_layer[:,cmd_lengths,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.gather(\n",
    "    encoding_last_layer,\n",
    "    [1,2],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.gather_nd(\n",
    "    encoding_last_layer,\n",
    "    np.array([[0,1,2,3,4], [1,4,3,2,5]]).T,\n",
    "    name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmd_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_command(sub_cmd, num_repeat):\n",
    "    return sub_cmd * num_repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_command(cmd):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uni_commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uni_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uni_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
