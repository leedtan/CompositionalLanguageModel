{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import common dependencies\n",
    "import pandas as pd  # noqa\n",
    "import numpy as np\n",
    "import matplotlib  # noqa\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime  # noqa\n",
    "import PIL  # noqa\n",
    "import glob  # noqa\n",
    "import pickle  # noqa\n",
    "from pathlib import Path  # noqa\n",
    "from scipy import misc  # noqa\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import pdb\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "TRADE_COST_FRAC = .003\n",
    "EPSILON = 1e-10\n",
    "ADV_MULT = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uni_tokens = set()\n",
    "uni_commands = set()\n",
    "uni_actions = set()\n",
    "fname = 'tasks_with_length_tags.txt'\n",
    "with open(fname) as f:\n",
    "    content = f.readlines()\n",
    "content2 = [c.split(' ') for c in content]\n",
    "# you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "commands = []\n",
    "actions = []\n",
    "content = [l.replace('\\n', '') for l in content]\n",
    "commands = [x.split(':::')[1].split(' ')[1:-1] for x in content]\n",
    "actions = [x.split(':::')[2].split(' ')[1:-2] for x in content]\n",
    "structures = [x.split(':::')[3].split(' ')[2:] for x in content]\n",
    "\n",
    "structures = [[int(l) for l in program] for program in structures]\n",
    "#actions = [[wd.replace('\\n', '') for wd in res] for res in actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 10, 49, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_actions_per_subprogram = max([max([s for s in struct]) for struct in structures]) + 1\n",
    "max_num_subprograms = max([len(s) for s in structures]) + 1\n",
    "max_cmd_len = max([len(s) for s in commands]) + 1\n",
    "max_act_len = max([len(a) for a in actions]) + 1\n",
    "cmd_lengths_list = [len(s)+1 for s in commands]\n",
    "cmd_lengths_np = np.array(cmd_lengths_list)\n",
    "max_num_subprograms, max_cmd_len, max_act_len, max_actions_per_subprogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_fmap_invmap(unique, num_unique):\n",
    "    fmap = dict(zip(unique, range(num_unique)))\n",
    "    invmap = dict(zip(range(num_unique), unique))\n",
    "    return fmap, invmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for li in content2:\n",
    "    for wd in li:\n",
    "        uni_tokens.add(wd)\n",
    "for li in commands:\n",
    "    for wd in li:\n",
    "        uni_commands.add(wd)\n",
    "for li in actions:\n",
    "    for wd in li:\n",
    "        uni_actions.add(wd)\n",
    "uni_commands.add('end_command')\n",
    "uni_actions.add('end_subprogram')\n",
    "uni_actions.add('end_action')\n",
    "num_cmd = len(uni_commands)\n",
    "num_act = len(uni_actions)\n",
    "command_map, command_invmap = build_fmap_invmap(uni_commands, num_cmd)\n",
    "action_map, action_invmap = build_fmap_invmap(uni_actions, num_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dense_scaled(prev_layer, layer_size, name=None, reuse=False, scale=1.0):\n",
    "    output = tf.layers.dense(prev_layer, layer_size, reuse=reuse) * scale\n",
    "    return output\n",
    "\n",
    "\n",
    "def dense_relu(dense_input, layer_size, scale=1.0):\n",
    "    dense = dense_scaled(dense_input, layer_size, scale=scale)\n",
    "    output = tf.nn.leaky_relu(dense)\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_grad_norm(opt_fcn, loss):\n",
    "    gvs = opt_fcn.compute_gradients(loss)\n",
    "    grad_norm = tf.sqrt(tf.reduce_sum(\n",
    "        [tf.reduce_sum(tf.square(grad)) for grad, var in gvs if grad is not None]))\n",
    "    return grad_norm\n",
    "\n",
    "\n",
    "def apply_clipped_optimizer(opt_fcn, loss, clip_norm=.1, clip_single=.03, clip_global_norm=False):\n",
    "    gvs = opt_fcn.compute_gradients(loss)\n",
    "\n",
    "    if clip_global_norm:\n",
    "        gs, vs = zip(*[(g, v) for g, v in gvs if g is not None])\n",
    "        capped_gs, grad_norm_total = tf.clip_by_global_norm([g for g in gs], clip_norm)\n",
    "        capped_gvs = list(zip(capped_gs, vs))\n",
    "    else:\n",
    "        grad_norm_total = tf.sqrt(\n",
    "            tf.reduce_sum([tf.reduce_sum(tf.square(grad)) for grad, var in gvs if grad is not None]))\n",
    "        capped_gvs = [(tf.clip_by_value(grad, -1 * clip_single, clip_single), var)\n",
    "                      for grad, var in gvs if grad is not None]\n",
    "        capped_gvs = [(tf.clip_by_norm(grad, clip_norm), var) for grad, var in capped_gvs if grad is not None]\n",
    "\n",
    "    optimizer = opt_fcn.apply_gradients(capped_gvs)\n",
    "\n",
    "    return optimizer, grad_norm_total\n",
    "\n",
    "\n",
    "def mlp(x, hidden_sizes, output_size=None, name='', reuse=False):\n",
    "    prev_layer = x\n",
    "\n",
    "    for idx, l in enumerate(hidden_sizes):\n",
    "        dense = dense_scaled(prev_layer, l, name='mlp' + name + '_' + str(idx))\n",
    "        prev_layer = tf.nn.leaky_relu(dense)\n",
    "\n",
    "    output = prev_layer\n",
    "\n",
    "    if output_size is not None:\n",
    "        output = dense_scaled(prev_layer, output_size, name='mlp' + name + 'final')\n",
    "\n",
    "    return output\n",
    "\n",
    "def mlp_with_adversaries(x, hidden_sizes, output_size=None, name='', reuse=False):\n",
    "    prev_layer = x\n",
    "    adv_phs = []\n",
    "    for idx, l in enumerate(hidden_sizes):\n",
    "        \n",
    "        adversary = tf.placeholder_with_default(tf.zeros_like(prev_layer), prev_layer.shape)\n",
    "        prev_layer = prev_layer + adversary\n",
    "        adv_phs.append(adversary)\n",
    "        \n",
    "        dense = dense_scaled(prev_layer, l, name='mlp' + name + '_' + str(idx))\n",
    "        prev_layer = tf.nn.leaky_relu(dense)\n",
    "\n",
    "    output = prev_layer\n",
    "\n",
    "    if output_size is not None:\n",
    "        output = dense_scaled(prev_layer, output_size, name='mlp' + name + 'final')\n",
    "\n",
    "    return output, adv_phs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "commands_ind = [[command_map[c] for c in cmd] + [0] * (max_cmd_len - len(cmd)) for cmd in commands]\n",
    "actions_ind = [[action_map[a] for a in act] + [0] * (max_act_len - len(act)) for act in actions]\n",
    "cmd_np = np.array(commands_ind)\n",
    "actions_structured = []\n",
    "mask_structured = []\n",
    "for row in range(len(structures)):\n",
    "    mask_row = []\n",
    "    action_row = []\n",
    "    act = actions_ind[row]\n",
    "    struct = structures[row]\n",
    "    start = 0\n",
    "    for step in struct:\n",
    "        end = start + step\n",
    "        a = act[start:end]\n",
    "        padding = max_actions_per_subprogram - step - 1\n",
    "        action_row.append(a + [action_map['end_action']] + [0] * padding)\n",
    "        start = end\n",
    "    actions_structured.append(\n",
    "        action_row + [[action_map['end_subprogram']] + [0] * (max_actions_per_subprogram - 1)] +\n",
    "        [[0] * max_actions_per_subprogram] * (max_num_subprograms - len(struct) - 1)\n",
    "    )\n",
    "act_np = np.array(actions_structured)\n",
    "struct_padded = [[sa + 1 for sa in s] + [1] + [0] * (max_num_subprograms - len(s) - 1) for s in structures]\n",
    "struct_np = np.array(struct_padded)\n",
    "\n",
    "mask_list = [[np.concatenate((np.ones(st), np.zeros(max_actions_per_subprogram - st)), 0) \n",
    "              for st in s] for s in struct_np]\n",
    "mask_np = np.array(mask_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\clip_ops.py:110: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "default_sizes = 128\n",
    "size_emb = 64\n",
    "num_layers_encoder = 6\n",
    "hidden_filters = 512\n",
    "num_layers_subprogram = 6\n",
    "hidden_filters_subprogram = 512\n",
    "init_mag = 1e-3\n",
    "cmd_mat = tf.Variable(init_mag*tf.random_normal([num_cmd, size_emb]))\n",
    "act_mat = tf.Variable(init_mag*tf.random_normal([num_act, size_emb]))\n",
    "act_st_emb = tf.Variable(init_mag*tf.random_normal([size_emb]))\n",
    "global_bs = None\n",
    "global_time_len = 7\n",
    "action_lengths = None\n",
    "max_num_actions= None\n",
    "# global_bs = 8\n",
    "global_time_len = 7\n",
    "max_num_actions = 9\n",
    "output_keep_prob = tf.placeholder_with_default(1.0, ())\n",
    "state_keep_prob = tf.placeholder_with_default(1.0, ())\n",
    "cmd_ind = tf.placeholder(tf.int32, shape=(global_bs, 10,))\n",
    "act_ind = tf.placeholder(tf.int32, shape=(global_bs, global_time_len, 9))\n",
    "mask_ph = tf.placeholder(tf.float32, shape=(global_bs, global_time_len, 9))\n",
    "cmd_lengths = tf.placeholder(tf.int32, shape=(global_bs,))\n",
    "act_lengths = tf.placeholder(tf.int32, shape=(global_bs, 7))\n",
    "learning_rate = tf.placeholder(tf.float32, shape = (None))\n",
    "\n",
    "cmd_emb = tf.nn.embedding_lookup(cmd_mat, cmd_ind)\n",
    "act_emb = tf.nn.embedding_lookup(act_mat, act_ind)\n",
    "tf_bs = tf.shape(act_ind)[0]\n",
    "act_st_emb_expanded = tf.tile(tf.reshape(\n",
    "    act_st_emb, [1, 1, 1, size_emb]), [tf_bs, global_time_len, 1, 1])\n",
    "act_emb_with_st = tf.concat((act_st_emb_expanded, act_emb), 2)\n",
    "\n",
    "first_cell_encoder = [tf.nn.rnn_cell.LSTMCell(\n",
    "    hidden_filters, forget_bias=1., name = 'layer1_'+d) for d in ['f', 'b']]\n",
    "hidden_cells_encoder = [[tf.nn.rnn_cell.LSTMCell(\n",
    "    hidden_filters,forget_bias=1., name = 'layer' + str(lidx) + '_' + d)  for d in ['f', 'b']]\n",
    "                        for lidx in range(num_layers_encoder - 1)]\n",
    "hidden_cells_encoder = [[tf.nn.rnn_cell.DropoutWrapper(cell,\n",
    "    output_keep_prob=output_keep_prob, state_keep_prob=state_keep_prob,\n",
    "    variational_recurrent=True, dtype=tf.float32) for cell in cells] \n",
    "                        for cells in hidden_cells_encoder[:-1]] + [hidden_cells_encoder[-1]]\n",
    "cells_encoder = [first_cell_encoder] + hidden_cells_encoder\n",
    "c1, c2 = zip(*cells_encoder)\n",
    "cells_encoder = [c1, c2]\n",
    "def encode(x, num_layers, cells, initial_states, lengths, name='',):\n",
    "    prev_layer = x\n",
    "    shortcut = x\n",
    "    hiddenlayers = []\n",
    "    returncells = []\n",
    "    cell_fw, cell_bw = cells\n",
    "    bs = tf.shape(x)[0]\n",
    "    for idx in range(num_layers):\n",
    "        prev_layer, c = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw = cell_fw[idx],\n",
    "                cell_bw = cell_bw[idx],\n",
    "                inputs = prev_layer,\n",
    "                sequence_length=lengths,\n",
    "                initial_state_fw=None,\n",
    "                initial_state_bw=None,\n",
    "                dtype=tf.float32,\n",
    "                scope='encoder'+str(idx)\n",
    "            )\n",
    "        prev_layer = tf.concat(prev_layer, 2)\n",
    "        prev_layer = tf.nn.leaky_relu(prev_layer)\n",
    "        returncells.append(c)\n",
    "        hiddenlayers.append(prev_layer)\n",
    "        if idx == num_layers - 1:\n",
    "            #pdb.set_trace()\n",
    "            output = tf.gather_nd(\n",
    "                        prev_layer,\n",
    "                        tf.stack([tf.range(bs), lengths], 1),\n",
    "                        name=None\n",
    "                    )\n",
    "            return prev_layer, returncells, hiddenlayers, output\n",
    "        prev_layer = tf.concat((prev_layer, shortcut), 2)\n",
    "encoding_last_layer, encoding_final_cells, encoding_hidden_layers, encoding_last_timestep = encode(\n",
    "    cmd_emb, num_layers_encoder, cells_encoder,None, lengths = cmd_lengths, name = 'encoder')\n",
    "# encoding_last_timestep = encoding_last_layer[:,cmd_lengths, :]\n",
    "hidden_filters_encoder = encoding_last_timestep.shape[-1].value\n",
    "first_cell_subprogram = tf.nn.rnn_cell.LSTMCell(\n",
    "    hidden_filters_subprogram, forget_bias=1., name = 'subpogramlayer1_')\n",
    "hidden_cells_subprogram = [tf.nn.rnn_cell.LSTMCell(\n",
    "    hidden_filters_subprogram,forget_bias=1., name = 'subpogramlayer' + str(lidx))\n",
    "                        for lidx in range(num_layers_subprogram - 1)]\n",
    "\n",
    "cells_subprogram_rnn = [first_cell_subprogram] + hidden_cells_subprogram\n",
    "\n",
    "attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "    num_units=hidden_filters_encoder, memory=encoding_last_layer,\n",
    "    memory_sequence_length=cmd_lengths)\n",
    "attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "    num_units=hidden_filters_encoder//2, memory=encoding_last_layer,\n",
    "    memory_sequence_length=cmd_lengths)\n",
    "cells_subprogram = [\n",
    "    tf.contrib.seq2seq.AttentionWrapper(\n",
    "        cell, attention_mechanism, attention_layer_size = hidden_filters_subprogram) \n",
    "    for cell in cells_subprogram_rnn]\n",
    "\n",
    "def subprogram(x, num_layers, cells, initial_states, lengths, reuse, name='',):\n",
    "    prev_layer = x\n",
    "    shortcut = x\n",
    "    hiddenlayers = []\n",
    "    returncells = []\n",
    "    bs = tf.shape(x)[0]\n",
    "    for idx in range(num_layers):\n",
    "        print(idx)\n",
    "        if idx == 0:\n",
    "            num_past_units = hidden_filters\n",
    "        else:\n",
    "            num_past_units = hidden_filters_subprogram\n",
    "        with tf.variable_scope(name + 'subprogram' + str(idx), reuse=reuse):\n",
    "#             self_attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n",
    "#                 num_units=num_past_units, memory=prev_layer,\n",
    "#                 memory_sequence_length=tf.expand_dims(tf.range(10), 0))\n",
    "#             cell_with_selfattention = tf.contrib.seq2seq.AttentionWrapper(\n",
    "#                     cells[idx], self_attention_mechanism, attention_layer_size = num_past_units)\n",
    "\n",
    "            prev_layer, c = tf.nn.dynamic_rnn(\n",
    "                    cell = cells[idx],\n",
    "                    inputs = prev_layer,\n",
    "                    sequence_length=lengths,\n",
    "                    initial_state = None,\n",
    "                    dtype=tf.float32,\n",
    "                )\n",
    "            prev_layer = tf.concat(prev_layer, 2)\n",
    "            prev_layer = tf.nn.leaky_relu(prev_layer)\n",
    "            returncells.append(c)\n",
    "            hiddenlayers.append(prev_layer)\n",
    "            if idx == num_layers - 1:\n",
    "                output = tf.gather_nd(\n",
    "                            prev_layer,\n",
    "                            tf.stack([tf.range(bs), lengths], 1),\n",
    "                            name=None\n",
    "                        )\n",
    "                return prev_layer, returncells, hiddenlayers, output\n",
    "            prev_layer = tf.concat((prev_layer, shortcut), 2)\n",
    "encodings = [encoding_last_timestep]\n",
    "last_encoding = encoding_last_timestep\n",
    "initial_cmb_encoding = last_encoding\n",
    "loss = 0\n",
    "action_probabilities_presoftmax = []\n",
    "for sub_idx in range(max_num_subprograms): \n",
    "    from_last_layer = tf.tile(tf.expand_dims(tf.concat((\n",
    "        last_encoding, initial_cmb_encoding), 1), 1), [1, max_num_actions + 1, 1])\n",
    "    autoregressive = act_emb_with_st[:,sub_idx, :, :]\n",
    "    x_input = tf.concat((from_last_layer, autoregressive), -1)\n",
    "    subprogram_last_layer, _, subprogram_hidden_layers, subprogram_output = subprogram(\n",
    "        x_input, num_layers_subprogram, cells_subprogram,None, \n",
    "        lengths = act_lengths[:, sub_idx], reuse = (sub_idx > 0), name = 'subprogram')\n",
    "    action_prob_flat = mlp(\n",
    "        tf.reshape(subprogram_last_layer, [-1, hidden_filters_subprogram]),\n",
    "        [], output_size = num_act, name = 'action_choice_mlp', reuse = (sub_idx > 0))\n",
    "    action_prob_expanded = tf.reshape(action_prob_flat, [-1, max_num_actions + 1, num_act])\n",
    "    action_probabilities_layer = tf.nn.softmax(action_prob_expanded, axis=-1)\n",
    "    action_probabilities_presoftmax.append(action_prob_expanded)\n",
    "    delta = mlp(\n",
    "        subprogram_output, [512,], output_size = hidden_filters_encoder, name = 'global_transform',\n",
    "        reuse = (sub_idx > 0)\n",
    "    )\n",
    "    last_encoding = last_encoding + delta\n",
    "    encodings.append(last_encoding)\n",
    "act_presoftmax = tf.stack(action_probabilities_presoftmax, 1)[:, :, :-1, :]\n",
    "#batch, subprogram, timestep, action_selection\n",
    "logprobabilities = tf.nn.log_softmax(act_presoftmax, -1)\n",
    "act_presoftmax_flat = tf.reshape(act_presoftmax, [-1, 9, num_act])\n",
    "mask_ph_flat = tf.reshape(mask_ph, [-1, max_actions_per_subprogram])\n",
    "act_ind_flat = tf.reshape(act_ind, [-1, max_actions_per_subprogram])\n",
    "ppl_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "    logits = act_presoftmax_flat,\n",
    "    targets = act_ind_flat,\n",
    "    weights = mask_ph_flat,\n",
    "    average_across_timesteps=False,\n",
    "    average_across_batch=False,\n",
    "    softmax_loss_function=None,\n",
    "    name=None\n",
    ")\n",
    "ppl_loss_avg = tf.reduce_mean(tf.pow(ppl_loss, 2.0)) * 10000 # + tf.reduce_mean(tf.pow(ppl_loss, 1.0)) * 100\n",
    "\n",
    "tfvars = tf.trainable_variables()\n",
    "weight_norm = tf.reduce_mean([tf.reduce_sum(tf.square(var)) for var in tfvars])*1e-3\n",
    "\n",
    "action_taken = tf.argmax(act_presoftmax, -1, output_type=tf.int32)\n",
    "correct_mat = tf.cast(tf.equal(action_taken, act_ind), tf.float32) * mask_ph\n",
    "correct_percent = tf.reduce_sum(correct_mat, [1, 2])/tf.reduce_sum(mask_ph, [1, 2])\n",
    "percent_correct = tf.reduce_mean(correct_percent)\n",
    "percent_fully_correct = tf.reduce_mean(tf.cast(tf.equal(correct_percent, 1), tf.float32))\n",
    "\n",
    "loss = ppl_loss_avg + weight_norm\n",
    "\n",
    "opt_fcn = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "#opt_fcn = tf.train.MomentumOptimizer(learning_rate=learning_rate, use_nesterov=True, momentum=.8)\n",
    "optimizer, grad_norm_total = apply_clipped_optimizer(opt_fcn, loss)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'LeakyRelu_5/Maximum:0' shape=(?, 10, 2048) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2091,), (18819,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "trn_percent = .1\n",
    "num_samples = mask_np.shape[0]\n",
    "ordered_samples = np.arange(num_samples)\n",
    "np.random.shuffle(ordered_samples)\n",
    "trn_samples = ordered_samples[:int(np.ceil(num_samples*trn_percent))]\n",
    "val_samples_all = ordered_samples[int(np.ceil(num_samples*trn_percent)):]\n",
    "val_samples = val_samples_all\n",
    "trn_samples.shape, val_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 100 trn_loss 8484.750252352602 val_loss 8619.573373708712\n",
      "itr: 100 trn_acc 0.0 trn_single_acc 0.37754691717569183 val_acc 0.0003188267176789415\n",
      "itr: 200 trn_loss 7414.392213273876 val_loss 8508.516431202448\n",
      "itr: 200 trn_acc 0.0 trn_single_acc 0.4010922729514336 val_acc 0.00029757160316701205\n",
      "itr: 300 trn_loss 6359.0626234564925 val_loss 8300.897975849657\n",
      "itr: 300 trn_acc 0.0 trn_single_acc 0.41865087969676806 val_acc 0.00028906955736224027\n",
      "itr: 400 trn_loss 5440.975984624989 val_loss 8041.408113661286\n",
      "itr: 400 trn_acc 0.0002641664462174592 trn_single_acc 0.4574863497085998 val_acc 0.000286731494765928\n",
      "itr: 500 trn_loss 5252.2979690460415 val_loss 7773.738372275858\n",
      "itr: 500 trn_acc 0.002833167558217425 trn_single_acc 0.4915852804001966 val_acc 0.00029525479568521173\n",
      "itr: 600 trn_loss 4696.057896436805 val_loss 7497.591956016712\n",
      "itr: 600 trn_acc 0.001808939927853609 trn_single_acc 0.5205172794310653 val_acc 0.0003348084382804612\n",
      "itr: 700 trn_loss 4318.371828944748 val_loss 7220.7884324858305\n",
      "itr: 700 trn_acc 0.003845556442377256 trn_single_acc 0.5416226342913056 val_acc 0.0003438378234762739\n",
      "itr: 800 trn_loss 3922.3459634226037 val_loss 6904.957972527884\n",
      "itr: 800 trn_acc 0.002015562787092264 trn_single_acc 0.5828764125546813 val_acc 0.0005592016366438176\n",
      "itr: 900 trn_loss 3560.4273391598654 val_loss 6601.248430949267\n",
      "itr: 900 trn_acc 0.008642042978731999 trn_single_acc 0.6346762441125685 val_acc 0.0009496388777299544\n",
      "itr: 1000 trn_loss 3186.289608947551 val_loss 6275.572806518718\n",
      "itr: 1000 trn_acc 0.009963492858442198 trn_single_acc 0.6716805779979471 val_acc 0.0015295248757107188\n",
      "itr: 1100 trn_loss 2956.6475154966884 val_loss 5978.169345228263\n",
      "itr: 1100 trn_acc 0.011429807956963618 trn_single_acc 0.690878545209193 val_acc 0.0023702489915723487\n",
      "itr: 1200 trn_loss 2745.2920667212325 val_loss 5693.656436345509\n",
      "itr: 1200 trn_acc 0.0136493853629052 trn_single_acc 0.6973991223977252 val_acc 0.003073762909567992\n",
      "itr: 1300 trn_loss 2490.4542824982886 val_loss 5406.231985693698\n",
      "itr: 1300 trn_acc 0.029274514299186326 trn_single_acc 0.7297042713407416 val_acc 0.004094831275606784\n",
      "itr: 1400 trn_loss 2392.2565208164215 val_loss 5135.075803173238\n",
      "itr: 1400 trn_acc 0.017200451365903636 trn_single_acc 0.7301637019496584 val_acc 0.004960655018761873\n",
      "itr: 1500 trn_loss 2182.1978870660705 val_loss 4889.655334311798\n",
      "itr: 1500 trn_acc 0.024357181140934454 trn_single_acc 0.7501151955086307 val_acc 0.005968388868604693\n",
      "itr: 1600 trn_loss 2013.734747453544 val_loss 4656.5284488306415\n",
      "itr: 1600 trn_acc 0.02627827481348796 trn_single_acc 0.7667891861450158 val_acc 0.0069338008983710385\n",
      "itr: 1700 trn_loss 1826.958892157593 val_loss 4438.88114932185\n",
      "itr: 1700 trn_acc 0.03505964556473973 trn_single_acc 0.7821105160426307 val_acc 0.00796208508400022\n",
      "itr: 1800 trn_loss 1776.1128161042204 val_loss 4221.066276389055\n",
      "itr: 1800 trn_acc 0.04240827045016368 trn_single_acc 0.7869207392049888 val_acc 0.009360467148956914\n",
      "itr: 1900 trn_loss 1653.1430819485956 val_loss 4022.6084477557465\n",
      "itr: 1900 trn_acc 0.044474521989013624 trn_single_acc 0.7988759556290614 val_acc 0.010884699938817055\n",
      "itr: 2000 trn_loss 1499.0557211443963 val_loss 3829.4881379663707\n",
      "itr: 2000 trn_acc 0.0469626773403756 trn_single_acc 0.8070533641341868 val_acc 0.012240568113807233\n",
      "itr: 2100 trn_loss 1328.1685564466468 val_loss 3643.1582207410283\n",
      "itr: 2100 trn_acc 0.05868722690913955 trn_single_acc 0.822349736234843 val_acc 0.014109130463912243\n",
      "itr: 2200 trn_loss 1274.4268927165795 val_loss 3465.1346818908332\n",
      "itr: 2200 trn_acc 0.059054459005415685 trn_single_acc 0.8245487923558487 val_acc 0.016274390434153147\n",
      "itr: 2300 trn_loss 1147.635270407438 val_loss 3305.9006200469466\n",
      "itr: 2300 trn_acc 0.07476526587528481 trn_single_acc 0.8419124069977963 val_acc 0.018446303109903583\n",
      "itr: 2400 trn_loss 1017.4231262276971 val_loss 3137.88604802168\n",
      "itr: 2400 trn_acc 0.08034840885588945 trn_single_acc 0.8464902579812064 val_acc 0.02070922367834359\n",
      "itr: 2500 trn_loss 954.9026528895238 val_loss 2980.6412761078423\n",
      "itr: 2500 trn_acc 0.08863181049754312 trn_single_acc 0.8489990323261687 val_acc 0.023075306464874497\n",
      "itr: 2600 trn_loss 913.3558932306889 val_loss 2830.567719758618\n",
      "itr: 2600 trn_acc 0.08954194235078888 trn_single_acc 0.8529352530751917 val_acc 0.025544862805101546\n",
      "itr: 2700 trn_loss 812.9312958710049 val_loss 2687.2035131230978\n",
      "itr: 2700 trn_acc 0.10595588639720184 trn_single_acc 0.8633847340816164 val_acc 0.027698384388983758\n",
      "itr: 2800 trn_loss 765.3497306672872 val_loss 2553.6742355708097\n",
      "itr: 2800 trn_acc 0.10219006154918 trn_single_acc 0.8637164307580747 val_acc 0.03049738595236925\n",
      "itr: 2900 trn_loss 718.937646586951 val_loss 2439.8209121259665\n",
      "itr: 2900 trn_acc 0.1065169209908247 trn_single_acc 0.8678137201778003 val_acc 0.032617953962159156\n",
      "itr: 3000 trn_loss 649.6802083426359 val_loss 2325.441551162602\n",
      "itr: 3000 trn_acc 0.13949368943770207 trn_single_acc 0.8735737026447988 val_acc 0.03507378436965226\n",
      "itr: 3100 trn_loss 662.0466729981312 val_loss 2234.752865304887\n",
      "itr: 3100 trn_acc 0.10986210493214055 trn_single_acc 0.8715116776164877 val_acc 0.03655604406436246\n",
      "itr: 3200 trn_loss 572.2674153449663 val_loss 2146.3644047346825\n",
      "itr: 3200 trn_acc 0.15489235697491688 trn_single_acc 0.8838966638147557 val_acc 0.03819296317139664\n",
      "itr: 3300 trn_loss 604.497085215943 val_loss 2061.336605925385\n",
      "itr: 3300 trn_acc 0.14092987675629648 trn_single_acc 0.8781391203009923 val_acc 0.04025601979559181\n",
      "itr: 3400 trn_loss 580.953697110463 val_loss 1978.315440600911\n",
      "itr: 3400 trn_acc 0.1508435357294382 trn_single_acc 0.8821584682704087 val_acc 0.042181849879372874\n",
      "itr: 3500 trn_loss 536.2419093863217 val_loss 1897.3219046624686\n",
      "itr: 3500 trn_acc 0.17293986304808798 trn_single_acc 0.8880463187805161 val_acc 0.044563377947389675\n",
      "itr: 3600 trn_loss 558.1656768591731 val_loss 1831.3372951610966\n",
      "itr: 3600 trn_acc 0.1710464871036708 trn_single_acc 0.8860359540840005 val_acc 0.04685022523156032\n",
      "itr: 3700 trn_loss 500.0292127955034 val_loss 1767.2305374706996\n",
      "itr: 3700 trn_acc 0.17395697914763755 trn_single_acc 0.8915944425527729 val_acc 0.048913701565941894\n",
      "itr: 3800 trn_loss 504.8440588181142 val_loss 1726.852702749256\n",
      "itr: 3800 trn_acc 0.18648370210750026 trn_single_acc 0.8929138667015437 val_acc 0.0511906187784959\n",
      "itr: 3900 trn_loss 510.1761113843686 val_loss 1683.178398160968\n",
      "itr: 3900 trn_acc 0.19644093984247446 trn_single_acc 0.8969218444020841 val_acc 0.05330360961364703\n",
      "itr: 4000 trn_loss 577.0329427399963 val_loss 1650.5620874676993\n",
      "itr: 4000 trn_acc 0.19755440822678666 trn_single_acc 0.8938806144092885 val_acc 0.055210615143594294\n",
      "itr: 4100 trn_loss 453.5379101712302 val_loss 1626.842682331494\n",
      "itr: 4100 trn_acc 0.23751120816782664 trn_single_acc 0.8996683733305162 val_acc 0.057197922830732294\n",
      "itr: 4200 trn_loss 468.97760703229585 val_loss 1588.7682163541683\n",
      "itr: 4200 trn_acc 0.21718913256830877 trn_single_acc 0.9010231180273545 val_acc 0.059804821657865784\n",
      "itr: 4300 trn_loss 486.0062892667164 val_loss 1562.3336103541455\n",
      "itr: 4300 trn_acc 0.18912357344116393 trn_single_acc 0.8963058438112754 val_acc 0.061428356708880326\n",
      "itr: 4400 trn_loss 463.1381337478927 val_loss 1544.124174654031\n",
      "itr: 4400 trn_acc 0.21924552259679556 trn_single_acc 0.9007745831543668 val_acc 0.06287359691875109\n",
      "itr: 4500 trn_loss 415.6472726255713 val_loss 1523.3723853171866\n",
      "itr: 4500 trn_acc 0.2663969999799889 trn_single_acc 0.9101250013474458 val_acc 0.0655452679938126\n",
      "itr: 4600 trn_loss 451.39544484959146 val_loss 1518.50971178633\n",
      "itr: 4600 trn_acc 0.2333261630458522 trn_single_acc 0.9070111079110644 val_acc 0.06698266425091681\n",
      "itr: 4700 trn_loss 440.2472758227508 val_loss 1503.5043613728578\n",
      "itr: 4700 trn_acc 0.27086346844998266 trn_single_acc 0.9110633319356171 val_acc 0.06852075469919779\n",
      "itr: 4800 trn_loss 382.27838089931475 val_loss 1498.5186680487514\n",
      "itr: 4800 trn_acc 0.31457645916090055 trn_single_acc 0.9150748034208339 val_acc 0.07070210289700639\n",
      "itr: 4900 trn_loss 367.1282880454947 val_loss 1488.706306527194\n",
      "itr: 4900 trn_acc 0.2937469282041884 trn_single_acc 0.9183079637101742 val_acc 0.07295757409957315\n",
      "itr: 5000 trn_loss 368.8704969929555 val_loss 1505.6749016199228\n",
      "itr: 5000 trn_acc 0.30921250424238617 trn_single_acc 0.9218456405041318 val_acc 0.07427013806694725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 5100 trn_loss 392.91652040684005 val_loss 1504.6925248172656\n",
      "itr: 5100 trn_acc 0.3117788440650202 trn_single_acc 0.9162467829300336 val_acc 0.07623788487484207\n",
      "itr: 5200 trn_loss 430.1242667939595 val_loss 1516.254996314117\n",
      "itr: 5200 trn_acc 0.3051416140736608 trn_single_acc 0.9188872640525094 val_acc 0.0780779361239528\n",
      "itr: 5300 trn_loss 376.73013638181044 val_loss 1525.8031116604466\n",
      "itr: 5300 trn_acc 0.3118889767912271 trn_single_acc 0.9195807508939349 val_acc 0.07942578308757113\n",
      "itr: 5400 trn_loss 370.64369728531955 val_loss 1533.5694891351195\n",
      "itr: 5400 trn_acc 0.3344749911020167 trn_single_acc 0.9216915805553549 val_acc 0.08098955474443283\n",
      "itr: 5500 trn_loss 358.38163783225764 val_loss 1550.4273480565744\n",
      "itr: 5500 trn_acc 0.36565483710083085 trn_single_acc 0.9283527325456096 val_acc 0.08255636259444782\n",
      "itr: 5600 trn_loss 353.77490661508153 val_loss 1565.7381786659969\n",
      "itr: 5600 trn_acc 0.3564729261420092 trn_single_acc 0.9261504147306001 val_acc 0.08463602576658709\n",
      "itr: 5700 trn_loss 404.0871564708368 val_loss 1573.2473177648346\n",
      "itr: 5700 trn_acc 0.3296580498269658 trn_single_acc 0.9230616912771129 val_acc 0.08652897773602436\n",
      "itr: 5800 trn_loss 354.76300316897164 val_loss 1587.2203264535044\n",
      "itr: 5800 trn_acc 0.37643228655107414 trn_single_acc 0.9293647568394211 val_acc 0.08897124973764908\n",
      "itr: 5900 trn_loss 336.96437283224736 val_loss 1602.405113316854\n",
      "itr: 5900 trn_acc 0.39222839737792936 trn_single_acc 0.9308721806388831 val_acc 0.0909301745010105\n",
      "itr: 6000 trn_loss 321.213088477317 val_loss 1627.3356605184122\n",
      "itr: 6000 trn_acc 0.4217031137377799 trn_single_acc 0.938099600395492 val_acc 0.09280479613906503\n",
      "itr: 6100 trn_loss 341.8984727875138 val_loss 1675.2973817267714\n",
      "itr: 6100 trn_acc 0.40772978523845566 trn_single_acc 0.9355574982977335 val_acc 0.09365769236887925\n",
      "itr: 6200 trn_loss 329.65620874739835 val_loss 1718.2157762467489\n",
      "itr: 6200 trn_acc 0.41535789378524 trn_single_acc 0.9388006501793411 val_acc 0.09514797286911764\n",
      "itr: 6300 trn_loss 298.93254154618916 val_loss 1755.3488500241203\n",
      "itr: 6300 trn_acc 0.4719757538556553 trn_single_acc 0.9438751167064214 val_acc 0.09654767688424\n",
      "itr: 6400 trn_loss 324.5750436170798 val_loss 1796.5048960258216\n",
      "itr: 6400 trn_acc 0.4569878398017357 trn_single_acc 0.9426076790081007 val_acc 0.09736105309294125\n",
      "itr: 6500 trn_loss 334.3555508558619 val_loss 1832.6959179485866\n",
      "itr: 6500 trn_acc 0.43999341070528253 trn_single_acc 0.9384059631064168 val_acc 0.09866697977259448\n",
      "itr: 6600 trn_loss 294.6871664079932 val_loss 1870.1703470036798\n",
      "itr: 6600 trn_acc 0.4953496435429896 trn_single_acc 0.947285638653743 val_acc 0.10108042420460225\n",
      "itr: 6700 trn_loss 290.8660094617147 val_loss 1904.722589079619\n",
      "itr: 6700 trn_acc 0.495455120213942 trn_single_acc 0.9483623450091584 val_acc 0.1031993864072878\n",
      "itr: 6800 trn_loss 284.2214487608902 val_loss 1942.4084119969464\n",
      "itr: 6800 trn_acc 0.5014370673808554 trn_single_acc 0.9496639792082937 val_acc 0.10532963109208006\n",
      "itr: 6900 trn_loss 260.5911378791109 val_loss 1971.5560208511836\n",
      "itr: 6900 trn_acc 0.5585172322503277 trn_single_acc 0.9555011947511488 val_acc 0.10748065756802429\n",
      "itr: 7000 trn_loss 316.1675620774969 val_loss 2021.3194077664268\n",
      "itr: 7000 trn_acc 0.5039982914949227 trn_single_acc 0.9505994083428472 val_acc 0.10979917345758884\n",
      "itr: 7100 trn_loss 256.79991569607193 val_loss 2065.0610818099026\n",
      "itr: 7100 trn_acc 0.5656886564902723 trn_single_acc 0.9575252022806375 val_acc 0.11198679955197023\n",
      "itr: 7200 trn_loss 298.3599171132191 val_loss 2114.9522526940364\n",
      "itr: 7200 trn_acc 0.5392327298505397 trn_single_acc 0.9524008632330961 val_acc 0.11313202734957623\n",
      "itr: 7300 trn_loss 306.79413247522996 val_loss 2171.87851509642\n",
      "itr: 7300 trn_acc 0.5312371749987457 trn_single_acc 0.9539378084022488 val_acc 0.11432745950504744\n",
      "itr: 7400 trn_loss 255.59652015388383 val_loss 2220.1061947446174\n",
      "itr: 7400 trn_acc 0.5726595334881288 trn_single_acc 0.9596151306480121 val_acc 0.11574874405563203\n",
      "itr: 7500 trn_loss 259.470396580178 val_loss 2283.539036776474\n",
      "itr: 7500 trn_acc 0.5680358082892472 trn_single_acc 0.9592046970033588 val_acc 0.11657091518915166\n",
      "itr: 7600 trn_loss 232.63546555813411 val_loss 2330.211298277765\n",
      "itr: 7600 trn_acc 0.5836334072543323 trn_single_acc 0.9599588098862296 val_acc 0.1181663875684245\n",
      "itr: 7700 trn_loss 263.45941292189286 val_loss 2388.5871256740966\n",
      "itr: 7700 trn_acc 0.5799422585339363 trn_single_acc 0.9625584495344555 val_acc 0.12033561416058998\n",
      "itr: 7800 trn_loss 259.14676670446744 val_loss 2452.2212998550676\n",
      "itr: 7800 trn_acc 0.6242360947833399 trn_single_acc 0.9656707947068885 val_acc 0.12197971893295757\n",
      "itr: 7900 trn_loss 170.8145143283078 val_loss 2493.284057777632\n",
      "itr: 7900 trn_acc 0.6960563633357597 trn_single_acc 0.9746937219460808 val_acc 0.12488881967917402\n",
      "itr: 8000 trn_loss 188.4775413126065 val_loss 2549.5644213315927\n",
      "itr: 8000 trn_acc 0.686333374768803 trn_single_acc 0.973568623872737 val_acc 0.1270340840528784\n",
      "itr: 8100 trn_loss 198.1234334548722 val_loss 2619.541741750095\n",
      "itr: 8100 trn_acc 0.6560074860085919 trn_single_acc 0.9709412173124727 val_acc 0.12866725038604532\n",
      "itr: 8200 trn_loss 179.8249813071393 val_loss 2680.6410441659964\n",
      "itr: 8200 trn_acc 0.7055601274967132 trn_single_acc 0.9751334686809046 val_acc 0.13080663619286295\n",
      "itr: 8300 trn_loss 166.13823743569796 val_loss 2750.430074617704\n",
      "itr: 8300 trn_acc 0.714598162074003 trn_single_acc 0.977149058856404 val_acc 0.13240794292285027\n",
      "itr: 8400 trn_loss 173.21359196891274 val_loss 2829.4762447246685\n",
      "itr: 8400 trn_acc 0.712442987933713 trn_single_acc 0.9760299823021183 val_acc 0.13475777512522383\n",
      "itr: 8500 trn_loss 207.46553163089763 val_loss 2899.570446098063\n",
      "itr: 8500 trn_acc 0.7015823290430611 trn_single_acc 0.9744906612055528 val_acc 0.13642626670245117\n",
      "itr: 8600 trn_loss 139.32084879020627 val_loss 2954.353106354816\n",
      "itr: 8600 trn_acc 0.7634814661306173 trn_single_acc 0.9815050747992768 val_acc 0.1374709241599493\n",
      "itr: 8700 trn_loss 171.50781181082846 val_loss 3039.2626241021226\n",
      "itr: 8700 trn_acc 0.7811554164899859 trn_single_acc 0.9819143308935816 val_acc 0.1398617774371368\n",
      "itr: 8800 trn_loss 181.87149235925548 val_loss 3133.9100063527358\n",
      "itr: 8800 trn_acc 0.7216813729713906 trn_single_acc 0.9780924358685116 val_acc 0.14040878424095488\n",
      "itr: 8900 trn_loss 184.94540759973847 val_loss 3223.8519152529498\n",
      "itr: 8900 trn_acc 0.7501936966158903 trn_single_acc 0.9790903819089208 val_acc 0.14092234547906143\n",
      "itr: 9000 trn_loss 132.91553222871295 val_loss 3303.388275108028\n",
      "itr: 9000 trn_acc 0.7626351857723757 trn_single_acc 0.9820890391967426 val_acc 0.14298399796022165\n",
      "itr: 9100 trn_loss 131.02541502598552 val_loss 3379.979505587986\n",
      "itr: 9100 trn_acc 0.8067685381758517 trn_single_acc 0.9860836544296733 val_acc 0.14521144969722463\n",
      "itr: 9200 trn_loss 132.4823555482813 val_loss 3450.805906559436\n",
      "itr: 9200 trn_acc 0.7814693351017175 trn_single_acc 0.9825057578096084 val_acc 0.1452766270614721\n",
      "itr: 9300 trn_loss 102.7621808723 val_loss 3528.8172255458007\n",
      "itr: 9300 trn_acc 0.8340997054742396 trn_single_acc 0.9878349408623868 val_acc 0.14828443382782505\n",
      "itr: 9400 trn_loss 112.92382022630304 val_loss 3617.749520510915\n",
      "itr: 9400 trn_acc 0.8089771469238838 trn_single_acc 0.986240515097965 val_acc 0.1499712144208117\n",
      "itr: 9500 trn_loss 115.38888341553475 val_loss 3711.4310723393487\n",
      "itr: 9500 trn_acc 0.818069305348715 trn_single_acc 0.9868113006677415 val_acc 0.15237140420674478\n",
      "itr: 9600 trn_loss 222.63944060139022 val_loss 3856.14892586098\n",
      "itr: 9600 trn_acc 0.780439424369483 trn_single_acc 0.9830770570412991 val_acc 0.15256547692173106\n",
      "itr: 9700 trn_loss 116.45247118482442 val_loss 3934.148272302344\n",
      "itr: 9700 trn_acc 0.8385407375200695 trn_single_acc 0.9880293143552848 val_acc 0.15389323232764926\n",
      "itr: 9800 trn_loss 86.60243521611362 val_loss 3998.449155140568\n",
      "itr: 9800 trn_acc 0.8672821798334132 trn_single_acc 0.9899011133612748 val_acc 0.15650699108648858\n",
      "itr: 9900 trn_loss 125.42992902206245 val_loss 4137.981579653234\n",
      "itr: 9900 trn_acc 0.8362483019117593 trn_single_acc 0.9877347177256182 val_acc 0.15560734144911875\n",
      "itr: 10000 trn_loss 81.63046220704247 val_loss 4251.373590160899\n",
      "itr: 10000 trn_acc 0.8704611836100602 trn_single_acc 0.9903397819167143 val_acc 0.15775211769264408\n",
      "itr: 10100 trn_loss 79.57599698161918 val_loss 4348.850136890935\n",
      "itr: 10100 trn_acc 0.91672901838183 trn_single_acc 0.9939551802075011 val_acc 0.16016597016712164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 10200 trn_loss 89.8852994520192 val_loss 4539.605410548573\n",
      "itr: 10200 trn_acc 0.8426772370768189 trn_single_acc 0.989268516633296 val_acc 0.16018104327119062\n",
      "itr: 10300 trn_loss 139.4577293476209 val_loss 4666.745986879134\n",
      "itr: 10300 trn_acc 0.8712917133742822 trn_single_acc 0.9910598206470835 val_acc 0.16025306062960212\n",
      "itr: 10400 trn_loss 87.88163581937695 val_loss 4802.112915065033\n",
      "itr: 10400 trn_acc 0.8403567399049453 trn_single_acc 0.9893911289186167 val_acc 0.16212987476431448\n",
      "itr: 10500 trn_loss 87.27082762459844 val_loss 4927.944227871014\n",
      "itr: 10500 trn_acc 0.8683803594100754 trn_single_acc 0.9906378905069637 val_acc 0.16223018767578887\n",
      "itr: 10600 trn_loss 68.75318651159158 val_loss 5039.959947900275\n",
      "itr: 10600 trn_acc 0.8989596277026323 trn_single_acc 0.9926836136725065 val_acc 0.16380301353348126\n",
      "itr: 10700 trn_loss 75.99197853368837 val_loss 5133.392739012763\n",
      "itr: 10700 trn_acc 0.8736648432466868 trn_single_acc 0.9914260789544668 val_acc 0.16567554176741092\n",
      "itr: 10800 trn_loss 81.52515433051353 val_loss 5285.699476841705\n",
      "itr: 10800 trn_acc 0.8821617613168911 trn_single_acc 0.9920706659144617 val_acc 0.16659563305551814\n",
      "itr: 10900 trn_loss 81.18623909301424 val_loss 5408.208431985234\n",
      "itr: 10900 trn_acc 0.8967164424802998 trn_single_acc 0.9927597345346957 val_acc 0.16802948597824624\n",
      "itr: 11000 trn_loss 65.04407116402581 val_loss 5509.288296352141\n",
      "itr: 11000 trn_acc 0.8977286442257182 trn_single_acc 0.9932907842291352 val_acc 0.1702604924260128\n",
      "itr: 11100 trn_loss 56.28998182984055 val_loss 5621.364454767152\n",
      "itr: 11100 trn_acc 0.9375647898303084 trn_single_acc 0.9958297045851671 val_acc 0.17157760700720662\n",
      "itr: 11200 trn_loss 39.73061284642336 val_loss 5830.1942205808045\n",
      "itr: 11200 trn_acc 0.9227805857557059 trn_single_acc 0.9953685518865447 val_acc 0.17193937444294377\n",
      "itr: 11300 trn_loss 41.44756909630354 val_loss 5928.335905430553\n",
      "itr: 11300 trn_acc 0.9344842578513157 trn_single_acc 0.9963477900360482 val_acc 0.17310985593695644\n",
      "itr: 11400 trn_loss 53.863800455335046 val_loss 6071.605034072304\n",
      "itr: 11400 trn_acc 0.9216679897387108 trn_single_acc 0.99441136012402 val_acc 0.17372224565544528\n",
      "itr: 11500 trn_loss 46.17538817404452 val_loss 6175.944903841265\n",
      "itr: 11500 trn_acc 0.9353729527714367 trn_single_acc 0.995695030643859 val_acc 0.17547431037216762\n",
      "itr: 11600 trn_loss 51.53400621994468 val_loss 6297.468790063\n",
      "itr: 11600 trn_acc 0.9098118234802717 trn_single_acc 0.9945504156138689 val_acc 0.17551548659357247\n",
      "itr: 11700 trn_loss 56.455558008122544 val_loss 6469.737090611304\n",
      "itr: 11700 trn_acc 0.936510872923363 trn_single_acc 0.9961770100258373 val_acc 0.17565350698676852\n",
      "itr: 11800 trn_loss 285.9841510610634 val_loss 6583.955869823536\n",
      "itr: 11800 trn_acc 0.9359674638428509 trn_single_acc 0.9957445683648437 val_acc 0.17665981259289001\n",
      "itr: 11900 trn_loss 73.81476585785343 val_loss 6747.389968297922\n",
      "itr: 11900 trn_acc 0.9112870725429414 trn_single_acc 0.9937874057966325 val_acc 0.1760085505004005\n",
      "itr: 12000 trn_loss 46.10580457521295 val_loss 6877.6522887974415\n",
      "itr: 12000 trn_acc 0.9360919112168286 trn_single_acc 0.9962108184484593 val_acc 0.1754542972890862\n",
      "itr: 12100 trn_loss 49.484057783747005 val_loss 7105.312038330472\n",
      "itr: 12100 trn_acc 0.9409177094357786 trn_single_acc 0.9963764067964156 val_acc 0.17696939149890867\n",
      "itr: 12200 trn_loss 42.358083349700166 val_loss 7227.642551690655\n",
      "itr: 12200 trn_acc 0.9481402928576586 trn_single_acc 0.9967353257277382 val_acc 0.17923100487571952\n",
      "itr: 12300 trn_loss 44.20181242889862 val_loss 7362.742870068022\n",
      "itr: 12300 trn_acc 0.9370129406812581 trn_single_acc 0.9960236398894465 val_acc 0.18057566569336997\n",
      "itr: 12400 trn_loss 59.176861503596214 val_loss 7430.664643576752\n",
      "itr: 12400 trn_acc 0.9215774364697857 trn_single_acc 0.9944087235013687 val_acc 0.18234912096366312\n",
      "itr: 12500 trn_loss 60.093021826021484 val_loss 7520.120352153927\n",
      "itr: 12500 trn_acc 0.9474862364152405 trn_single_acc 0.9966297010012759 val_acc 0.18141587208000737\n",
      "itr: 12600 trn_loss 32.64732561989627 val_loss 7586.931784499784\n",
      "itr: 12600 trn_acc 0.9563944182675508 trn_single_acc 0.9973506549457408 val_acc 0.18330191652087213\n",
      "itr: 12700 trn_loss 34.04939319821291 val_loss 7755.989880744346\n",
      "itr: 12700 trn_acc 0.9568788519841803 trn_single_acc 0.9965257891687649 val_acc 0.18336271270039023\n",
      "itr: 12800 trn_loss 54.57682359514402 val_loss 8002.129423605236\n",
      "itr: 12800 trn_acc 0.9176192290081457 trn_single_acc 0.9940507422957772 val_acc 0.18184986456654337\n",
      "itr: 12900 trn_loss 40.520911628606164 val_loss 8135.109499660899\n",
      "itr: 12900 trn_acc 0.9552601721088012 trn_single_acc 0.997097383449274 val_acc 0.1842079462858272\n",
      "itr: 13000 trn_loss 30.397475382779085 val_loss 8328.787536579843\n",
      "itr: 13000 trn_acc 0.9578523832801933 trn_single_acc 0.9972955571586549 val_acc 0.18616017891708722\n",
      "itr: 13100 trn_loss 146.71615298422728 val_loss 8469.183487656872\n",
      "itr: 13100 trn_acc 0.9490283898197519 trn_single_acc 0.9959014924590806 val_acc 0.1871360628269078\n",
      "itr: 13200 trn_loss 28.6961712355884 val_loss 8694.136819311016\n",
      "itr: 13200 trn_acc 0.9664373013852653 trn_single_acc 0.9979301582447085 val_acc 0.18742984269650992\n",
      "itr: 13300 trn_loss 51.61701029273902 val_loss 8834.244689964838\n",
      "itr: 13300 trn_acc 0.9472975469391139 trn_single_acc 0.9968197884834287 val_acc 0.18912365103023746\n",
      "itr: 13400 trn_loss 38.330148851211796 val_loss 8974.873430969354\n",
      "itr: 13400 trn_acc 0.955687496012437 trn_single_acc 0.9970113071529866 val_acc 0.18953218501855756\n",
      "itr: 13500 trn_loss 38.206033573770014 val_loss 9091.819766022763\n",
      "itr: 13500 trn_acc 0.9431386817454281 trn_single_acc 0.9962319532476891 val_acc 0.1898361002645099\n",
      "itr: 13600 trn_loss 38.3840180228134 val_loss 9209.15102283091\n",
      "itr: 13600 trn_acc 0.9562013244682092 trn_single_acc 0.997043958794167 val_acc 0.19201195673468466\n",
      "itr: 13700 trn_loss 31.329936828559305 val_loss 9355.355882750975\n",
      "itr: 13700 trn_acc 0.9602671366695894 trn_single_acc 0.9971185799124691 val_acc 0.19282776515282574\n",
      "itr: 13800 trn_loss 77.39639975584733 val_loss 9447.39406306995\n",
      "itr: 13800 trn_acc 0.961720555847849 trn_single_acc 0.9974694500331693 val_acc 0.19400835013390322\n",
      "itr: 13900 trn_loss 26.592046047451 val_loss 9537.671523492869\n",
      "itr: 13900 trn_acc 0.9731158248607282 trn_single_acc 0.9982041963906251 val_acc 0.1961442598997254\n",
      "itr: 14000 trn_loss 25.176512203417168 val_loss 9698.163736682307\n",
      "itr: 14000 trn_acc 0.9677180484224692 trn_single_acc 0.9979891169640309 val_acc 0.19746612170400335\n",
      "itr: 14100 trn_loss 33.656531186718084 val_loss 9836.441259539964\n",
      "itr: 14100 trn_acc 0.9574162014550494 trn_single_acc 0.9968732237108826 val_acc 0.1980872230146594\n",
      "itr: 14200 trn_loss 95.84230694005316 val_loss 10055.949573225282\n",
      "itr: 14200 trn_acc 0.9410454048448369 trn_single_acc 0.9961242804043757 val_acc 0.19711053217076294\n",
      "itr: 14300 trn_loss 58.95633072647457 val_loss 10174.394788595888\n",
      "itr: 14300 trn_acc 0.9673830007718425 trn_single_acc 0.9974865493818373 val_acc 0.19784689911432113\n",
      "itr: 14400 trn_loss 29.162149519206178 val_loss 10215.29348300217\n",
      "itr: 14400 trn_acc 0.9693763333350788 trn_single_acc 0.9977604047417418 val_acc 0.20032162787566546\n",
      "itr: 14500 trn_loss 26.741943196481746 val_loss 10432.754361775696\n",
      "itr: 14500 trn_acc 0.9665844016508399 trn_single_acc 0.9980474403492999 val_acc 0.20033835185147636\n",
      "itr: 14600 trn_loss 16.87358926830064 val_loss 10626.71034039013\n",
      "itr: 14600 trn_acc 0.9773335030886124 trn_single_acc 0.9984144591652329 val_acc 0.2009485466360402\n",
      "itr: 14700 trn_loss 25.931218346597706 val_loss 10688.694999717238\n",
      "itr: 14700 trn_acc 0.9733474507860236 trn_single_acc 0.9981090888050974 val_acc 0.20197064824019642\n",
      "itr: 14800 trn_loss 13.852692356452609 val_loss 10926.294629039818\n",
      "itr: 14800 trn_acc 0.9847346970105747 trn_single_acc 0.9989987548419468 val_acc 0.20191280441638826\n",
      "itr: 14900 trn_loss 20.042973636574832 val_loss 11003.172684185558\n",
      "itr: 14900 trn_acc 0.9745610391814084 trn_single_acc 0.9984119501796691 val_acc 0.20333266165491204\n",
      "itr: 15000 trn_loss 20.93796564617344 val_loss 11279.272815678038\n",
      "itr: 15000 trn_acc 0.9659005403400278 trn_single_acc 0.9976144034278288 val_acc 0.20252221816862803\n",
      "itr: 15100 trn_loss 21.719002267976332 val_loss 11425.609286876357\n",
      "itr: 15100 trn_acc 0.9695849707502338 trn_single_acc 0.9978327195058143 val_acc 0.20277055429852117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 15200 trn_loss 35.44759400576696 val_loss 11619.26259778246\n",
      "itr: 15200 trn_acc 0.9671502449034803 trn_single_acc 0.9978842419290649 val_acc 0.20217042112824607\n",
      "itr: 15300 trn_loss 14.466993354460826 val_loss 11784.814200501474\n",
      "itr: 15300 trn_acc 0.9816037759947353 trn_single_acc 0.9987640460763524 val_acc 0.2028259014661362\n",
      "itr: 15400 trn_loss 19.264483884792053 val_loss 11938.265020220435\n",
      "itr: 15400 trn_acc 0.972781244576585 trn_single_acc 0.9981986788687239 val_acc 0.2037346604880746\n",
      "itr: 15500 trn_loss 30.09210303452219 val_loss 12150.602680663853\n",
      "itr: 15500 trn_acc 0.9699622825314409 trn_single_acc 0.9981174690945351 val_acc 0.203691711470086\n",
      "itr: 15600 trn_loss 9.388609364740034 val_loss 12340.708508816508\n",
      "itr: 15600 trn_acc 0.9833245808853048 trn_single_acc 0.9990794433801624 val_acc 0.2054650558658799\n",
      "itr: 15700 trn_loss 14.421023720115448 val_loss 12487.867297333225\n",
      "itr: 15700 trn_acc 0.9863512721715844 trn_single_acc 0.9991001601310153 val_acc 0.20729487208172562\n",
      "itr: 15800 trn_loss 20.056027014333 val_loss 12646.582507430077\n",
      "itr: 15800 trn_acc 0.9849659665827342 trn_single_acc 0.9990633779105426 val_acc 0.20830936701941524\n",
      "itr: 15900 trn_loss 20.29634696343352 val_loss 12752.300931024856\n",
      "itr: 15900 trn_acc 0.9826536165948024 trn_single_acc 0.9986704345555643 val_acc 0.2092596089135734\n",
      "itr: 16000 trn_loss 13.159883172573638 val_loss 12857.080227249853\n",
      "itr: 16000 trn_acc 0.9784937989740541 trn_single_acc 0.9986758331716045 val_acc 0.211347623260166\n",
      "itr: 16100 trn_loss 27.568779679689744 val_loss 12969.071963456272\n",
      "itr: 16100 trn_acc 0.977044569023881 trn_single_acc 0.9984325812327859 val_acc 0.21242445559927403\n",
      "itr: 16200 trn_loss 29.085951887845187 val_loss 13077.66687711763\n",
      "itr: 16200 trn_acc 0.9834010420294559 trn_single_acc 0.9989184594086968 val_acc 0.21361678340668813\n",
      "itr: 16300 trn_loss 23.069989896511903 val_loss 13187.640204300795\n",
      "itr: 16300 trn_acc 0.9881876656115718 trn_single_acc 0.9992559190641068 val_acc 0.21445607217388796\n",
      "itr: 16400 trn_loss 18.209783564562127 val_loss 13386.10384915362\n",
      "itr: 16400 trn_acc 0.9876537801240862 trn_single_acc 0.9990106046665731 val_acc 0.21580126149207388\n",
      "itr: 16500 trn_loss 9.72106859563373 val_loss 13534.412587474126\n",
      "itr: 16500 trn_acc 0.9906379491131202 trn_single_acc 0.9994318246746309 val_acc 0.21700661809981323\n",
      "itr: 16600 trn_loss 19.449915518094976 val_loss 13626.084985233789\n",
      "itr: 16600 trn_acc 0.9795297892360513 trn_single_acc 0.9985987159003392 val_acc 0.21883005427606816\n",
      "itr: 16700 trn_loss 9.706331306101035 val_loss 13804.471879228442\n",
      "itr: 16700 trn_acc 0.9899496109510859 trn_single_acc 0.9991889272141875 val_acc 0.2208377975600284\n",
      "itr: 16800 trn_loss 21.13384301580534 val_loss 14071.67106475547\n",
      "itr: 16800 trn_acc 0.9767738068767791 trn_single_acc 0.998101845516789 val_acc 0.22174673792746358\n",
      "itr: 16900 trn_loss 8.26454165243957 val_loss 14224.60947623362\n",
      "itr: 16900 trn_acc 0.9936601550440867 trn_single_acc 0.9994755191845419 val_acc 0.22242662601382773\n",
      "itr: 17000 trn_loss 24.5729121086145 val_loss 14310.687833084377\n",
      "itr: 17000 trn_acc 0.9820266829892241 trn_single_acc 0.9988061347595862 val_acc 0.22376651296358904\n",
      "itr: 17100 trn_loss 10.911954875892445 val_loss 14606.138059030218\n",
      "itr: 17100 trn_acc 0.989342288968569 trn_single_acc 0.999051521133855 val_acc 0.22384589014924194\n",
      "itr: 17200 trn_loss 15.386200969243545 val_loss 14745.78244319867\n",
      "itr: 17200 trn_acc 0.9881420083919424 trn_single_acc 0.9992099907276496 val_acc 0.22364632690630246\n",
      "itr: 17300 trn_loss 33.116913378212566 val_loss 15011.031100749711\n",
      "itr: 17300 trn_acc 0.9762426488479155 trn_single_acc 0.9982319533021637 val_acc 0.22236676781166456\n",
      "itr: 17400 trn_loss 7.7706847372970795 val_loss 15258.866678070746\n",
      "itr: 17400 trn_acc 0.9919316411857737 trn_single_acc 0.9995014622867059 val_acc 0.22260206084839387\n",
      "itr: 17500 trn_loss 12.84183781575148 val_loss 15500.342432715715\n",
      "itr: 17500 trn_acc 0.9837158615862005 trn_single_acc 0.9991446096502171 val_acc 0.22245248763474745\n",
      "itr: 17600 trn_loss 5.041597416815001 val_loss 15682.90554541376\n",
      "itr: 17600 trn_acc 0.9962139146081562 trn_single_acc 0.9996898949320046 val_acc 0.22430522494933106\n",
      "itr: 17700 trn_loss 21.153335455244154 val_loss 15870.902478426\n",
      "itr: 17700 trn_acc 0.9768190126036281 trn_single_acc 0.9986912572250827 val_acc 0.22397470776833495\n",
      "itr: 17800 trn_loss 20.681345848627338 val_loss 16046.57653323091\n",
      "itr: 17800 trn_acc 0.9741054643386069 trn_single_acc 0.998288656001061 val_acc 0.22467623268749914\n",
      "itr: 17900 trn_loss 35.04736382417231 val_loss 16323.759234961119\n",
      "itr: 17900 trn_acc 0.9704058092872127 trn_single_acc 0.9979863995516666 val_acc 0.22499940595432394\n",
      "itr: 18000 trn_loss 5.535530611591197 val_loss 16474.606110522807\n",
      "itr: 18000 trn_acc 0.9930853621609861 trn_single_acc 0.9994771806760241 val_acc 0.22694816082639674\n",
      "itr: 18100 trn_loss 15.566503259768528 val_loss 16693.252592773424\n",
      "itr: 18100 trn_acc 0.9808883484232124 trn_single_acc 0.999021354095262 val_acc 0.22782526673764517\n",
      "itr: 18200 trn_loss 6.383031015629704 val_loss 16964.497060842157\n",
      "itr: 18200 trn_acc 0.9874153888514491 trn_single_acc 0.999375318285851 val_acc 0.22807797041634253\n",
      "itr: 18300 trn_loss 8.502382532697336 val_loss 17051.096039740434\n",
      "itr: 18300 trn_acc 0.9955574744502956 trn_single_acc 0.9997215740550052 val_acc 0.23069660410976225\n",
      "itr: 18400 trn_loss 50.32822301169401 val_loss 17269.43101870119\n",
      "itr: 18400 trn_acc 0.9877262775721107 trn_single_acc 0.9991414728775291 val_acc 0.23143798573093335\n",
      "itr: 18500 trn_loss 8.519470895631386 val_loss 17402.427987602405\n",
      "itr: 18500 trn_acc 0.9932988731813295 trn_single_acc 0.9994968305624126 val_acc 0.23212648430449925\n",
      "itr: 18600 trn_loss 20.247186621071187 val_loss 17748.372047395398\n",
      "itr: 18600 trn_acc 0.9832252341875761 trn_single_acc 0.99862144037712 val_acc 0.23261328855485064\n",
      "itr: 18700 trn_loss 21.947814993810184 val_loss 17976.395844645103\n",
      "itr: 18700 trn_acc 0.988575634530727 trn_single_acc 0.9993124290516892 val_acc 0.2324031313877114\n",
      "itr: 18800 trn_loss 7.878027583469967 val_loss 18026.389474493586\n",
      "itr: 18800 trn_acc 0.9906590374406088 trn_single_acc 0.99939295034859 val_acc 0.2332714318842546\n",
      "itr: 18900 trn_loss 73.23890974046958 val_loss 18081.270035710346\n",
      "itr: 18900 trn_acc 0.9768628463838427 trn_single_acc 0.9982472162513116 val_acc 0.2344620632854981\n",
      "itr: 19000 trn_loss 5.942840241947114 val_loss 18251.52554446401\n",
      "itr: 19000 trn_acc 0.9921076454346888 trn_single_acc 0.9996375928957627 val_acc 0.23534764929463786\n",
      "itr: 19100 trn_loss 9.313833901182914 val_loss 18389.64384918426\n",
      "itr: 19100 trn_acc 0.9963875113921645 trn_single_acc 0.9996665940755858 val_acc 0.23638911051975087\n",
      "itr: 19200 trn_loss 11.950140365075576 val_loss 18486.353660114437\n",
      "itr: 19200 trn_acc 0.9930042210958598 trn_single_acc 0.999551635078655 val_acc 0.2366568895152268\n",
      "itr: 19300 trn_loss 8.120213559591626 val_loss 18563.831850226874\n",
      "itr: 19300 trn_acc 0.9932721411591561 trn_single_acc 0.9994365451978146 val_acc 0.2381094321381767\n",
      "itr: 19400 trn_loss 9.054790167052815 val_loss 18790.6931768259\n",
      "itr: 19400 trn_acc 0.9926083094515665 trn_single_acc 0.9994471433024021 val_acc 0.23770568378062132\n",
      "itr: 19500 trn_loss 23.40354138290186 val_loss 19041.35784920312\n",
      "itr: 19500 trn_acc 0.9831971100208029 trn_single_acc 0.9987122636875726 val_acc 0.2382828490761327\n",
      "itr: 19600 trn_loss 4.1376050439872 val_loss 19069.781351737787\n",
      "itr: 19600 trn_acc 0.9966893582817687 trn_single_acc 0.9997529615298385 val_acc 0.23994476024710917\n",
      "itr: 19700 trn_loss 8.86473715581934 val_loss 19213.197340186474\n",
      "itr: 19700 trn_acc 0.9937710571383538 trn_single_acc 0.999621437445095 val_acc 0.241137594919193\n",
      "itr: 19800 trn_loss 38.19265409022613 val_loss 19478.243459888636\n",
      "itr: 19800 trn_acc 0.9884110718110269 trn_single_acc 0.9993535821164217 val_acc 0.24136625532221925\n",
      "itr: 19900 trn_loss 5.934194079506607 val_loss 19625.83544700229\n",
      "itr: 19900 trn_acc 0.9939414677271622 trn_single_acc 0.9996550180276482 val_acc 0.2427145120899591\n",
      "itr: 20000 trn_loss 4.2892337182463285 val_loss 19645.787095704403\n",
      "itr: 20000 trn_acc 0.9949294424202202 trn_single_acc 0.999598123184392 val_acc 0.24365162669226986\n",
      "itr: 20100 trn_loss 14.831070857024018 val_loss 19687.500069449\n",
      "itr: 20100 trn_acc 0.9950813299752108 trn_single_acc 0.9996749606174145 val_acc 0.24392114174252746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itr: 20200 trn_loss 13.134339795219864 val_loss 19757.033215451775\n",
      "itr: 20200 trn_acc 0.9928993122673296 trn_single_acc 0.9994221414646991 val_acc 0.24390864391361614\n",
      "itr: 20300 trn_loss 33.159071180764485 val_loss 19910.86747837051\n",
      "itr: 20300 trn_acc 0.9804292929315674 trn_single_acc 0.9989669321354502 val_acc 0.24275493346257973\n",
      "itr: 20400 trn_loss 6.526700773389885 val_loss 20087.878257257515\n",
      "itr: 20400 trn_acc 0.9919970817780166 trn_single_acc 0.999567658954375 val_acc 0.242609308866148\n",
      "itr: 20500 trn_loss 5.185260472060158 val_loss 20270.78758456589\n",
      "itr: 20500 trn_acc 0.9928510574901703 trn_single_acc 0.9996618729776404 val_acc 0.24370041581379542\n",
      "itr: 20600 trn_loss 11.138045155445827 val_loss 20384.081532943987\n",
      "itr: 20600 trn_acc 0.9874379318332173 trn_single_acc 0.9992509549373434 val_acc 0.24333271229517056\n",
      "itr: 20700 trn_loss 15.142975507111963 val_loss 20579.73757123186\n",
      "itr: 20700 trn_acc 0.9923412110344605 trn_single_acc 0.9996254609382567 val_acc 0.24337905741099491\n",
      "itr: 20800 trn_loss 4.530614706551349 val_loss 20763.94527564273\n",
      "itr: 20800 trn_acc 0.9979264785347606 trn_single_acc 0.9998775835615619 val_acc 0.24308599996167396\n",
      "itr: 20900 trn_loss 5.637194760271951 val_loss 20893.115334162085\n",
      "itr: 20900 trn_acc 0.9944549071343701 trn_single_acc 0.9995793611656483 val_acc 0.24374684573855404\n",
      "itr: 21000 trn_loss 3.8612697303076193 val_loss 20926.931080421775\n",
      "itr: 21000 trn_acc 0.9955105731349431 trn_single_acc 0.9996606618568621 val_acc 0.24462323720487084\n",
      "itr: 21100 trn_loss 5.616521029066207 val_loss 21107.286662294457\n",
      "itr: 21100 trn_acc 0.9982033750124587 trn_single_acc 0.9998705597675969 val_acc 0.24498157345584773\n",
      "itr: 21200 trn_loss 3.078282273120408 val_loss 21373.130196738926\n",
      "itr: 21200 trn_acc 0.9982374896910076 trn_single_acc 0.999907900262572 val_acc 0.24597892596732232\n",
      "itr: 21300 trn_loss 33.52315404517625 val_loss 21606.574169541054\n",
      "itr: 21300 trn_acc 0.9926995380269431 trn_single_acc 0.9989707799351677 val_acc 0.245319606089809\n",
      "itr: 21400 trn_loss 4.269454639613359 val_loss 21807.78199274276\n",
      "itr: 21400 trn_acc 0.9979744541323291 trn_single_acc 0.9998767428543182 val_acc 0.246421313582215\n",
      "itr: 21500 trn_loss 4.413304228315492 val_loss 21961.88332613722\n",
      "itr: 21500 trn_acc 0.9949827328490853 trn_single_acc 0.9995852647659377 val_acc 0.24554771402711695\n",
      "itr: 21600 trn_loss 3.8747928444589763 val_loss 22125.42037585091\n",
      "itr: 21600 trn_acc 0.9946294918895013 trn_single_acc 0.9997421031784275 val_acc 0.24521845938953518\n",
      "itr: 21700 trn_loss 8.117237006586048 val_loss 22246.23874361995\n",
      "itr: 21700 trn_acc 0.9952533614104139 trn_single_acc 0.9995816355580535 val_acc 0.24656408781175818\n",
      "itr: 21800 trn_loss 9.956151746222828 val_loss 22350.420254178425\n",
      "itr: 21800 trn_acc 0.9937773726462202 trn_single_acc 0.9995557110383261 val_acc 0.24745101289545193\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-faea40de0e79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m                 \u001b[0mcmd_lengths\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcmd_lengths_np\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_samples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             }\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mval_loss_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_val_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpercent_fully_correct\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_feed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[1;31m# print(eval_itr, sample_size, num_eval_samples, acc_val_i)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mval_loss_i\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msample_size\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnum_eval_samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_eval_itr = -1\n",
    "bs = 1024# bad: 64#trn_samples.shape[0]//10\n",
    "bs_eval = 2048# 128#bs * 4\n",
    "num_eval_samples = float(val_samples_all.shape[0])\n",
    "num_eval_batches = int(np.ceil(num_eval_samples / bs_eval))\n",
    "for itr in range(100000):\n",
    "    if True: # itr == 0:\n",
    "        samples = np.random.choice(trn_samples, size=bs, replace=False)\n",
    "        trn_feed_dict = {\n",
    "            cmd_ind: cmd_np[samples],\n",
    "            act_ind: act_np[samples],\n",
    "            mask_ph: mask_np[samples],\n",
    "            act_lengths: np.clip(struct_np[samples], a_min=1, a_max=None),\n",
    "            cmd_lengths: cmd_lengths_np[samples],\n",
    "        }\n",
    "\n",
    "    trn_feed_dict[learning_rate] = .02 / (np.power(itr + 10, .6))\n",
    "    _, trn_loss, acc_trn_single, acc_trn = sess.run(\n",
    "        [optimizer, loss, percent_correct, percent_fully_correct], trn_feed_dict)\n",
    "    if itr == 0:\n",
    "        trn_loss_avg = trn_loss\n",
    "        acc_trn_avg = acc_trn\n",
    "        acc_trn_single_avg = acc_trn_single\n",
    "    else:\n",
    "        trn_loss_avg = trn_loss_avg * .9 + trn_loss * .1\n",
    "        acc_trn_avg = acc_trn_avg * .9 + acc_trn * .1\n",
    "        acc_trn_single_avg = acc_trn_single_avg * .9 + acc_trn_single * .1\n",
    "    if itr % 100 == 0 and itr > 0:\n",
    "        total_eval_itr += 1.\n",
    "        val_loss = 0.\n",
    "        acc_val = 0.\n",
    "        for eval_itr in range(num_eval_batches):\n",
    "            val_samples = val_samples_all[eval_itr * bs_eval: (eval_itr + 1) * bs_eval]\n",
    "            if eval_itr == num_eval_batches - 1:\n",
    "                val_samples = val_samples_all[eval_itr * bs_eval:]\n",
    "            sample_size = float(val_samples.shape[0])\n",
    "            val_feed_dict = {\n",
    "                cmd_ind: cmd_np[val_samples],\n",
    "                act_ind: act_np[val_samples],\n",
    "                mask_ph: mask_np[val_samples],\n",
    "                act_lengths: np.clip(struct_np[val_samples], a_min=1, a_max=None),\n",
    "                cmd_lengths: cmd_lengths_np[val_samples]\n",
    "            }\n",
    "            val_loss_i, acc_val_i = sess.run([loss, percent_fully_correct], val_feed_dict)\n",
    "            # print(eval_itr, sample_size, num_eval_samples, acc_val_i)\n",
    "            val_loss = val_loss + val_loss_i * sample_size / num_eval_samples\n",
    "            acc_val = acc_val + acc_val_i * sample_size / num_eval_samples\n",
    "        if total_eval_itr == 0:\n",
    "            val_loss_avg = val_loss\n",
    "            acc_val_avg = acc_val\n",
    "        else:\n",
    "            val_loss_avg = val_loss_avg * .9 + val_loss * .1\n",
    "            acc_val_avg = acc_val_avg * .9 + acc_val * .1\n",
    "        print('itr:', itr, 'trn_loss', trn_loss_avg, 'val_loss', val_loss_avg)\n",
    "        print('itr:', itr, 'trn_acc', acc_trn_avg,\n",
    "              'trn_single_acc', acc_trn_single_avg, 'val_acc', acc_val_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "break\n",
    "eval_itr = -1\n",
    "bs = trn_samples.shape[0]\n",
    "num_eval_samples = val_samples.shape[0]\n",
    "num_eval_batches = int(np.ceil(num_eval_samples/bs))\n",
    "for itr in range(100000):\n",
    "    if itr == 0:\n",
    "        samples = np.random.choice(trn_samples, size = bs, replace = False)\n",
    "        trn_feed_dict = {\n",
    "            cmd_ind : cmd_np[samples],\n",
    "            act_ind : act_np[samples],\n",
    "            mask_ph : mask_np[samples],\n",
    "            act_lengths : np.clip(struct_np[samples], a_min = 1, a_max = None),\n",
    "            cmd_lengths : cmd_lengths_np[samples],\n",
    "        }\n",
    "        \n",
    "    trn_feed_dict[learning_rate] = .02 / (np.power(itr + 10, .6))\n",
    "    _, trn_loss, acc_trn_single, acc_trn = sess.run(\n",
    "        [optimizer, loss, percent_correct, percent_fully_correct], trn_feed_dict)\n",
    "    if itr == 0:\n",
    "        trn_loss_avg = trn_loss\n",
    "        acc_trn_avg = acc_trn\n",
    "        acc_trn_single_avg = acc_trn_single\n",
    "    else:\n",
    "        trn_loss_avg = trn_loss_avg * .9 + trn_loss * .1\n",
    "        acc_trn_avg = acc_trn_avg * .9 + acc_trn * .1\n",
    "        acc_trn_single_avg = acc_trn_single_avg * .9 + acc_trn_single * .1\n",
    "    if itr % 10 == 0 and itr > 0:\n",
    "        # val_samples = np.random.choice(val_samples_all, size = bs, replace = False)\n",
    "        eval_itr += 1\n",
    "        val_feed_dict = {\n",
    "            cmd_ind : cmd_np[val_samples],\n",
    "            act_ind : act_np[val_samples],\n",
    "            mask_ph : mask_np[val_samples],\n",
    "            act_lengths : np.clip(struct_np[val_samples], a_min = 1, a_max = None),\n",
    "            cmd_lengths : cmd_lengths_np[val_samples]\n",
    "        }\n",
    "        val_loss, acc_val = sess.run([loss, percent_fully_correct], val_feed_dict)\n",
    "        if eval_itr == 0:\n",
    "            val_loss_avg = val_loss\n",
    "            acc_val_avg = acc_val\n",
    "        else:\n",
    "            val_loss_avg = val_loss_avg * .9 + val_loss * .1\n",
    "            acc_val_avg = acc_val_avg * .9 + acc_val * .1\n",
    "        print('itr:', itr, 'trn_loss', trn_loss_avg, 'val_loss', val_loss_avg)\n",
    "        print('itr:', itr, 'trn_acc', acc_trn_avg, \n",
    "              'trn_single_acc', acc_trn_single_avg, 'val_acc', acc_val_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_percent.shape, percent_fully_correct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "10, None, 7, 9, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_np.shape, act_np.shape, mask_np.shape, struct_np.shape, cmd_lengths_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cmd_ind.shape, act_ind.shape, mask_ph.shape, act_lengths.shape, cmd_lengths.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_samples.shape, val_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "act_presoftmax = tf.stack(action_probabilities_presoftmax, 1)[:, :, :-1, :]\n",
    "#batch, subprogram, timestep, action_selection\n",
    "logprobabilities = tf.nn.log_softmax(act_presoftmax, -1)\n",
    "act_presoftmax_flat = tf.reshape(act_presoftmax, [-1, 9, num_act])\n",
    "mask_ph_flat = tf.reshape(mask_ph, [-1, max_actions_per_subprogram])\n",
    "act_ind_flat = tf.reshape(act_ind, [-1, max_actions_per_subprogram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "act_presoftmax_flat = tf.reshape(act_presoftmax, [-1, 9, num_act])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_actions_per_subprogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess.run(act_presoftmax, feed_dict).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(*actions_ind[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "command_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subprogram_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subprogram_last_layer[:,cmd_lengths,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoding_last_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.gather(\n",
    "    encoding_last_layer,\n",
    "    [1,2],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.gather_nd(\n",
    "    encoding_last_layer,\n",
    "    np.array([[0,1,2,3,4], [1,4,3,2,5]]).T,\n",
    "    name=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cmd_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_command(sub_cmd, num_repeat):\n",
    "    return sub_cmd * num_repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_command(cmd):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uni_commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uni_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uni_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
